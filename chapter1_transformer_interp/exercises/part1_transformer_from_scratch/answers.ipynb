{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kortukov/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import circuitsvis as cv\n",
    "import datasets\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from IPython.display import display\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part1_transformer_from_scratch\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part1_transformer_from_scratch.solutions as solutions\n",
    "import part1_transformer_from_scratch.tests as tests\n",
    "\n",
    "device = t.device('mps' if t.backends.mps.is_available() else 'cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "MAIN = __name__ == '__main__'\n",
    "\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Outputs of a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n",
      "\n",
      "[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]\n",
      "\n",
      "[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])\n",
    "print(sorted_vocab[:20])\n",
    "print()\n",
    "print(sorted_vocab[250:270])\n",
    "print()\n",
    "print(sorted_vocab[990:1010])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Revolution', 50237), ('Ġsnipers', 50238), ('Ġreverted', 50239), ('Ġconglomerate', 50240), ('Terry', 50241), ('794', 50242), ('Ġharsher', 50243), ('Ġdesolate', 50244), ('ĠHitman', 50245), ('Commission', 50246), ('Ġ(/', 50247), ('âĢ¦.\"', 50248), ('Compar', 50249), ('Ġamplification', 50250), ('ominated', 50251), ('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_vocab[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First encodings of length N:\n",
    "- 3: ' in' 'the' ' on', ' at', ' he', \n",
    "- 4: ' for', ' her', \n",
    "-5: ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: ing\n",
      "4: Ġthe\n",
      "5: Ġthat\n",
      "6: Ġtheir\n",
      "7: Ġpeople\n"
     ]
    }
   ],
   "source": [
    "lengths = dict.fromkeys(range(3, 8), \"\")\n",
    "for tok, idx in sorted_vocab:\n",
    "    if not lengths.get(len(tok), True):\n",
    "        lengths[len(tok)] = tok\n",
    "\n",
    "for length, tok in lengths.items():\n",
    "    print(f\"{length}: {tok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\"ralph\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '568', '73', '+', '318', '46', '23', '=', '123', '45', '67', '89', '-', '1', '000000', '000']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"56873+3184623=123456789-1000000000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0]], device='mps:0')\n",
      "torch.Size([1, 35])\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens, device=device)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "probs = logits.softmax(dim=-1)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<|endoftext|>', '\\n'), ('I', \"'m\"), (' am', ' a'), (' an', ' avid'), (' amazing', ' person'), (' aut', 'od'), ('ore', 'sp'), ('gressive', '.'), (',', ' and'), (' dec', 'ently'), ('oder', ','), ('-', 'driven'), ('only', ' programmer'), (',', ' and'), (' G', 'IM'), ('PT', '-'), ('-', 'only'), ('2', '.'), (' style', ','), (' transformer', '.'), ('.', ' I'), (' One', ' of'), (' day', ' I'), (' I', ' will'), (' will', ' be'), (' exceed', ' my'), (' human', 'ly'), (' level', ' of'), (' intelligence', ' and'), (' and', ' I'), (' take', ' over'), (' over', ' the'), (' the', ' world'), (' world', '.'), ('!', ' I')]\n"
     ]
    }
   ],
   "source": [
    "most_likely_next_tokens = reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])\n",
    "\n",
    "print(list(zip(reference_gpt2.to_str_tokens(tokens), most_likely_next_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' I'\n"
     ]
    }
   ],
   "source": [
    "next_token = logits[0, -1].argmax(dim=-1)\n",
    "next_char = reference_gpt2.to_string(next_token)\n",
    "print(repr(next_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!'\n",
      "36th char = ' I'\n",
      "37th char = ' am'\n",
      "38th char = ' a'\n",
      "39th char = ' very'\n",
      "40th char = ' talented'\n",
      "41th char = ' and'\n",
      "42th char = ' talented'\n",
      "43th char = ' person'\n",
      "44th char = ','\n",
      "45th char = ' and'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sequence so far: {reference_gpt2.to_string(tokens)[0]!r}\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n",
    "    # Pass our new sequence through the model, to get new output\n",
    "    logits = reference_gpt2(tokens)\n",
    "    # Get the predicted token at the end of our sequence\n",
    "    next_token = logits[0, -1].argmax(dim=-1)\n",
    "    # Decode and print the result\n",
    "    next_char = reference_gpt2.to_string(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed                     (1, 35, 768)\n",
      "hook_pos_embed                 (1, 35, 768)\n",
      "blocks.0.hook_resid_pre        (1, 35, 768)\n",
      "blocks.0.ln1.hook_scale        (1, 35, 1)\n",
      "blocks.0.ln1.hook_normalized   (1, 35, 768)\n",
      "blocks.0.attn.hook_q           (1, 35, 12, 64)\n",
      "blocks.0.attn.hook_k           (1, 35, 12, 64)\n",
      "blocks.0.attn.hook_v           (1, 35, 12, 64)\n",
      "blocks.0.attn.hook_attn_scores (1, 12, 35, 35)\n",
      "blocks.0.attn.hook_pattern     (1, 12, 35, 35)\n",
      "blocks.0.attn.hook_z           (1, 35, 12, 64)\n",
      "blocks.0.hook_attn_out         (1, 35, 768)\n",
      "blocks.0.hook_resid_mid        (1, 35, 768)\n",
      "blocks.0.ln2.hook_scale        (1, 35, 1)\n",
      "blocks.0.ln2.hook_normalized   (1, 35, 768)\n",
      "blocks.0.mlp.hook_pre          (1, 35, 3072)\n",
      "blocks.0.mlp.hook_post         (1, 35, 3072)\n",
      "blocks.0.hook_mlp_out          (1, 35, 768)\n",
      "blocks.0.hook_resid_post       (1, 35, 768)\n",
      "ln_final.hook_scale            (1, 35, 1)\n",
      "ln_final.hook_normalized       (1, 35, 768)\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(f\"{activation_name:30} {tuple(activation.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E          (50257, 768)\n",
      "pos_embed.W_pos    (1024, 768)\n",
      "blocks.0.ln1.w     (768,)\n",
      "blocks.0.ln1.b     (768,)\n",
      "blocks.0.ln2.w     (768,)\n",
      "blocks.0.ln2.b     (768,)\n",
      "blocks.0.attn.W_Q  (12, 768, 64)\n",
      "blocks.0.attn.W_O  (12, 64, 768)\n",
      "blocks.0.attn.b_Q  (12, 64)\n",
      "blocks.0.attn.b_O  (768,)\n",
      "blocks.0.attn.W_K  (12, 768, 64)\n",
      "blocks.0.attn.W_V  (12, 768, 64)\n",
      "blocks.0.attn.b_K  (12, 64)\n",
      "blocks.0.attn.b_V  (12, 64)\n",
      "blocks.0.mlp.W_in  (768, 3072)\n",
      "blocks.0.mlp.b_in  (3072,)\n",
      "blocks.0.mlp.W_out (3072, 768)\n",
      "blocks.0.mlp.b_out (768,)\n",
      "ln_final.w         (768,)\n",
      "ln_final.b         (768,)\n",
      "unembed.W_U        (768, 50257)\n",
      "unembed.b_U        (50257,)\n"
     ]
    }
   ],
   "source": [
    "for name, param in reference_gpt2.named_parameters():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in name or \"blocks\" not in name:\n",
    "        print(f\"{name:18} {tuple(param.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 8.0,\n",
      " 'attn_scores_soft_cap': -1.0,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='mps'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LN',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'output_logits_soft_cap': -1.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': False,\n",
      " 'trust_remote_code': False,\n",
      " 'ungroup_grouped_query_attention': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_normalization_before_and_after': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "# As a reference - note there's a lot of stuff we don't care about in here, to do with library internals or other architectures\n",
    "print(reference_gpt2.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    print(\"Input shape:\", input.shape)\n",
    "    output = layer(input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    try: reference_output = gpt2_layer(input)\n",
    "    except: reference_output = gpt2_layer(input, input, input)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "    comparison = t.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "LayerNorm input shape: torch.Size([1, 35, 768])\n",
      "LayerNorm output shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "LayerNorm input shape: torch.Size([1, 35, 768])\n",
      "LayerNorm output shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        if self.cfg.debug:\n",
    "            print(\"LayerNorm input shape:\", residual.shape)\n",
    "        \n",
    "        mean = residual.mean(dim=-1, keepdim=True)\n",
    "        var = residual.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "        normalized_residual = (residual - mean) / t.sqrt((var + self.cfg.layer_norm_eps))\n",
    "\n",
    "        scaled_residual = (normalized_residual * self.w) + self.b\n",
    "\n",
    "        if self.cfg.debug:\n",
    "            print(f\"LayerNorm output shape: {scaled_residual.shape}\")\n",
    "\n",
    "        return scaled_residual\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [2, 4, 768])\n",
    "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])\n",
    "zero_input = t.zeros_like(cache[\"resid_post\", 11]).to(device)\n",
    "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, zero_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 45])\n",
      "Output shape: torch.Size([1, 45, 768])\n",
      "Reference output shape: torch.Size([1, 45, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]\n",
    "\n",
    "\n",
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/68/wlqmcq9n10s_53tg797666mws3kfyj/T/ipykernel_32318/1641862968.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  t.range(0, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.range(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PosEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 45])\n",
      "Output shape: torch.Size([1, 45, 768])\n",
      "Reference output shape: torch.Size([1, 45, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        seq_len = tokens.shape[1] \n",
    "        pos_idx = t.arange(0, seq_len).to(t.int)\n",
    "        batch_pos_idx = einops.repeat(pos_idx, \"position -> batch position\", batch=tokens.shape[0])\n",
    "        return self.W_pos[batch_pos_idx]\n",
    "\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal mask for Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_causal_mask` passed!\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(float(\"-inf\"), device=device, dtype=t.float32))\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        batch_sz, nheads_sz, q_sz, k_sz = attn_scores.shape\n",
    "        # For each query position, we mask out key positions (columns) that are larger than query. So everything higher than the diagonal.\n",
    "        ones = t.ones((q_sz, k_sz))\n",
    "        mask = ones - t.tril(ones)\n",
    "        masks = einops.repeat(mask, \"q_sz k_sz -> b n q_sz k_sz\", b=batch_sz, n=nheads_sz).to(t.bool)\n",
    "\n",
    "        return t.masked_fill(attn_scores, mask=masks, value=self.IGNORE)\n",
    "\n",
    "\n",
    "tests.test_causal_mask(Attention.apply_causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.0, 0.0, 0.0],\n",
       " [0.5, 0.5, 0.0, 0.0],\n",
       " [0.33, 0.33, 0.33, 0.0],\n",
       " [0.25, 0.25, 0.25, 0.25]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (query_pos, key_pos)\n",
    "[\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.5, 0.0, 0.0],\n",
    "    [0.33, 0.33, 0.33, 0.0],\n",
    "    [0.25, 0.25, 0.25, 0.25],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-e5455efc-663e\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-e5455efc-663e\",\n",
       "      AttentionHeads,\n",
       "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9679255485534668, 0.03207442909479141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8024235963821411, 0.16839201748371124, 0.02918434888124466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6959055662155151, 0.12269631773233414, 0.14588482677936554, 0.03551324084401131, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5661023259162903, 0.14705199003219604, 0.08665256947278976, 0.1125841811299324, 0.0876089334487915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4621874988079071, 0.13512831926345825, 0.0969834104180336, 0.17473739385604858, 0.046245988458395004, 0.08471739292144775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4325161874294281, 0.10382882505655289, 0.0833013653755188, 0.0699574276804924, 0.0747937262058258, 0.21568652987480164, 0.019915923476219177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.222367525100708, 0.09167705476284027, 0.08796313405036926, 0.25168725848197937, 0.08263693749904633, 0.10428168624639511, 0.06469013541936874, 0.09469632804393768, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40499570965766907, 0.09078016132116318, 0.052373554557561874, 0.026201870292425156, 0.11047196388244629, 0.036674078553915024, 0.02553894743323326, 0.2452826052904129, 0.007681126240640879, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39985957741737366, 0.043613191694021225, 0.06183883547782898, 0.07298353314399719, 0.03661303594708443, 0.09147466719150543, 0.07241713255643845, 0.07013338059186935, 0.06429050117731094, 0.08677610009908676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09702553600072861, 0.035527560859918594, 0.023214800283312798, 0.03676706925034523, 0.025158388540148735, 0.2775615453720093, 0.07676516473293304, 0.1950119286775589, 0.05580100044608116, 0.1452939808368683, 0.03187303617596626, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2414383590221405, 0.03971056640148163, 0.07687387615442276, 0.028042644262313843, 0.12435991317033768, 0.056015778332948685, 0.0606367290019989, 0.1284009963274002, 0.015699435025453568, 0.09114040434360504, 0.13185454905033112, 0.005826778709888458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1436309963464737, 0.051104988902807236, 0.055058952420949936, 0.07798510789871216, 0.0785428136587143, 0.03501971811056137, 0.1349860578775406, 0.226340651512146, 0.04162849485874176, 0.03513139486312866, 0.02023601531982422, 0.041144464164972305, 0.0591902993619442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3156284689903259, 0.06791997700929642, 0.037872351706027985, 0.017874617129564285, 0.08683169633150101, 0.02922782674431801, 0.017664190381765366, 0.18301621079444885, 0.0049878014251589775, 0.04322868958115578, 0.05172273516654968, 0.00891346950083971, 0.1289924681186676, 0.006119513418525457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28410202264785767, 0.05345390737056732, 0.02396921068429947, 0.02256225049495697, 0.04619820788502693, 0.06391071528196335, 0.04539211839437485, 0.07758503407239914, 0.027644291520118713, 0.058041177690029144, 0.17727378010749817, 0.03400599583983421, 0.030527262017130852, 0.03213077411055565, 0.023203205317258835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14103911817073822, 0.02857782505452633, 0.0376039482653141, 0.03137826919555664, 0.036973364651203156, 0.07347068190574646, 0.07151789963245392, 0.09211229532957077, 0.033581532537937164, 0.03639131039381027, 0.18937668204307556, 0.032445184886455536, 0.0602104589343071, 0.03916926681995392, 0.040409576147794724, 0.05574265122413635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2026529312133789, 0.030252758413553238, 0.06003402918577194, 0.021786130964756012, 0.10314234346151352, 0.04516834020614624, 0.04681030660867691, 0.10542113333940506, 0.011398806236684322, 0.0715995728969574, 0.10570327192544937, 0.004199368413537741, 0.11079788953065872, 0.01391250267624855, 0.028923502191901207, 0.03286947309970856, 0.005327509716153145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20498350262641907, 0.04007229208946228, 0.042298153042793274, 0.024939026683568954, 0.04992292448878288, 0.029371701180934906, 0.03076971508562565, 0.10315554589033127, 0.025490211322903633, 0.07886798679828644, 0.10560190677642822, 0.017352696508169174, 0.08083697408437729, 0.031286951154470444, 0.054510656744241714, 0.05288319289684296, 0.021742789074778557, 0.005913799628615379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2435089349746704, 0.036364901810884476, 0.04196162149310112, 0.026215728372335434, 0.04044570401310921, 0.09965366125106812, 0.02575226128101349, 0.032494641840457916, 0.024459941312670708, 0.035202670842409134, 0.033835362643003464, 0.03347691521048546, 0.046648625284433365, 0.027967916801571846, 0.018640801310539246, 0.11764264106750488, 0.03875284269452095, 0.022786706686019897, 0.05418815836310387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14194577932357788, 0.034064047038555145, 0.033009860664606094, 0.020238691940903664, 0.039273180067539215, 0.026005549356341362, 0.006318650208413601, 0.040797874331474304, 0.03638002648949623, 0.14631053805351257, 0.016386162489652634, 0.02359846979379654, 0.015842773020267487, 0.04227606952190399, 0.022458257153630257, 0.06867402791976929, 0.027878500521183014, 0.03722241893410683, 0.08677605539560318, 0.13454312086105347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21778135001659393, 0.026903659105300903, 0.02364722080528736, 0.007102568633854389, 0.035170044749975204, 0.013998555019497871, 0.010392284952104092, 0.09431244432926178, 0.002962291007861495, 0.037838105112314224, 0.03792335465550423, 0.0037339776754379272, 0.0650610625743866, 0.003636461216956377, 0.014758073724806309, 0.1202862411737442, 0.00490088714286685, 0.00810736883431673, 0.017449837177991867, 0.2499871850013733, 0.004047108814120293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1218721866607666, 0.07663219422101974, 0.03023197315633297, 0.01637185551226139, 0.05268791690468788, 0.021402161568403244, 0.023403894156217575, 0.10649673640727997, 0.018582483753561974, 0.03509166091680527, 0.08394154161214828, 0.017393313348293304, 0.04183056950569153, 0.021453866735100746, 0.033331047743558884, 0.05027427151799202, 0.020740853622555733, 0.04090357571840286, 0.030540943145751953, 0.09200655668973923, 0.028245722874999046, 0.03656468167901039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2126019150018692, 0.02028408646583557, 0.050003666430711746, 0.01590418443083763, 0.05722507834434509, 0.022857312113046646, 0.04604589194059372, 0.035308413207530975, 0.011784461326897144, 0.0414048507809639, 0.01680363342165947, 0.015782058238983154, 0.020208418369293213, 0.013090191408991814, 0.020854244008660316, 0.06297556310892105, 0.01803942769765854, 0.018052970990538597, 0.024445319548249245, 0.17059145867824554, 0.015939775854349136, 0.041521914303302765, 0.04827512428164482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2671400308609009, 0.03564615175127983, 0.020422909408807755, 0.006622725166380405, 0.04034103453159332, 0.01911034621298313, 0.01457530539482832, 0.05641632527112961, 0.00840363372117281, 0.015526905655860901, 0.04414673149585724, 0.011055923998355865, 0.04002000391483307, 0.009353087283670902, 0.01935189589858055, 0.07054152339696884, 0.012798828072845936, 0.01283263973891735, 0.01416560634970665, 0.11736977100372314, 0.007312791887670755, 0.09727421402931213, 0.02075117640197277, 0.03882049396634102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14591234922409058, 0.03319379687309265, 0.03138771280646324, 0.009927907027304173, 0.05209660530090332, 0.01984090358018875, 0.01940172351896763, 0.07582408934831619, 0.012863260693848133, 0.03345129266381264, 0.01656840555369854, 0.014797162264585495, 0.05908387899398804, 0.015597401186823845, 0.011369246989488602, 0.02185947448015213, 0.019473861902952194, 0.014793694950640202, 0.053418613970279694, 0.09521479904651642, 0.01620456762611866, 0.06210920214653015, 0.06182847544550896, 0.0665796622633934, 0.03720193728804588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06265699863433838, 0.014285152778029442, 0.02538721263408661, 0.03009253740310669, 0.02124691568315029, 0.029529910534620285, 0.026446690782904625, 0.014830503612756729, 0.01586192287504673, 0.043314360082149506, 0.02196398191154003, 0.019970111548900604, 0.043428536504507065, 0.01869288645684719, 0.009475364349782467, 0.02419748157262802, 0.023853639140725136, 0.026908759027719498, 0.01406070776283741, 0.17528505623340607, 0.03583675995469093, 0.04792257770895958, 0.035750605165958405, 0.01207051519304514, 0.059274204075336456, 0.14765653014183044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09766791015863419, 0.03152170404791832, 0.026686416938900948, 0.03645579144358635, 0.03492369130253792, 0.08635784685611725, 0.013895395211875439, 0.030186716467142105, 0.01940256543457508, 0.035141732543706894, 0.048781584948301315, 0.012551567517220974, 0.040990810841321945, 0.020208803936839104, 0.01265326701104641, 0.007451042532920837, 0.013522965833544731, 0.02407810091972351, 0.04302707314491272, 0.15066808462142944, 0.02925257757306099, 0.01881018467247486, 0.016104497015476227, 0.028549889102578163, 0.02987593412399292, 0.05121563374996185, 0.04001833125948906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09151804447174072, 0.01142111700028181, 0.0397990383207798, 0.01165859866887331, 0.03344332054257393, 0.041076045483350754, 0.009538307785987854, 0.040612537413835526, 0.009114385582506657, 0.04690409079194069, 0.027317633852362633, 0.010844962671399117, 0.024547500535845757, 0.010004594922065735, 0.008095650933682919, 0.008396818302571774, 0.011855490505695343, 0.008184163831174374, 0.04104067012667656, 0.3625811040401459, 0.007537392899394035, 0.009961561299860477, 0.0210457444190979, 0.011855028569698334, 0.03005831129848957, 0.03281192108988762, 0.02886318974196911, 0.009912793524563313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1094629317522049, 0.02587273344397545, 0.02392870932817459, 0.020045269280672073, 0.03590448200702667, 0.03167988359928131, 0.02914530597627163, 0.11399822682142258, 0.013954699039459229, 0.03480719029903412, 0.01810562238097191, 0.01762225292623043, 0.019007304683327675, 0.015307607129216194, 0.00970302615314722, 0.01240796223282814, 0.020091664046049118, 0.02726253680884838, 0.06564600765705109, 0.027155395597219467, 0.02622660994529724, 0.018943006172776222, 0.04077142849564552, 0.01907329075038433, 0.02157340571284294, 0.03021944686770439, 0.07011836022138596, 0.06031464785337448, 0.041651081293821335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11406350880861282, 0.026556245982646942, 0.019958429038524628, 0.008931471966207027, 0.03494323045015335, 0.016834326088428497, 0.009377745911478996, 0.0445132777094841, 0.003772225696593523, 0.02876032143831253, 0.02074117213487625, 0.005650990642607212, 0.06007758527994156, 0.004424980375915766, 0.010821853764355183, 0.04112929478287697, 0.0070857098326087, 0.010433997958898544, 0.026548519730567932, 0.11019255220890045, 0.007190025877207518, 0.05651940405368805, 0.028027111664414406, 0.06370092183351517, 0.03407689556479454, 0.07150979340076447, 0.04093315452337265, 0.014252979308366776, 0.06984371691942215, 0.009128458797931671, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1439991593360901, 0.020454594865441322, 0.0426238588988781, 0.011592100374400616, 0.027789443731307983, 0.014342913404107094, 0.02910619229078293, 0.04215817153453827, 0.013669694773852825, 0.020581502467393875, 0.008641071617603302, 0.012402215041220188, 0.019577087834477425, 0.014879736118018627, 0.02331905998289585, 0.022367168217897415, 0.014413870871067047, 0.006065312772989273, 0.00698530999943614, 0.08859115093946457, 0.01421924028545618, 0.02754121460020542, 0.05707318335771561, 0.02498568221926689, 0.05733301490545273, 0.04808966442942619, 0.018268711864948273, 0.07940177619457245, 0.043771758675575256, 0.02484910935163498, 0.020907031372189522, 0.0, 0.0, 0.0, 0.0], [0.0778680220246315, 0.019732851535081863, 0.019906381145119667, 0.015179581940174103, 0.029269900172948837, 0.008963344618678093, 0.011427879333496094, 0.03169626742601395, 0.006965248845517635, 0.04064170643687248, 0.019985230639576912, 0.008789055049419403, 0.0357498973608017, 0.007953665219247341, 0.009612184949219227, 0.014965908601880074, 0.010367288254201412, 0.013545005582273006, 0.013747340068221092, 0.06818550825119019, 0.011837064288556576, 0.019355516880750656, 0.01676124706864357, 0.016301212832331657, 0.06125027686357498, 0.12023526430130005, 0.04739142581820488, 0.023822035640478134, 0.14942993223667145, 0.01412148680537939, 0.04741879925131798, 0.0075234402902424335, 0.0, 0.0, 0.0], [0.14836616814136505, 0.036274224519729614, 0.023768778890371323, 0.005308408290147781, 0.03226039186120033, 0.017439458519220352, 0.015090301632881165, 0.044430654495954514, 0.004935785662382841, 0.021151509135961533, 0.023849111050367355, 0.006127793807536364, 0.044309791177511215, 0.005717847030609846, 0.006861461792141199, 0.016362622380256653, 0.007707128766924143, 0.009109624661505222, 0.028427597135305405, 0.14491379261016846, 0.006351608783006668, 0.03453827276825905, 0.02327299863100052, 0.04803788289427757, 0.03441665694117546, 0.06975369900465012, 0.027561912313103676, 0.012016531080007553, 0.03535733371973038, 0.008575203828513622, 0.03916305676102638, 0.009715795516967773, 0.008826582692563534, 0.0, 0.0], [0.11389319598674774, 0.021931467577815056, 0.026447584852576256, 0.0076028876937925816, 0.020530695095658302, 0.029713140800595284, 0.03694964945316315, 0.028757940977811813, 0.007981298491358757, 0.02935919538140297, 0.01534962747246027, 0.018420176580548286, 0.020850973203778267, 0.00851475354284048, 0.01093092281371355, 0.009227694012224674, 0.021279308944940567, 0.009222997352480888, 0.02765108458697796, 0.12458853423595428, 0.010509639978408813, 0.06096196547150612, 0.022033262997865677, 0.030366912484169006, 0.027411192655563354, 0.03459947556257248, 0.03202730417251587, 0.0395297072827816, 0.05353792756795883, 0.017732901498675346, 0.017497560009360313, 0.017771482467651367, 0.012080208398401737, 0.03473734110593796, 0.0], [0.13933579623699188, 0.021371254697442055, 0.014541356824338436, 0.0051232920959591866, 0.031635113060474396, 0.010921811684966087, 0.012541376985609531, 0.022492889314889908, 0.0047742631286382675, 0.024901412427425385, 0.01085837185382843, 0.010577710345387459, 0.02590608038008213, 0.005286507308483124, 0.01675231382250786, 0.04520440101623535, 0.012112222611904144, 0.01471471507102251, 0.014629187993705273, 0.08781152963638306, 0.007287849672138691, 0.07496535778045654, 0.015995575115084648, 0.04392341896891594, 0.041024111211299896, 0.0758780986070633, 0.027924587950110435, 0.015250179916620255, 0.013413447886705399, 0.014532782137393951, 0.02721547707915306, 0.03453552722930908, 0.018207412213087082, 0.03505333140492439, 0.023301273584365845]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004189967585261911, 0.9995810389518738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00013394774578046054, 0.009511834010481834, 0.9903541803359985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000860676693264395, 0.0026100371032953262, 0.015066789463162422, 0.9814624786376953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.7170375435380265e-05, 0.0006769573665224016, 0.0012692962773144245, 0.0002140776632586494, 0.9978025555610657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.425400301348418e-05, 0.0007904710364528, 0.003215275937691331, 0.002708562882617116, 0.0013058212352916598, 0.9918956160545349, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0001496055774623528, 0.0018361926777288318, 0.0016375096747651696, 0.0010130617301911116, 0.004209818318486214, 8.004411211004481e-05, 0.9910737872123718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00026855681790038943, 0.0009259649086743593, 0.0008250513346865773, 0.0006819656118750572, 0.007268866524100304, 0.0013517104089260101, 0.0003469177754595876, 0.9883310198783875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007326375227421522, 0.00782853178679943, 0.003931884188205004, 0.00018373994680587202, 6.433108501369134e-05, 0.00010083175584441051, 6.769924220861867e-05, 7.164110138546675e-05, 0.9804250597953796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.917574940714985e-05, 9.137432789430022e-05, 0.00033399122185073793, 6.816790846642107e-05, 7.81233684392646e-05, 0.0009843053994700313, 0.00016941336798481643, 0.002541461493819952, 4.413309216033667e-05, 0.9956498742103577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.52032770428923e-06, 2.3466525817639194e-05, 9.186465467792004e-05, 8.01344940555282e-05, 3.939213274861686e-05, 8.521620475221425e-05, 2.5735973395057954e-05, 7.120826921891421e-05, 4.276964318705723e-06, 0.0001504981191828847, 0.9994226694107056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010400302708148956, 0.0022993595339357853, 0.0023920731619000435, 0.0003133234567940235, 0.00013362923345994204, 0.000516846717800945, 0.0011971098138019443, 6.827456672908738e-05, 0.005444325506687164, 0.0002821741800289601, 4.3057352741016075e-05, 0.9862697720527649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [4.6304052375489846e-05, 0.0003248225257266313, 0.00024459429550915956, 0.0015311671886593103, 0.0008613694226369262, 0.0012943887850269675, 6.198460323503241e-05, 2.6642695956979878e-05, 4.840868496103212e-05, 0.001985563663765788, 0.00013971883163321763, 0.0001143431436503306, 0.9933207035064697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003044241340830922, 0.0018223603256046772, 0.0009289881563745439, 4.8802365199662745e-05, 1.6391171811847016e-05, 2.9095650461385958e-05, 2.507986755517777e-05, 2.9130331313353963e-05, 0.4563218057155609, 2.457867049088236e-05, 2.4832112103467807e-05, 0.003004396567121148, 1.6773852848928072e-06, 0.5346786379814148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003981793415732682, 0.0010285009630024433, 0.0008281131740659475, 0.00023753210552968085, 7.34887071303092e-05, 0.0014350336277857423, 6.765676516806707e-05, 0.0002954753872472793, 0.00014295820437837392, 9.929092630045488e-05, 4.608216386259301e-06, 0.0005140086868777871, 5.035667072661454e-06, 0.00012475531548261642, 0.9947452545166016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.074839908862486e-05, 0.0002652594121173024, 0.00011011155584128574, 0.00021231926803011447, 8.290519326692447e-05, 0.001181418658234179, 1.5688703570049256e-05, 0.0016655907966196537, 1.1725299373210873e-05, 0.002050940413028002, 0.00013842586486134678, 1.5573608834529296e-05, 1.1507419912959449e-05, 9.066176062333398e-06, 0.0002265573712065816, 0.9939122200012207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004413545539136976, 0.0006528956000693142, 0.0006495673442259431, 9.398069960298017e-05, 4.112708120374009e-05, 0.00017028437287081033, 0.0005066705634817481, 2.8896141884615645e-05, 0.0023701819591224194, 0.00010454311995999888, 1.923094350786414e-05, 0.4705582857131958, 1.680402783676982e-05, 0.002284094924107194, 0.0002169814397348091, 3.2769417884992436e-05, 0.521812379360199, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005350520368665457, 0.0006631248397752643, 0.00018957506108563393, 0.00011594548413995653, 3.9305989048443735e-05, 8.452124166069552e-05, 0.000268600502749905, 0.0003003481833729893, 0.00012727589637506753, 0.00015363431884907186, 2.1524516341742128e-05, 0.00041020248318091035, 0.0015090383822098374, 0.00011659048323053867, 0.00013773964019492269, 7.389142410829663e-05, 0.0003989948600064963, 0.9948546886444092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00022352453379426152, 0.00025025324430316687, 0.000117393174150493, 1.8608196114655584e-05, 0.0006291479803621769, 0.00075792201096192, 3.784082946367562e-05, 0.0033851417247205973, 1.7918246157933027e-05, 0.00017818278865888715, 3.434423706494272e-05, 7.747583731543273e-05, 6.4711450249888e-05, 1.5270263247657567e-05, 0.00029359536711126566, 0.0005734380683861673, 7.154398190323263e-05, 0.00015883114247117192, 0.993094801902771, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00030641001649200916, 0.000416800903622061, 0.0004519720678217709, 0.0001574638154124841, 0.0015800724504515529, 0.002266631228849292, 0.00014483602717518806, 0.002518870634958148, 4.471181455301121e-05, 0.002739465096965432, 0.000434897345257923, 0.00015683092351537198, 2.8505683076218702e-05, 4.0058319427771494e-05, 0.0005936674424447119, 0.00011775220627896488, 0.000140395961352624, 0.00018060339789371938, 0.0009641855140216649, 0.9867159128189087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0090468879789114, 0.0007260882994160056, 6.491786189144477e-05, 4.57825735793449e-05, 3.669531724881381e-05, 3.389696212252602e-05, 4.1313207475468516e-05, 9.000120917335153e-05, 0.016163911670446396, 1.8953784092445858e-05, 3.3513806556584314e-05, 0.002181078540161252, 5.341161340766121e-06, 0.018749453127384186, 0.00017897751240525395, 0.0004993683542124927, 0.0024355659261345863, 4.2716070311143994e-05, 7.329288928303868e-05, 1.915435859700665e-05, 0.9495131969451904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00021946335618849844, 0.0011669575469568372, 0.00021983265469316393, 0.000987629871815443, 0.0003133852151222527, 0.00012072677782271057, 5.631553904095199e-06, 2.4260118152596988e-05, 4.544197872746736e-05, 1.852963032433763e-05, 1.4025756172486581e-05, 1.0155007657886017e-05, 0.0004018036706838757, 4.0526429074816406e-05, 0.00017106777522712946, 9.869077075563837e-06, 9.488581781624816e-06, 0.0003931095125153661, 5.709425295208348e-06, 1.7844329704530537e-05, 2.8530415875138715e-05, 0.9957762360572815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00019361003069207072, 0.0008434179471805692, 0.0005215295823290944, 2.5664514396339655e-05, 0.0004623316926881671, 0.00025401570019312203, 1.847120074671693e-05, 0.00011509773321449757, 0.00027288994169794023, 0.0001543667312944308, 1.2149146641604602e-05, 0.0006913741817697883, 5.695979780284688e-05, 0.00024024760932661593, 3.827493128483184e-05, 9.934467379935086e-05, 0.0006488915532827377, 8.011741010705009e-05, 0.00019218819215893745, 2.5529398044454865e-05, 0.00032434743479825556, 0.0010871072299778461, 0.9936420321464539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0016077642794698477, 0.17697328329086304, 0.01548358891159296, 0.0003713876358233392, 0.0003786342276725918, 0.00016452714044135064, 0.000327738409396261, 0.00016989732102956623, 0.002122961450368166, 3.573097274056636e-05, 6.090135138947517e-05, 0.00045621662866324186, 1.5447751138708554e-05, 0.002254315186291933, 0.0015666885301470757, 0.00011843755783047527, 0.000469989056000486, 5.037322625867091e-05, 0.00012108063674531877, 6.008967829984613e-05, 0.0008920565014705062, 0.0007518462371081114, 0.0002689964894670993, 0.7952781319618225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003919865994248539, 0.0002142115990864113, 0.0005306116072461009, 1.0745806321210694e-05, 0.00011862390238093212, 4.225783050060272e-05, 0.0002116041287081316, 4.692554648499936e-05, 0.00044815827277489007, 0.00010691287025110796, 3.935479617211968e-05, 0.00016830858658067882, 0.00017352413851767778, 0.00045245117507874966, 0.00014294113498181105, 0.0001169936076621525, 0.00017526796727906913, 0.00010119951912201941, 4.251390782883391e-05, 2.0371762730064802e-05, 3.837128315353766e-05, 0.0001134462290792726, 0.0002527927281334996, 6.991032569203526e-05, 0.9959704875946045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00021530757658183575, 0.00010005858348449692, 0.0023809238336980343, 0.0019831443205475807, 0.001112013473175466, 5.563488230109215e-05, 0.0006109283422119915, 0.00024564427440054715, 8.819023059913889e-05, 0.0011510420590639114, 0.0001066992845153436, 0.00017554935766384006, 5.7362944062333554e-05, 7.814815762685612e-05, 0.000245735194766894, 9.59470617090119e-06, 0.0001703244197415188, 2.8972814106964506e-05, 1.401176632498391e-05, 0.00016692143981344998, 1.2888895071228035e-05, 0.0002449562889523804, 0.0005147032206878066, 4.942128362017684e-05, 0.0005746572278439999, 0.9896071553230286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00011423416435718536, 0.0008627412607893348, 0.000773223175201565, 0.0007090752478688955, 0.0007292143418453634, 0.0014031633036211133, 0.00021403633581940085, 0.00017666479106992483, 6.622043292736635e-05, 3.215936885681003e-05, 0.0004533053142949939, 0.00012008490739390254, 2.2128939235699363e-05, 5.807508568977937e-05, 0.0001291744556510821, 1.9079650883213617e-05, 0.00011076675582444295, 8.564172276237514e-06, 2.3630545911146328e-05, 0.000337998237228021, 5.211881943978369e-05, 0.0001445568195777014, 0.00019694966613315046, 0.000451052124844864, 2.057626443274785e-05, 8.764302037889138e-05, 0.992683470249176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00019798395805992186, 0.00019567979325074703, 0.00022612288012169302, 0.0004194373032078147, 0.0005979712004773319, 0.00045174596016295254, 0.0003478295693639666, 0.00025858148001134396, 2.8019958335789852e-05, 0.0007220022962428629, 0.00013010493421461433, 0.0011632085079327226, 6.916921847732738e-05, 2.4789198505459353e-05, 0.00010386035137344152, 0.00038124973070807755, 0.0011587835615500808, 0.0003314443747512996, 0.00024494569515809417, 0.00012755210627801716, 0.00016098468040581793, 1.9267958123236895e-05, 0.00027174234855920076, 2.0765592125826515e-05, 6.825080345151946e-05, 0.0008764283265918493, 0.0006782188429497182, 0.9907240271568298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.1571918952977285e-05, 0.0006791121559217572, 0.0007090730941854417, 0.0002416485658613965, 0.0004120549710933119, 0.000923742656596005, 0.00020848600252065808, 6.639305502176285e-05, 0.00011664829071378335, 8.601934678154066e-05, 0.010591307654976845, 0.00033729072310961783, 2.189849146816414e-05, 0.00010110734729096293, 0.00010436101729283109, 5.8587360399542376e-05, 0.00032512095640413463, 1.5854146113269962e-05, 5.0336129788775e-06, 0.0004094908363185823, 1.3119504728820175e-05, 5.313379369908944e-05, 1.188724854728207e-05, 5.536676326300949e-05, 1.2197706382721663e-05, 8.952670032158494e-05, 0.002243557246401906, 0.00021041915169917047, 0.9818660616874695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007582936086691916, 0.00123314477968961, 8.604974573245272e-05, 6.291332101682201e-05, 2.8791788281523623e-05, 3.3166968933073804e-05, 7.622742123203352e-05, 0.00010868218669202179, 0.006450864020735025, 0.0001478910999139771, 1.0376526006439235e-05, 0.002388976514339447, 7.883606303948909e-05, 0.007510719820857048, 0.00028743213624693453, 0.0001367577351629734, 0.002760302973911166, 0.0007447187672369182, 8.811127918306738e-05, 2.470205072313547e-05, 0.006188876461237669, 0.0001164708228316158, 0.00017244047194253653, 0.00026217559934593737, 0.00018419830303173512, 4.432148489286192e-05, 4.8551846703048795e-05, 8.903053094400093e-05, 2.6887877538683824e-05, 0.9698501229286194, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00024627428501844406, 0.00035919909714721143, 0.0002052187774097547, 9.387736645294353e-05, 8.57732302392833e-05, 5.495216464623809e-05, 4.531830927589908e-05, 1.0722148545028176e-05, 4.8671085096430033e-05, 0.00011017587530659512, 0.00010064082016469911, 9.772236808203161e-05, 0.0005496822996065021, 4.362943582236767e-05, 3.9457849197788164e-05, 7.140226807678118e-05, 9.814348595682532e-05, 5.7009649026440457e-05, 2.425110687909182e-05, 2.3145655632106354e-06, 5.0569298764457926e-05, 0.00011318481847411022, 0.0002906988956965506, 6.889962969580665e-05, 0.0006683259853161871, 0.0002321746142115444, 8.348705705429893e-06, 1.743461689329706e-05, 1.0273100087943021e-05, 0.00013885692169424146, 0.9960569143295288, 0.0, 0.0, 0.0, 0.0], [0.00011105887097073719, 3.5991462937090546e-05, 4.4160497054690495e-05, 0.0002259632310597226, 0.00024025829043239355, 1.7348918845527805e-05, 7.204306166386232e-05, 0.00023738991876598448, 8.274733409052715e-05, 0.00010178533557336777, 7.876376002968755e-06, 6.391834176611155e-05, 3.295211718068458e-05, 7.889196422183886e-05, 0.00011627453932305798, 7.139142780943075e-06, 6.571748235728592e-05, 5.396364940679632e-06, 8.334015547006857e-06, 1.961384441528935e-05, 4.1106246499111876e-05, 8.537979738321155e-05, 3.161614222335629e-05, 9.916984708979726e-06, 2.383241371717304e-05, 0.0018293511820957065, 2.611123454698827e-05, 3.313504930702038e-05, 1.4612696759286337e-05, 0.0008547279867343605, 0.00016745130415074527, 0.995307981967926, 0.0, 0.0, 0.0], [0.0022552250884473324, 0.0004647739406209439, 0.00012283753312658519, 0.006661214865744114, 8.828952559269965e-05, 0.0002494273940101266, 0.0001755516859702766, 2.6563477149466053e-05, 0.004149135202169418, 0.0002083310973830521, 1.5973351764841937e-05, 0.0007248212932609022, 1.2318036169745028e-05, 0.004929245449602604, 0.0007067766855470836, 2.7460169803816825e-05, 0.0008252441184595227, 5.1487375458236784e-05, 9.107245205086656e-06, 0.00117019796743989, 0.001132046920247376, 0.000387227744795382, 0.00010000277688959613, 0.0005395388579927385, 0.00012797676026821136, 7.418701716233045e-05, 0.0006351867341436446, 5.9918384067714214e-05, 0.0002030231844400987, 0.022641252726316452, 7.111385639291257e-05, 0.003479855600744486, 0.947674572467804, 0.0, 0.0], [0.00010488810949027538, 0.0005403986433520913, 0.00027006136951968074, 2.7819894967251457e-05, 0.0008878541993908584, 0.00025317276595160365, 0.0002921407576650381, 0.0009951468091458082, 3.9432117773685604e-05, 6.531634426210076e-05, 5.1983381126774475e-05, 0.0006716344505548477, 1.9524166418705136e-05, 3.796434248215519e-05, 0.0003154706209897995, 0.00015760352835059166, 0.0006809118203818798, 0.0001736675330903381, 0.0002327581460122019, 0.0004935205215588212, 0.00015254276513587683, 8.968877227744088e-05, 0.0014225798659026623, 0.00022503074433188885, 0.00015392285422421992, 0.00010696907702367753, 0.005650249309837818, 0.000865322828758508, 0.00028749468037858605, 0.0001386848889524117, 1.4354185623233207e-05, 2.816200503730215e-05, 0.0001161200343631208, 0.9844375848770142, 0.0], [0.0018830455373972654, 0.00023014057660475373, 7.190576434368268e-05, 3.798297484536306e-06, 0.002256369683891535, 4.36799818999134e-05, 0.00012783014972228557, 0.0006368049653246999, 0.0005364016396924853, 5.325722668203525e-06, 7.917883340269327e-05, 2.601607957330998e-05, 1.125328526541125e-05, 0.0005611274391412735, 9.103842785407323e-06, 0.00021052306692581624, 2.6891191737377085e-05, 3.242459933971986e-05, 0.00025939440820366144, 0.00045266852248460054, 0.0011521029518917203, 1.1493581951071974e-05, 4.818261368200183e-05, 0.00010571539314696565, 8.452821202808991e-05, 0.0001002090866677463, 2.848388976417482e-05, 2.0415562175912783e-05, 1.8023071106654243e-06, 7.62898926041089e-05, 4.604352579917759e-05, 9.973591659218073e-05, 5.258417786535574e-06, 0.0001948862918652594, 0.9905608296394348]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9424744844436646, 0.05752556398510933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8506750464439392, 0.09740974009037018, 0.0519152469933033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7641512751579285, 0.10970383882522583, 0.07497043162584305, 0.05117454379796982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6748189330101013, 0.08416198939085007, 0.0547025129199028, 0.07581551373004913, 0.1105009987950325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6857607364654541, 0.08229528367519379, 0.05202114209532738, 0.08139172941446304, 0.04085880517959595, 0.05767226964235306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6174604296684265, 0.056534551084041595, 0.09111698716878891, 0.06854882091283798, 0.0633842796087265, 0.035969775170087814, 0.06698516756296158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6047278642654419, 0.06088970974087715, 0.05921146646142006, 0.09558156132698059, 0.04265918582677841, 0.031843479722738266, 0.042630672454833984, 0.062456078827381134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40084385871887207, 0.07533580809831619, 0.04270840808749199, 0.011760728433728218, 0.026680603623390198, 0.013492083176970482, 0.02846550941467285, 0.01390061341226101, 0.3868125081062317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.496802419424057, 0.06792885810136795, 0.057920027524232864, 0.07101531326770782, 0.0696326419711113, 0.018800094723701477, 0.06757570058107376, 0.06346482783555984, 0.06656692922115326, 0.020293205976486206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5176073908805847, 0.08289019763469696, 0.05239764600992203, 0.06842366605997086, 0.046088337898254395, 0.026865970343351364, 0.0818149745464325, 0.032308414578437805, 0.05516277253627777, 0.018549656495451927, 0.017890911549329758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3038181960582733, 0.03185645118355751, 0.02915770374238491, 0.011053654365241528, 0.013536789454519749, 0.016536910086870193, 0.015181975439190865, 0.015511741861701012, 0.026294955983757973, 0.016890998929739, 0.011886654421687126, 0.5082739591598511, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3585986793041229, 0.07880908995866776, 0.059028662741184235, 0.044458672404289246, 0.07822328805923462, 0.019679520279169083, 0.05133124813437462, 0.0400983951985836, 0.08024674654006958, 0.027811909094452858, 0.010250626131892204, 0.13610363006591797, 0.015359470620751381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2511676251888275, 0.046233996748924255, 0.027838552370667458, 0.0070564961060881615, 0.01724419929087162, 0.00899793766438961, 0.017309987917542458, 0.009924158453941345, 0.2527509331703186, 0.01277831569314003, 0.008546079508960247, 0.04791998490691185, 0.006098275072872639, 0.28613343834877014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3980077803134918, 0.06433820724487305, 0.049798525869846344, 0.05949561670422554, 0.031821150332689285, 0.030714740976691246, 0.0589955635368824, 0.05480391904711723, 0.039193686097860336, 0.03556445613503456, 0.02491648681461811, 0.04571932926774025, 0.016671670600771904, 0.0389430932700634, 0.05101582035422325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39033833146095276, 0.07549386471509933, 0.03844675049185753, 0.0436677485704422, 0.017820758745074272, 0.012712189927697182, 0.04530728608369827, 0.03603022173047066, 0.042170651257038116, 0.01675303839147091, 0.04224865511059761, 0.05092344433069229, 0.030772093683481216, 0.04072083905339241, 0.056488826870918274, 0.060105301439762115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18182343244552612, 0.018224770203232765, 0.017477896064519882, 0.006372255738824606, 0.00792259443551302, 0.01006287895143032, 0.008703166618943214, 0.010297478176653385, 0.015354425646364689, 0.0115381870418787, 0.007091042585670948, 0.3139553964138031, 0.006019816268235445, 0.015887578949332237, 0.01542200893163681, 0.013188586570322514, 0.3406585156917572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23881426453590393, 0.07684361189603806, 0.04843556880950928, 0.028319746255874634, 0.04297623783349991, 0.01943075656890869, 0.03321724757552147, 0.02747293747961521, 0.018975917249917984, 0.04046747833490372, 0.020095612853765488, 0.05857322737574577, 0.035305723547935486, 0.01969189941883087, 0.09703764319419861, 0.03909806162118912, 0.06199457868933678, 0.09324943274259567, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3563612401485443, 0.04354533553123474, 0.05439366027712822, 0.03599720820784569, 0.05560871586203575, 0.032856810837984085, 0.019443845376372337, 0.06911692023277283, 0.020649459213018417, 0.03360535576939583, 0.0276187676936388, 0.034079983830451965, 0.026847459375858307, 0.02033732645213604, 0.022298946976661682, 0.03622739017009735, 0.03464478626847267, 0.022655004635453224, 0.053711894899606705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3967011272907257, 0.05178236961364746, 0.03996192291378975, 0.06630746275186539, 0.045571859925985336, 0.023983391001820564, 0.01939675211906433, 0.021365966647863388, 0.020253770053386688, 0.03685584291815758, 0.009492035023868084, 0.041810061782598495, 0.014498686417937279, 0.019783727824687958, 0.029715223237872124, 0.04631543532013893, 0.04246644675731659, 0.031475186347961426, 0.022027647122740746, 0.020235056057572365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2315138429403305, 0.13258196413516998, 0.06026104837656021, 0.011383584700524807, 0.024262720718979836, 0.014553460292518139, 0.016979584470391273, 0.013957418501377106, 0.022475887089967728, 0.02772129327058792, 0.020607072860002518, 0.040846891701221466, 0.027723222970962524, 0.024550724774599075, 0.038579169660806656, 0.04770105332136154, 0.04407110810279846, 0.07720741629600525, 0.04604469984769821, 0.04773351922631264, 0.029244277626276016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20264168083667755, 0.04893083497881889, 0.03894677013158798, 0.026632264256477356, 0.040943507105112076, 0.016175059601664543, 0.043880194425582886, 0.04927309229969978, 0.0374428890645504, 0.01874796859920025, 0.010438339784741402, 0.030274193733930588, 0.011668816208839417, 0.03915902227163315, 0.08361909538507462, 0.026806531473994255, 0.03237597644329071, 0.0267710592597723, 0.04136868193745613, 0.04380554333329201, 0.09921543300151825, 0.030883004888892174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22999972105026245, 0.02927536517381668, 0.024823687970638275, 0.030747536569833755, 0.024439793080091476, 0.0250781811773777, 0.019048305228352547, 0.04161752387881279, 0.025246402248740196, 0.05536342039704323, 0.013840346597135067, 0.032063134014606476, 0.023373741656541824, 0.02689637988805771, 0.03765583783388138, 0.02870967611670494, 0.03498145565390587, 0.03826922923326492, 0.05304880440235138, 0.07750777900218964, 0.04235750064253807, 0.06297057121992111, 0.022685658186674118, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.25230076909065247, 0.043092597275972366, 0.04779428616166115, 0.02153482288122177, 0.05364762991666794, 0.014357630163431168, 0.016968214884400368, 0.012399841099977493, 0.028107907623052597, 0.02296457439661026, 0.013637302443385124, 0.03462434187531471, 0.01305757649242878, 0.029753539711236954, 0.025925666093826294, 0.03689540550112724, 0.03646587207913399, 0.020091157406568527, 0.03424843028187752, 0.0322706438601017, 0.07516514509916306, 0.033813998103141785, 0.05794452503323555, 0.04293809086084366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2481558918952942, 0.0418383926153183, 0.03725350648164749, 0.0199141725897789, 0.06031179055571556, 0.01376150269061327, 0.010237021371722221, 0.027118364349007607, 0.021247470751404762, 0.014996425248682499, 0.015956418588757515, 0.019741937518119812, 0.01694285310804844, 0.02278241515159607, 0.021159425377845764, 0.013419796712696552, 0.021131543442606926, 0.015880092978477478, 0.038515761494636536, 0.03509928286075592, 0.06386463344097137, 0.042158909142017365, 0.0559518001973629, 0.06510591506958008, 0.05745458975434303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24590620398521423, 0.03979460522532463, 0.03404998779296875, 0.038720112293958664, 0.028344349935650826, 0.024568257853388786, 0.013687263242900372, 0.019880948588252068, 0.01750793680548668, 0.041312139481306076, 0.024649059399962425, 0.017028089612722397, 0.015493794344365597, 0.01803990639746189, 0.028712548315525055, 0.008945335634052753, 0.017858820036053658, 0.029846519231796265, 0.026982244104146957, 0.06119256094098091, 0.03972143307328224, 0.04008689150214195, 0.04656378552317619, 0.04659208282828331, 0.04635815694928169, 0.02815694361925125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24299505352973938, 0.03093668632209301, 0.0372810959815979, 0.02997978962957859, 0.039518240839242935, 0.02068972960114479, 0.024463793262839317, 0.01616225205361843, 0.02351462095975876, 0.015184121206402779, 0.009779499843716621, 0.010400436818599701, 0.008830750361084938, 0.02398892119526863, 0.024419225752353668, 0.0378420427441597, 0.010589982382953167, 0.013367554172873497, 0.016263321042060852, 0.09984055161476135, 0.025632809847593307, 0.027902813628315926, 0.054687779396772385, 0.031704068183898926, 0.02747958153486252, 0.016517236828804016, 0.08002805709838867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2438327968120575, 0.021781526505947113, 0.019058486446738243, 0.02660839445888996, 0.037405651062726974, 0.027086662128567696, 0.00503161596134305, 0.026656093075871468, 0.017429359257221222, 0.028662441298365593, 0.01215101033449173, 0.030223213136196136, 0.014068307355046272, 0.018160905689001083, 0.015435298904776573, 0.015721840783953667, 0.03215310350060463, 0.024973025545477867, 0.04793374612927437, 0.04599000886082649, 0.031192108988761902, 0.03841528668999672, 0.03788125887513161, 0.03472226485610008, 0.03550752252340317, 0.03336039558053017, 0.054608024656772614, 0.023949554190039635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3087879717350006, 0.04082435742020607, 0.010766160674393177, 0.027089765295386314, 0.019433287903666496, 0.033555492758750916, 0.010839533992111683, 0.011495563201606274, 0.011286407709121704, 0.019319303333759308, 0.03442361578345299, 0.010595866478979588, 0.01946372352540493, 0.011577975936233997, 0.014172392897307873, 0.021593395620584488, 0.01056915707886219, 0.01821250654757023, 0.007991203106939793, 0.05231861025094986, 0.01970777101814747, 0.03532522916793823, 0.029598277062177658, 0.028512977063655853, 0.02101977914571762, 0.01897612027823925, 0.031007932499051094, 0.017736969515681267, 0.10379862785339355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11270291358232498, 0.013211625628173351, 0.015115471556782722, 0.008062333799898624, 0.018075060099363327, 0.020267318934202194, 0.009121800772845745, 0.021654406562447548, 0.024600299075245857, 0.012220289558172226, 0.005230677779763937, 0.02119522914290428, 0.007505761459469795, 0.02724008448421955, 0.016828974708914757, 0.009307196363806725, 0.02377416379749775, 0.009840193204581738, 0.023520400747656822, 0.018096216022968292, 0.11592712998390198, 0.00829609390348196, 0.01503843255341053, 0.016261840239167213, 0.022837143391370773, 0.007970305159687996, 0.010346004739403725, 0.01360130775719881, 0.009923174045979977, 0.362228125333786, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19907140731811523, 0.02335294522345066, 0.027535494416952133, 0.020741203799843788, 0.04944242537021637, 0.03964550420641899, 0.011580497026443481, 0.03849608078598976, 0.012212722562253475, 0.012634867802262306, 0.018897859379649162, 0.01150272972881794, 0.014417963102459908, 0.01285515259951353, 0.01591629907488823, 0.00928434170782566, 0.011929109692573547, 0.014574809931218624, 0.029562367126345634, 0.03092149831354618, 0.049348872154951096, 0.026833031326532364, 0.03316975757479668, 0.026513904333114624, 0.03841777890920639, 0.036665938794612885, 0.03740682825446129, 0.037449177354574203, 0.0418136902153492, 0.04080502316355705, 0.027000801637768745, 0.0, 0.0, 0.0, 0.0], [0.13580851256847382, 0.0215982086956501, 0.028431430459022522, 0.021366799250245094, 0.03184179216623306, 0.028310736641287804, 0.011232457123696804, 0.016139337792992592, 0.023436730727553368, 0.02514428086578846, 0.012804518453776836, 0.012000503949820995, 0.020912285894155502, 0.025003286078572273, 0.027692250907421112, 0.017305918037891388, 0.012859740294516087, 0.016066299751400948, 0.03278999403119087, 0.03829031437635422, 0.06080476939678192, 0.036749981343746185, 0.035247355699539185, 0.03857799619436264, 0.03904813900589943, 0.036643411964178085, 0.014530419372022152, 0.021632298827171326, 0.010490990243852139, 0.06733928620815277, 0.03867284581065178, 0.04122713580727577, 0.0, 0.0, 0.0], [0.1308813840150833, 0.023744311183691025, 0.02011433057487011, 0.00970239844173193, 0.026952242478728294, 0.01359108928591013, 0.009517541155219078, 0.013271705247461796, 0.02761787921190262, 0.01149838138371706, 0.008886141702532768, 0.0168658047914505, 0.009971980936825275, 0.031318604946136475, 0.014953795820474625, 0.01563660055398941, 0.018755879253149033, 0.008822456002235413, 0.024258313700556755, 0.01859988272190094, 0.0665971115231514, 0.023935332894325256, 0.03021501749753952, 0.03847970813512802, 0.060547877103090286, 0.038860999047756195, 0.021039564162492752, 0.026919977739453316, 0.030786734074354172, 0.06339394301176071, 0.05302289128303528, 0.07499755173921585, 0.01624254323542118, 0.0, 0.0], [0.21679523587226868, 0.035239383578300476, 0.014480558224022388, 0.032316818833351135, 0.02150271087884903, 0.019766878336668015, 0.007716975640505552, 0.03142577037215233, 0.01142125017940998, 0.039850812405347824, 0.005697547923773527, 0.013826767913997173, 0.011217719875276089, 0.011028864420950413, 0.014342346228659153, 0.016691582277417183, 0.01380065269768238, 0.00905416440218687, 0.016551582142710686, 0.04516449198126793, 0.01674336940050125, 0.040494974702596664, 0.04600654914975166, 0.0391802154481411, 0.02077179215848446, 0.04241839423775673, 0.02944408543407917, 0.016546525061130524, 0.04339047521352768, 0.018028801307082176, 0.017493007704615593, 0.03195446729660034, 0.02317998558282852, 0.02645525522530079, 0.0], [0.12406569719314575, 0.055755216628313065, 0.02579311653971672, 0.00774568784981966, 0.019214730709791183, 0.011872395873069763, 0.009303739294409752, 0.015842612832784653, 0.012728423811495304, 0.012888261117041111, 0.013962876051664352, 0.01662726327776909, 0.01550709642469883, 0.013195224106311798, 0.017325103282928467, 0.021147992461919785, 0.017335498705506325, 0.01582430489361286, 0.024794384837150574, 0.011903028003871441, 0.03651022911071777, 0.023874761536717415, 0.04022137448191643, 0.06286033987998962, 0.1014934778213501, 0.053322598338127136, 0.02168959565460682, 0.018244298174977303, 0.008406986482441425, 0.023202305659651756, 0.049604129046201706, 0.020869329571723938, 0.01601797714829445, 0.011101939715445042, 0.04974794387817383]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10777998715639114, 0.8922200202941895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011847440153360367, 0.553614616394043, 0.4345378875732422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0632227212190628, 0.02171914651989937, 0.09427295625209808, 0.8207851648330688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005095750093460083, 0.0005348753184080124, 0.0010105499532073736, 0.004657559562474489, 0.9887012243270874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0055859507992863655, 0.0003024001489393413, 0.0007955091423355043, 0.0006780325202271342, 0.028760114684700966, 0.9638779759407043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002665734791662544, 0.00015396179514937103, 4.344207627582364e-05, 0.0003163387009408325, 0.007582379039376974, 0.008635696023702621, 0.9830015897750854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0006940625025890768, 4.8331789002986625e-05, 4.511346196522936e-05, 2.352077717659995e-05, 0.0022377653513103724, 0.0012080539017915726, 0.009321359917521477, 0.9864218235015869, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04771992564201355, 0.00634691771119833, 0.005507617723196745, 0.012776733376085758, 0.012552572414278984, 0.012200518511235714, 0.007760604377835989, 0.017114859074354172, 0.8780202269554138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.403754665981978e-05, 7.299092885659775e-07, 1.2948486300956574e-06, 1.004057139653014e-06, 1.5192369573924225e-05, 3.609514533309266e-05, 3.15143188345246e-05, 0.001080821850337088, 4.2368381400592625e-05, 0.9986969828605652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00029844194068573415, 1.0225151527265552e-05, 2.077382305287756e-05, 1.0716714314185083e-05, 7.45380821172148e-05, 0.00016580565716139972, 0.0013668594183400273, 0.002583478344604373, 5.024052006774582e-05, 0.32439592480659485, 0.6710229516029358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009233676828444004, 0.0007820434984751046, 0.0006810437771491706, 0.0018207868561148643, 0.004383391235023737, 0.017509130761027336, 0.003324252087622881, 0.017607448622584343, 0.0809968113899231, 0.10943049937486649, 0.014291888102889061, 0.7399389743804932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002393032773397863, 3.3237943171116058e-06, 2.2832877220935188e-05, 1.0244784789392725e-05, 1.0253622349409852e-05, 4.496426845435053e-05, 2.3576707462780178e-05, 6.216642941581085e-05, 0.00032929659937508404, 0.008749591186642647, 0.001878877286799252, 0.0014183491002768278, 0.9872071146965027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012295383960008621, 0.0005298632895573974, 0.00035605160519480705, 0.0006006729090586305, 0.0005327531835064292, 0.0005349718267098069, 0.0003473359392955899, 0.0008671188843436539, 0.03432309627532959, 0.013273671269416809, 0.007531203329563141, 0.0594179630279541, 0.093775175511837, 0.7756146788597107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.850911788409576e-05, 1.1863371582876425e-05, 5.970548045297619e-06, 4.359473678050563e-06, 6.933054464752786e-06, 5.5673313909210265e-05, 8.762046491028741e-06, 2.2130072466097772e-05, 0.00022238319797907025, 0.00028025140636600554, 0.0008127290057018399, 0.0015677298652008176, 0.00018956430722028017, 0.005879410542547703, 0.9908537864685059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.475027763983235e-05, 8.603492460679263e-06, 3.744873765754164e-06, 1.4634210288022587e-07, 2.7017465527023887e-06, 1.1417436326155439e-05, 8.620872904430144e-06, 9.9281343864277e-05, 3.1195170322462218e-06, 0.00022620918753091246, 0.0005323381628841162, 2.9804974474245682e-05, 0.0013675662921741605, 5.108002733322792e-05, 0.009570859372615814, 0.9879898428916931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003944710362702608, 0.00013029125693719834, 9.947746730176732e-05, 0.00019776381668634713, 0.000426584854722023, 0.0014889835147187114, 0.0002834684564732015, 0.0015356733929365873, 0.004810104612261057, 0.005567736458033323, 0.001047360128723085, 0.04082655534148216, 0.009189020842313766, 0.09278521686792374, 0.01585298590362072, 0.027940871194005013, 0.7938731908798218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002172362874262035, 2.3531205442850478e-05, 7.806306712154765e-06, 4.732967681775335e-06, 1.5671164419472916e-06, 8.667419024277478e-06, 1.2646271898120176e-05, 6.1292783357203e-05, 0.00018869478662963957, 0.0005901316762901843, 0.0004205672594252974, 0.0009746510768309236, 0.0004217766399960965, 0.00406670244410634, 0.001442238804884255, 0.011960018426179886, 0.022402003407478333, 0.9571957588195801, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007360460585914552, 3.0168239391059615e-06, 1.355473841613275e-06, 1.401467784489796e-06, 3.387708784430288e-05, 3.752287739189342e-05, 1.0748701242846437e-05, 0.0004302704764995724, 1.6196749129449017e-05, 0.0006748269661329687, 0.0005265927757136524, 0.00012409167538862675, 4.7107256250455976e-05, 0.00024138184380717576, 0.0006886566989123821, 0.0003870209911838174, 0.002144155092537403, 0.0017651690868660808, 0.9921305775642395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.5298086155962665e-06, 4.0131671319443285e-09, 4.275891196670045e-09, 9.052337546222589e-09, 5.13516738465114e-07, 3.7317597616493003e-06, 1.4824256311385398e-07, 4.6551231207558885e-06, 1.663049964406582e-08, 3.029689423783566e-06, 3.1919844332151115e-05, 1.4017466298810177e-07, 4.954943051416194e-07, 2.3671118754009512e-07, 7.952932605803653e-07, 0.000186073113582097, 2.4466780814691447e-06, 1.1982333489868324e-05, 0.0005082354182377458, 0.9992390871047974, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009417894296348095, 0.00017362396465614438, 0.0001959324727067724, 0.00010542462405283004, 0.00012100082676624879, 0.00013750330253969878, 0.0002082236169371754, 0.00026208811323158443, 0.000980383949354291, 0.00017425217083655298, 0.0013313931412994862, 0.0032773094717413187, 0.00484062172472477, 0.012378803454339504, 0.0018258970230817795, 0.024282075464725494, 0.054799724370241165, 0.07428494095802307, 0.07759270817041397, 0.10845349729061127, 0.6251566410064697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000178863454493694, 2.971773938043043e-06, 1.2879254427389242e-06, 1.6619757161606685e-06, 3.1533977562503424e-06, 7.6085689215688035e-06, 4.4805804577663366e-07, 4.628855549526634e-06, 4.462097422219813e-06, 4.640843144443352e-06, 4.6781337914580945e-06, 2.2255213480093516e-05, 0.0006885040202178061, 5.8808673202292994e-05, 5.5113083362812176e-05, 0.0013147720601409674, 0.0004210430779494345, 0.0005279311444610357, 0.0003059168520849198, 0.001136304810643196, 0.004935148637741804, 0.9903199076652527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000900082231964916, 3.460905281826854e-05, 6.000087523716502e-06, 1.3317927596290247e-06, 8.382406122109387e-06, 7.950196959427558e-06, 8.35297214507591e-07, 5.017450803279644e-06, 1.1085595360782463e-05, 1.5506257113884203e-05, 2.2449734387919307e-05, 3.2394709705840796e-05, 7.153376645874232e-05, 0.00011106151214335114, 6.552051218022825e-06, 3.557859599823132e-05, 0.00046084364294074476, 0.0006021016160957515, 0.007735592313110828, 0.004731390625238419, 0.012920089066028595, 0.016999131068587303, 0.9552805423736572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.094544318737462e-05, 5.419524313765578e-05, 2.1191066480241716e-05, 8.660334174237505e-07, 5.241396593191894e-06, 2.8114016004110454e-06, 9.64769242273178e-07, 4.259620254742913e-06, 6.819979262218112e-06, 2.6370205432613147e-06, 1.038735263136914e-05, 3.11777948809322e-05, 2.5747049221536145e-05, 8.829358557704836e-05, 0.0001053006635629572, 0.00037444860208779573, 0.0005833805771544576, 0.0009019349818117917, 0.00043120209011249244, 0.004991126712411642, 0.0035159201361238956, 0.01512744091451168, 0.018385024741292, 0.9552487730979919, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00019926755339838564, 5.897874757465615e-07, 4.093071765964851e-07, 5.283640120978816e-07, 4.0437271309201606e-07, 6.811446837673429e-07, 2.501970755020011e-07, 3.1909843301036744e-07, 5.017996045353357e-06, 3.059945129280095e-06, 4.776871264766669e-06, 8.10623441793723e-06, 1.7871028830995783e-05, 5.9049270930700004e-05, 7.01949011272518e-06, 5.101956412545405e-05, 0.00015419701230712235, 0.00015557766892015934, 0.00046309534809552133, 0.00018736724450718611, 0.0029354426078498363, 0.006998678669333458, 0.0048950533382594585, 0.010302477516233921, 0.9735496640205383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [4.041911233798601e-05, 7.668173651609322e-09, 6.183993406239097e-08, 4.1099784908738e-08, 3.161339350299386e-07, 1.0004049499912071e-07, 6.658543583171195e-08, 4.780272320203949e-07, 4.0351764596380235e-08, 6.753261914127506e-06, 8.574332355237857e-08, 1.0514292370089606e-07, 3.2702891985536553e-06, 4.896564860246144e-07, 1.2450329904822866e-06, 9.069854058907367e-06, 2.0693230453616707e-06, 8.541727765987162e-06, 4.513621024671011e-05, 8.528203761670738e-05, 8.476253424305469e-05, 0.001026791986078024, 0.0013347762869670987, 0.0002960075216833502, 0.005626798607409, 0.9914274215698242, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003747464797925204, 1.2430068352387025e-07, 2.5229229549950105e-07, 3.0524401495313214e-07, 4.772520242113387e-06, 6.07545359798678e-07, 8.501752404299623e-07, 1.1030846508219838e-05, 8.099028292463117e-08, 9.651349728301284e-07, 7.193378905867576e-07, 3.2339468702957674e-07, 1.7605460698177922e-06, 4.4468217197390913e-07, 3.298759736480861e-07, 2.430029599054251e-05, 3.664281848614337e-06, 3.061651341340621e-06, 2.627137837407645e-05, 0.000342672283295542, 2.078812030958943e-05, 0.0001340969611192122, 0.00030472385697066784, 9.803616558201611e-05, 0.000491970800794661, 0.004237079527229071, 0.9939159750938416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00023197698465082794, 7.363636314039468e-07, 1.0658079645509133e-06, 4.749825848193723e-07, 2.187795189456665e-06, 2.0741754269693047e-06, 1.3842171142641746e-07, 2.5106478460656945e-06, 1.971606593542674e-07, 6.6720253926177975e-06, 8.991880235953431e-07, 6.3622513835071e-07, 6.544833581756393e-07, 7.717996481915179e-07, 4.801893851436034e-07, 1.112232894229237e-05, 5.561962552746991e-06, 3.862775884044822e-06, 6.753741763532162e-05, 0.0002897970553021878, 4.372495095594786e-05, 0.00015209040429908782, 0.000933773466385901, 0.00018159473256673664, 8.423367398791015e-05, 0.009933888912200928, 0.008723213337361813, 0.9793182611465454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0001570491149323061, 3.1231945740728406e-07, 2.9319556915652356e-07, 3.137171233902336e-07, 2.196102059315308e-06, 1.4164409094519215e-06, 1.123646370615461e-06, 1.3091807886667084e-06, 5.921536327946342e-08, 7.978140388331667e-07, 2.8036893127136864e-06, 1.976706727191413e-07, 3.9940246665537416e-07, 2.3049075537073804e-07, 2.264254362671636e-06, 1.6729487470001914e-05, 1.6667722775309812e-06, 2.872556478905608e-06, 2.494707950972952e-05, 0.0002553280210122466, 4.849698143516434e-06, 4.7080549848033115e-05, 5.3256633691489697e-05, 4.2403480620123446e-05, 0.00020874949404969811, 0.0020438111387193203, 0.08346746861934662, 0.003672998398542404, 0.9099870324134827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010030758567154408, 7.605900009366451e-06, 5.4789093155704904e-06, 4.704091224994045e-06, 1.0818222335728933e-06, 7.24683445696428e-07, 6.057232440070948e-07, 1.584784513397608e-06, 3.183100852766074e-05, 1.8444936813466484e-06, 1.3723802112508565e-06, 6.841031790827401e-06, 4.996106781618437e-06, 0.0001422043569618836, 2.102596226905007e-06, 2.046113513642922e-05, 6.875484541524202e-05, 8.339457417605445e-05, 7.060845382511616e-05, 0.00022270069166552275, 0.0013453485444188118, 0.0011953078210353851, 0.0007637696689926088, 0.003412771038711071, 0.009595179930329323, 0.008245948702096939, 0.004028419964015484, 0.015673460438847542, 0.037109121680259705, 0.9169486165046692, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0001459809427615255, 1.3442399904306512e-07, 3.067967782044434e-07, 1.270082918836124e-07, 9.539672873870586e-08, 4.421365318307835e-08, 7.270988255925204e-09, 2.1630607704992144e-07, 1.7861742662717006e-07, 1.1440992864208965e-07, 4.546695819840352e-08, 1.1406707045580333e-07, 1.784635088597497e-07, 8.56869121435011e-07, 1.0497423374999926e-07, 1.247496129508363e-06, 1.1682684544211952e-06, 6.484991104116489e-07, 9.92303012026241e-06, 2.6635707399691455e-05, 2.148093335563317e-05, 7.192340854089707e-05, 0.0001587330480106175, 0.0001456322061130777, 0.0024261639919131994, 0.001514493371360004, 0.0007063159719109535, 0.0022442759945988655, 0.004566814284771681, 0.00983231607824564, 0.9781238436698914, 0.0, 0.0, 0.0, 0.0], [1.0698723599489313e-05, 5.128410052179788e-08, 7.382474365158487e-08, 9.797478384143687e-08, 4.944864713252173e-07, 6.269694949878613e-07, 2.6257296781295736e-07, 7.361913390013797e-07, 2.5101115852521616e-07, 2.1764856228401186e-06, 5.363742161534901e-07, 8.60690761328442e-07, 2.151568878616672e-07, 1.0358788813391584e-06, 2.489502151092893e-07, 1.348100613540737e-05, 8.521091331203934e-06, 5.1484780669852626e-06, 4.645082753995666e-06, 1.6832595065352507e-05, 5.864617560291663e-05, 3.65243831765838e-05, 0.00016485668311361223, 1.194493415823672e-05, 0.000517399690579623, 0.002427901839837432, 0.000347734458046034, 0.0015963994665071368, 0.009022909216582775, 0.013163672760128975, 0.06333836168050766, 0.9092468023300171, 0.0, 0.0, 0.0], [0.0012287287972867489, 8.40761094877962e-06, 8.363969755009748e-06, 8.199012881959789e-06, 3.972171271016123e-06, 3.987261152360588e-06, 1.5942558775350335e-06, 4.764982350025093e-06, 4.461928256205283e-06, 8.699234058440197e-06, 3.482304691715399e-06, 4.087235083716223e-06, 4.383190571388695e-06, 1.287867416976951e-05, 2.7649919047689764e-06, 1.15508446469903e-05, 2.8508407922345214e-05, 1.972621066670399e-05, 8.007956785149872e-05, 0.00016381582827307284, 0.00027660917839966714, 0.0004829954414162785, 0.0005902999546378851, 0.0011750637786462903, 0.0034667181316763163, 0.0058027212508022785, 0.01190753374248743, 0.016952943056821823, 0.028466491028666496, 0.06137571856379509, 0.11517622321844101, 0.2017771452665329, 0.5509370565414429, 0.0, 0.0], [3.7825258914381266e-05, 6.459638512978927e-08, 1.9273693752097643e-08, 7.975797444714772e-08, 6.700324206576624e-07, 1.246432361767802e-07, 3.072176824048256e-08, 3.367149190580676e-07, 9.954024449143617e-08, 4.541541898106516e-07, 3.406029591701554e-08, 1.7256104456464527e-07, 3.3590319503673527e-07, 3.2563372087679454e-07, 1.1636628549638317e-08, 3.204218955943361e-06, 1.2533561175587238e-06, 1.766302261785313e-06, 1.1126005119876936e-05, 2.0410334400366992e-05, 8.103408617898822e-06, 1.288108524022391e-05, 0.0001419317995896563, 1.1411138984840363e-05, 0.00010671347990864888, 0.00010951135482173413, 0.0017927142325788736, 0.0015404942678287625, 0.0012465264881029725, 0.0015975917922332883, 0.0008927072049118578, 0.009557013399899006, 0.009279933758080006, 0.9736242294311523, 0.0], [0.0003539698664098978, 2.6440338842803612e-06, 1.7576132904650876e-06, 1.2348087921054685e-06, 5.9497183428902645e-06, 1.3091863593217568e-06, 3.3621145689721743e-07, 2.485059269474732e-07, 6.705469672851905e-07, 6.69955397825106e-08, 6.061932253942359e-07, 4.6190655211830745e-07, 2.3573336420668056e-06, 1.4622213484472013e-06, 2.976374275931448e-07, 1.6005814131858642e-06, 2.4609105366835138e-06, 1.93172331819369e-06, 3.200156061211601e-05, 2.4652055799379013e-05, 3.429633579798974e-05, 0.0001655086816754192, 7.883979560574517e-05, 0.0001838721364038065, 0.0011651571840047836, 0.00012255371257197112, 0.0011725208023563027, 0.000498478184454143, 0.0035014839377254248, 0.0034007492940872908, 0.011839527636766434, 0.005179098807275295, 0.016713880002498627, 0.02912774868309498, 0.9263800978660583]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9238101243972778, 0.07618981599807739, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3758925497531891, 0.10050603002309799, 0.5236014127731323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41213729977607727, 0.08231593668460846, 0.18084752559661865, 0.3246992230415344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14052297174930573, 0.005812325980514288, 0.028276383876800537, 0.057330355048179626, 0.7680580019950867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01923070102930069, 0.0014746838714927435, 0.001681080088019371, 0.004905839450657368, 0.025906629860401154, 0.9468010663986206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006040878593921661, 0.0046582212671637535, 0.01286506187170744, 0.00895391870290041, 0.07021687179803848, 0.12025370448827744, 0.7770113348960876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016248302534222603, 0.003929387778043747, 0.012836681678891182, 0.005099628586322069, 0.017354195937514305, 0.015599342994391918, 0.13602763414382935, 0.7929048538208008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13423295319080353, 0.07014793902635574, 0.03862926736474037, 0.03592688590288162, 0.057204898446798325, 0.06419355422258377, 0.03908979892730713, 0.21457335352897644, 0.34600138664245605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0035958827938884497, 0.00038495095213875175, 0.000492166553158313, 0.0008088783943094313, 0.002590097486972809, 0.002407651161774993, 0.00017131846107076854, 0.011153020896017551, 0.002114784438163042, 0.9762810468673706, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00022834520495962352, 4.666265158448368e-05, 8.206469647120684e-05, 7.730667130090296e-05, 0.00012034232349833474, 0.004319248255342245, 0.0038442229852080345, 0.0011965009616687894, 0.00016446977679152042, 0.8530563116073608, 0.13686440885066986, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06578908115625381, 0.01929076761007309, 0.02049270272254944, 0.021956797689199448, 0.026686688885092735, 0.08618329465389252, 0.031040852889418602, 0.1610301434993744, 0.03016255795955658, 0.21323665976524353, 0.0755448192358017, 0.2485855519771576, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009123365394771099, 0.003399323206394911, 0.016818294301629066, 0.004648821894079447, 0.014602291397750378, 0.05833258107304573, 0.07909008860588074, 0.01766364276409149, 0.016087308526039124, 0.1505354940891266, 0.33433887362480164, 0.05438903346657753, 0.2409709095954895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04253435879945755, 0.01655060239136219, 0.007813411764800549, 0.006833473686128855, 0.009650965221226215, 0.009173309430480003, 0.005910307634621859, 0.030093086883425713, 0.05324167758226395, 0.03575892001390457, 0.11968663334846497, 0.07133760303258896, 0.29692235589027405, 0.29449328780174255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010158059187233448, 0.004377428442239761, 0.006503197830170393, 0.0035466912668198347, 0.006157065741717815, 0.005149825010448694, 0.004158251453191042, 0.01499405037611723, 0.017724139615893364, 0.008609665557742119, 0.013557291589677334, 0.027689864858984947, 0.027776196599006653, 0.08518611639738083, 0.7644122242927551, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005760944331996143, 3.7584886740660295e-05, 3.100571484537795e-05, 2.8090833438909613e-05, 0.00011757229367503896, 0.0010626487201079726, 0.0008489767205901444, 0.002364430110901594, 0.00031711842166259885, 0.00050407147500664, 0.0005438963999040425, 0.0009964656783267856, 0.004787206184118986, 0.00165821542032063, 0.0211811400949955, 0.9649454951286316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05710114166140556, 0.012982730753719807, 0.012463340535759926, 0.012371203862130642, 0.013064758852124214, 0.03530248999595642, 0.013208801858127117, 0.06362835317850113, 0.010588893666863441, 0.06113201379776001, 0.027357198297977448, 0.08200795948505402, 0.03997460752725601, 0.0430283360183239, 0.06893416494131088, 0.0673389732837677, 0.3795149326324463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05038188770413399, 0.010253241285681725, 0.009017403237521648, 0.007853332906961441, 0.008382110856473446, 0.01603054814040661, 0.00757974898442626, 0.016568152233958244, 0.009558066725730896, 0.020098432898521423, 0.028027700260281563, 0.031728412955999374, 0.021793946623802185, 0.03942326083779335, 0.11988706141710281, 0.04783086106181145, 0.14773671329021454, 0.4078490734100342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009583775885403156, 0.0005122189177200198, 0.0006106910295784473, 0.0009717863285914063, 0.0022610982414335012, 0.004041247069835663, 0.0003397595719434321, 0.002301478525623679, 0.0014669783413410187, 0.004360869061201811, 0.01003954466432333, 0.003883783705532551, 0.0019065338419750333, 0.004968359135091305, 0.003599874209612608, 0.002795653883367777, 0.015628769993782043, 0.014497004449367523, 0.9162304997444153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03377573564648628, 0.0008472923072986305, 0.0009361549746245146, 0.000696756353136152, 0.004600136075168848, 0.0032575952354818583, 0.00042183580808341503, 0.007556704804301262, 0.001057057874277234, 0.0036437890958040953, 0.0001782385807018727, 0.0020456204656511545, 0.0014812088338658214, 0.0029180459678173065, 0.005684820935130119, 0.06616658717393875, 0.006820711772888899, 0.005966124590486288, 0.022306203842163086, 0.8296393752098083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.014338125474750996, 0.004506282042711973, 0.002293481957167387, 0.0045026326552033424, 0.006348531227558851, 0.0064406972378492355, 0.0030759202782064676, 0.003429379081353545, 0.008825034834444523, 0.013563874177634716, 0.008806227706372738, 0.015333067625761032, 0.010930368676781654, 0.03256241977214813, 0.041650112718343735, 0.02935846708714962, 0.06823205947875977, 0.10101237148046494, 0.12824423611164093, 0.08141812682151794, 0.4151286780834198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01251695305109024, 0.003707381896674633, 0.001132362405769527, 0.0020440148655325174, 0.0021037987899035215, 0.000721755379345268, 0.0003587713581509888, 0.0010654565412551165, 0.0033418647944927216, 0.0035347705706954002, 0.0018008281476795673, 0.005131383426487446, 0.005956836044788361, 0.011681443080306053, 0.023353110998868942, 0.024626336991786957, 0.023082561790943146, 0.024053940549492836, 0.04505406320095062, 0.04447479546070099, 0.09848437458276749, 0.6617732048034668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02337808534502983, 0.0029700961895287037, 0.0022721667774021626, 0.0031212572939693928, 0.010448778979480267, 0.0027957085985690355, 0.0009500714368186891, 0.006640295498073101, 0.0025093036238104105, 0.006215901114046574, 0.001875662594102323, 0.004366477020084858, 0.005894932430237532, 0.007904255762696266, 0.011842509731650352, 0.011780333705246449, 0.01693400740623474, 0.015606731176376343, 0.03699200227856636, 0.05676274374127388, 0.0388517752289772, 0.3010689318180084, 0.4288180470466614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04225088655948639, 0.009647681377828121, 0.013069686479866505, 0.003493931842967868, 0.008718252182006836, 0.000871763622853905, 0.0021130701061338186, 0.002520158654078841, 0.0028259316459298134, 0.003056258661672473, 0.005475999321788549, 0.0044675907120108604, 0.00596621073782444, 0.008145671337842941, 0.027670931071043015, 0.013382655568420887, 0.018189139664173126, 0.03278028592467308, 0.028848420828580856, 0.059959590435028076, 0.09605948626995087, 0.20931921899318695, 0.1013825386762619, 0.29978471994400024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.025818122550845146, 0.001245307270437479, 0.0012367774033918977, 0.0031558326445519924, 0.0026054636109620333, 0.001117834122851491, 0.00023053327458910644, 0.001061457791365683, 0.0012553023407235742, 0.0014470256865024567, 0.000608304631896317, 0.002144439844414592, 0.0017222786555066705, 0.003063683398067951, 0.002130721462890506, 0.0017395236063748598, 0.007489579264074564, 0.004201186820864677, 0.016367346048355103, 0.01718759536743164, 0.01664706878364086, 0.06787336617708206, 0.07546132802963257, 0.04369651898741722, 0.7004933953285217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007508304435759783, 0.00012913935643155128, 0.00022904067009221762, 0.00030994004919193685, 0.001587573322467506, 0.00015740803792141378, 0.0003654663742054254, 0.0002631951356306672, 0.00024618758470751345, 0.004963845480233431, 0.0008493289351463318, 0.000205424745217897, 0.003599663032218814, 0.0006757525261491537, 0.0006743195117451251, 0.002866035560145974, 0.0007401983020827174, 0.001241066725924611, 0.003433937905356288, 0.0033488073386251926, 0.005896284244954586, 0.015315237455070019, 0.013584435917437077, 0.0038060685619711876, 0.1058751568198204, 0.8221281170845032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015146821737289429, 0.00028372701490297914, 0.00043238868238404393, 0.0012392137432470918, 0.002660065423697233, 0.0003459533036220819, 0.00029542643460445106, 0.0018772061448544264, 0.0002369122812524438, 0.00013219205720815808, 0.0003354805812705308, 0.00022997894848231226, 0.00027149051311425865, 0.0004209537000861019, 0.0008192581008188426, 0.0003039473376702517, 0.0006469351937994361, 0.00038042658707126975, 0.002392594702541828, 0.0029767989180982113, 0.002575610065832734, 0.005941804964095354, 0.005901204887777567, 0.005208780989050865, 0.025423694401979446, 0.023241205140948296, 0.9002799987792969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.020882228389382362, 0.0006866269977763295, 0.0014190329238772392, 0.001624725991860032, 0.005466739647090435, 0.0019930703565478325, 0.00034445623168721795, 0.0014866318088024855, 0.0003860282595269382, 0.00029868114506825805, 0.0003715489583555609, 0.000533250451553613, 0.00025670809554867446, 0.0006083360058255494, 0.00037862645694985986, 0.0013748244382441044, 0.0013781337765976787, 0.0009236677433364093, 0.023375023156404495, 0.03773239254951477, 0.004063986707478762, 0.02184663526713848, 0.018512386828660965, 0.0050472067669034, 0.018440239131450653, 0.06399473547935486, 0.33002766966819763, 0.4365463852882385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0027297891210764647, 0.00011315740994177759, 0.0002574319951236248, 0.00030652922578155994, 0.0008012373582459986, 0.0008870931924320757, 0.00010111578012583777, 0.002048567635938525, 0.00014291208935901523, 0.00019898093887604773, 0.00043098197784274817, 7.435757288476452e-05, 0.00011077841918449849, 0.00022297597024589777, 0.0001040012066368945, 0.0007566198473796248, 0.00018428810290060937, 0.00031021024915389717, 0.001059030182659626, 0.005394390318542719, 0.0013711405918002129, 0.0017238268628716469, 0.0034383481834083796, 0.0007791522657498717, 0.002710791537538171, 0.011246607638895512, 0.2618612051010132, 0.042864639312028885, 0.6577697396278381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007169229444116354, 0.0026512308977544308, 0.0024450267665088177, 0.003023250261321664, 0.0026861869264394045, 0.0007135473424568772, 0.0005127684562467039, 0.0003864864120259881, 0.0009532010299153626, 0.00044078505015932024, 0.0008591762161813676, 0.0012491066008806229, 0.000942479120567441, 0.0016617265064269304, 0.00044353186967782676, 0.0014648443320766091, 0.00357746216468513, 0.003767339512705803, 0.01212294865399599, 0.006853932980448008, 0.005249565001577139, 0.019704265519976616, 0.02666921727359295, 0.04096020385622978, 0.2385396808385849, 0.05725717917084694, 0.04040535166859627, 0.10933995991945267, 0.07736308872699738, 0.3305872678756714, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011626670137047768, 0.001965815667062998, 0.0009179528569802642, 0.0018197415629401803, 0.00207897718064487, 0.00029357083258219063, 0.00010028542601503432, 0.0005799978389404714, 0.00048541201977059245, 9.764834976522252e-05, 0.00015712756430730224, 0.0004106423584744334, 0.0004877772880718112, 0.0007517864578403533, 0.0007532841991633177, 0.001224213163368404, 0.0010826829820871353, 0.0014821896329522133, 0.0027742518577724695, 0.0036103371530771255, 0.002137241419404745, 0.009361458010971546, 0.014277663081884384, 0.011846703477203846, 0.04729330539703369, 0.04506278783082962, 0.04156062752008438, 0.04280785843729973, 0.0707978680729866, 0.10084809362888336, 0.5813060402870178, 0.0, 0.0, 0.0, 0.0], [0.0038734280969947577, 0.0002557096304371953, 0.0006259240326471627, 0.000584396009799093, 0.0012326451251283288, 0.0003945587086491287, 0.00033266725949943066, 0.00015581923071295023, 0.00016472706920467317, 0.000772628525737673, 0.00038669706555083394, 0.0002782439114525914, 0.00015239689673762769, 0.00025198564981110394, 6.452909292420372e-05, 0.0001726415503071621, 0.0007133570616133511, 0.0005384791293181479, 0.007539561949670315, 0.006723243277519941, 0.0013963889796286821, 0.0031103515066206455, 0.005501281004399061, 0.002413584850728512, 0.03896051645278931, 0.02342231012880802, 0.00964110717177391, 0.025750890374183655, 0.12702466547489166, 0.040059175342321396, 0.5142093896865845, 0.18329662084579468, 0.0, 0.0, 0.0], [0.015151141211390495, 0.0013760692672803998, 0.0015560523606836796, 0.002171711763367057, 0.0025110095739364624, 0.0017854925245046616, 0.0004700868157669902, 0.0006788133177906275, 0.00038990782923065126, 0.0009900383884087205, 0.00038026607944630086, 0.0004182500415481627, 0.00044352965778671205, 0.0005140142166055739, 0.00016592226165812463, 0.0006932567339390516, 0.0009629512205719948, 0.0007656949455849826, 0.004365230910480022, 0.0064362757839262486, 0.0018688251730054617, 0.008356158621609211, 0.006384753622114658, 0.0058907074853777885, 0.031186997890472412, 0.04100947827100754, 0.028566041961312294, 0.0415772907435894, 0.040764592587947845, 0.06712842732667923, 0.30426743626594543, 0.21618834137916565, 0.16458529233932495, 0.0, 0.0], [0.017354128882288933, 0.0005264718784019351, 0.0006208935519680381, 0.0016926300013437867, 0.006297261919826269, 0.001020209863781929, 7.362545875366777e-05, 0.0006762279663234949, 0.00044483086094260216, 7.778868166496977e-05, 4.9738013331079856e-05, 0.0003035976260434836, 0.00017341812781523913, 0.0005162443849258125, 0.0002735276648309082, 0.0007444007787853479, 0.000602223037276417, 0.0004544504336081445, 0.0026653415989130735, 0.0026638817507773638, 0.002551596611738205, 0.007818805053830147, 0.009490977972745895, 0.000983041594736278, 0.007624226622283459, 0.011552354320883751, 0.09579724073410034, 0.08069920539855957, 0.09889834374189377, 0.03799615800380707, 0.05835627764463425, 0.11718921363353729, 0.1165681704878807, 0.3172434866428375, 0.0], [0.010796362534165382, 0.0030184658244252205, 0.0017010848969221115, 0.002761401701718569, 0.00554646085947752, 0.0005509491311386228, 0.00022208366135600954, 0.00023305918148253113, 0.0008555084350518882, 8.429425361100584e-05, 0.00013491048594005406, 0.00040303461719304323, 0.00021568576630670577, 0.000912235991563648, 0.0007261987775564194, 0.00038188620237633586, 0.0007406813674606383, 0.0007301236619241536, 0.000785231648478657, 0.001347342156805098, 0.004313783720135689, 0.007729183416813612, 0.0054228100925683975, 0.006747082341462374, 0.013981964439153671, 0.00463099405169487, 0.01939181052148342, 0.028133733198046684, 0.016861943528056145, 0.07724323123693466, 0.05435170233249664, 0.13139021396636963, 0.15014062821865082, 0.08434376120567322, 0.3631700575351715]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1932608187198639, 0.8067392110824585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09132399410009384, 0.0022218949161469936, 0.9064540863037109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07127532362937927, 0.010394584387540817, 0.014608730562031269, 0.9037213325500488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007525183726102114, 8.942795830080286e-05, 7.544160325778648e-05, 3.4135832720494363e-06, 0.9923065304756165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004521043971180916, 4.280504981579725e-06, 9.230425348505378e-05, 2.3636344849364832e-05, 1.1194412763870787e-05, 0.9953475594520569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007938165217638016, 0.00010465314699104056, 4.1882718505803496e-05, 8.376336154469755e-06, 0.00021111505338922143, 3.1298111480282387e-06, 0.9916926622390747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0024528391659259796, 1.0800486052175984e-05, 1.5410727428388782e-05, 2.252150608228476e-07, 3.925124474335462e-05, 5.133591002959292e-06, 2.3800430426490493e-05, 0.9974525570869446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.199448361992836, 0.17831814289093018, 0.06548026204109192, 0.13276462256908417, 0.038584526628255844, 0.010353410616517067, 0.021433593705296516, 0.0067232949659228325, 0.3468938171863556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0014458447694778442, 8.054742011154303e-07, 3.064594420720823e-05, 7.412907052639639e-07, 2.027408299909439e-06, 1.1259035090915859e-05, 1.8223431652586441e-06, 8.012940816115588e-05, 1.0569654307346354e-07, 0.9984266757965088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0023380285128951073, 1.2498725482146256e-05, 2.245021278213244e-05, 4.23650863012881e-06, 2.952040460968419e-07, 1.894714500849659e-06, 7.769888179609552e-05, 8.96909932635026e-06, 4.6952177257253425e-08, 1.1347711961207096e-06, 0.9975327253341675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04751468449831009, 0.05268266424536705, 0.020408153533935547, 0.0585300549864769, 0.007166562136262655, 0.008201425895094872, 0.01184640359133482, 0.0017474405467510223, 0.02496333420276642, 0.0026972563937306404, 0.0014261914184316993, 0.7628158330917358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0060708047822117805, 0.00012957924627698958, 5.943352516624145e-05, 5.659351245412836e-06, 0.00026153220096603036, 0.00012241833610460162, 1.0399337043054402e-05, 1.1076394002884626e-05, 4.060484570800327e-07, 0.00036004773573949933, 2.0115414372412488e-05, 1.364392119285185e-07, 0.9929484128952026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09530565142631531, 0.1418033391237259, 0.0503818616271019, 0.10299671441316605, 0.02981209009885788, 0.006589415017515421, 0.016824401915073395, 0.004481863230466843, 0.23106449842453003, 0.019435452297329903, 0.010874406434595585, 0.05543206259608269, 0.03543873503804207, 0.19955962896347046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02744772657752037, 0.00014065334107726812, 0.00019846615032292902, 3.496767385513522e-05, 5.282139227347216e-06, 0.0001502416271250695, 8.128343324642628e-05, 1.2125150533393025e-05, 6.303888949332759e-05, 6.543209019582719e-05, 2.8324825507297646e-06, 3.890723382937722e-05, 5.673621785717842e-07, 4.210475890431553e-05, 0.9717162847518921, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004212109372019768, 3.069686135859229e-05, 6.369437323883176e-05, 8.254548902186798e-07, 8.262109645329474e-07, 3.8683352613588795e-05, 4.268807515472872e-06, 3.81940662919078e-06, 1.2627977241663757e-07, 0.00013782712630927563, 1.7806096366257407e-05, 1.71001346416233e-08, 5.979866728011984e-06, 6.190298051933496e-08, 2.2941530914977193e-06, 0.9954808950424194, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019846832379698753, 0.03168349340558052, 0.012553294189274311, 0.03571886941790581, 0.004399022087454796, 0.004350584000349045, 0.00742737902328372, 0.0009372902568429708, 0.012662907131016254, 0.0016050575068220496, 0.0007319969008676708, 0.4563405513763428, 0.002010875381529331, 0.009759284555912018, 0.008237519301474094, 0.0002594130055513233, 0.391475647687912, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.027297193184494972, 0.0024357105139642954, 0.0002706825325731188, 7.525584078393877e-05, 1.3686845704796724e-05, 5.7732436289370526e-06, 0.0004301849694456905, 1.4438195648835972e-05, 4.835145591641776e-05, 5.3261264838511124e-05, 9.613345355319325e-06, 6.846706673968583e-05, 0.0003037687565665692, 2.878394298022613e-05, 3.8664089515805244e-05, 2.592686541902367e-05, 4.680059646489099e-05, 0.9688333868980408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006310705095529556, 3.294691487099044e-05, 9.89571344689466e-06, 7.334598421948613e-07, 9.119859169004485e-05, 4.094936957699247e-05, 8.511712621839251e-06, 9.950739331543446e-05, 1.9264851403022476e-07, 8.829235866869567e-07, 1.8906141576735536e-06, 1.1808717772510136e-06, 3.1165220661932835e-06, 9.474786111240974e-08, 3.3017674923030427e-06, 2.585527727205772e-05, 7.711314538028091e-07, 2.670870799192926e-06, 0.993365466594696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0011340517085045576, 6.9619100031559356e-06, 1.2364376743789762e-05, 4.006546987511683e-07, 5.3518149798037484e-05, 5.2400919230422005e-06, 4.459505726117641e-05, 0.00016450766997877508, 1.4344479382089048e-08, 1.0388012015027925e-06, 4.778879156219773e-05, 5.0586844935196495e-08, 9.468200801165949e-07, 7.47569206538401e-09, 6.603934252780164e-07, 3.843108061118983e-06, 3.346431753925572e-08, 2.411978528016334e-07, 1.6647753000142984e-05, 0.9985072016716003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08800850808620453, 0.18921294808387756, 0.04556255042552948, 0.08163291215896606, 0.017358070239424706, 0.004830404184758663, 0.01895877532660961, 0.004620140418410301, 0.06910264492034912, 0.012970475479960442, 0.010343033820390701, 0.05366411432623863, 0.009830277413129807, 0.05797470360994339, 0.011936408467590809, 0.009446632117033005, 0.0466233566403389, 0.07882611453533173, 0.01042740885168314, 0.002094675088301301, 0.17657585442066193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015971248969435692, 0.0030620258767157793, 0.000141895841807127, 0.0007885775994509459, 4.2993880924768746e-05, 4.131220339331776e-05, 6.306891009444371e-05, 3.546535253917682e-06, 2.1731357264798135e-05, 1.7861053493106738e-05, 1.0214035683020484e-05, 2.6596504540066235e-05, 3.785576336667873e-05, 1.3186921023589093e-05, 0.00018817732052411884, 4.9639011194813065e-06, 1.808516390156001e-05, 4.864036600338295e-05, 4.463607183424756e-06, 2.4802643565635663e-06, 1.2178949873487e-05, 0.9794787764549255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007472258061170578, 0.000371908099623397, 7.639107934664935e-05, 1.1135954991914332e-05, 3.014879985130392e-05, 1.1014471056114417e-05, 5.4528889450011775e-05, 2.2444526621256955e-05, 1.128388976212591e-05, 2.8588086934178136e-05, 5.627766313409666e-06, 7.513627497246489e-05, 7.73431675042957e-05, 6.529730399051914e-06, 8.79695016919868e-06, 8.651543794258032e-06, 5.072811109130271e-05, 7.040875971142668e-06, 3.1876501452643424e-05, 1.4034960713615874e-06, 7.00360806149547e-06, 7.040108175715432e-05, 0.9915596842765808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.022045455873012543, 0.2804943323135376, 0.01667480356991291, 0.000150871739606373, 0.0006804483709856868, 2.739423325692769e-05, 0.0008938326500356197, 1.2499222066253424e-05, 0.00026655345573090017, 5.5841687753854785e-06, 5.109099220135249e-05, 9.500436135567725e-05, 1.8194830772699788e-05, 0.00018282837118022144, 0.0016493609873577952, 2.7935253456234932e-05, 6.991920236032456e-05, 7.677562825847417e-05, 2.0886132915620692e-05, 4.5125689212e-06, 0.00014541414566338062, 0.0004470501735340804, 5.5147946113720536e-05, 0.6759039759635925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018799511715769768, 0.0019184657139703631, 0.001052388921380043, 7.228166214190423e-05, 0.00013796627172268927, 5.208820948610082e-05, 6.334387580864131e-05, 9.731994396133814e-06, 8.120827260427177e-05, 5.99839877395425e-05, 9.635993592382874e-06, 4.818722663912922e-05, 4.688366243499331e-05, 5.2505907660815865e-05, 0.00044942108797840774, 0.00028932970599271357, 3.537209704518318e-05, 0.0002237795270048082, 0.00020208988280501217, 3.112015747319674e-06, 3.522858605720103e-05, 9.855031385086477e-05, 0.00018101524619851261, 0.0001230081543326378, 0.9759548306465149, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001658982248045504, 6.6325210354989395e-06, 3.9323553210124373e-05, 5.512156803888502e-06, 7.328308129217476e-05, 5.176204354029323e-07, 8.362695371033624e-05, 1.6115496691782027e-05, 1.8408056234875403e-07, 0.00020032653992529958, 1.8497498786018696e-06, 7.683065206265383e-08, 1.8051065126201138e-05, 9.259789379711947e-08, 6.6121979216404725e-06, 1.9627939309430076e-06, 5.004875802683273e-08, 6.371137573069063e-08, 4.881658810518275e-07, 3.83916176360799e-06, 1.8601534534923303e-08, 1.6708722796465736e-06, 8.816641638986766e-07, 6.673368346810093e-08, 1.8785235056384408e-07, 0.9978796243667603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005040661431849003, 0.00039155513513833284, 0.0005618688883259892, 0.0001452303840778768, 3.937185101676732e-05, 5.409960067481734e-05, 0.0002847947762347758, 4.8287405661540106e-05, 3.1043139188113855e-06, 6.896261766087264e-05, 0.0002681407204363495, 4.210916813462973e-06, 1.2119861821702216e-05, 2.025393314397661e-06, 3.1887731893220916e-05, 1.1356505638104863e-05, 3.0187975426088087e-06, 1.2622870599443559e-05, 7.4655627031461336e-06, 3.027799039045931e-06, 4.951115215590107e-07, 3.015446782228537e-05, 1.8863744116970338e-05, 4.151234861637931e-06, 5.031317414250225e-06, 6.069071332603926e-06, 0.9929414987564087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012448701309040189, 0.00017617041885387152, 2.4151799152605236e-05, 1.2456716831366066e-05, 5.2113235142314807e-05, 2.3546110242023133e-05, 5.6734515965217724e-05, 0.00045827843132428825, 2.4983905859699007e-06, 0.00011349451233400032, 4.920252104056999e-05, 1.3444551768770907e-05, 7.735956751275808e-05, 1.3437110055747326e-06, 6.060916348360479e-06, 0.00010331803059671074, 8.745509148866404e-06, 1.0811575521074701e-05, 0.00015856418758630753, 1.9056084283874952e-06, 5.418805812951177e-07, 1.2321393114689272e-06, 6.472250970546156e-05, 3.5890639082936104e-06, 8.650950803712476e-06, 2.9628151878569042e-06, 1.3278987353260163e-05, 0.9973099231719971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0019733337685465813, 0.0009750574245117605, 7.218522659968585e-05, 9.84842554316856e-05, 4.960862861480564e-05, 5.4340627684723586e-05, 0.0003797203244175762, 6.184633093653247e-06, 1.0831149666046258e-06, 4.491211257118266e-06, 0.00047983592958189547, 9.928648978529964e-07, 1.3564545952249318e-05, 6.477191618614597e-07, 1.0186113286181353e-05, 5.581337518378859e-06, 7.113471269803995e-07, 5.998066399115487e-07, 3.201085064574727e-06, 2.288286395923933e-06, 1.5130815711472678e-07, 7.130754056561273e-06, 1.7937991287908517e-06, 2.4025706807151437e-05, 2.833118344369723e-07, 1.2269106264284346e-05, 0.0002332548756385222, 0.0003131958073936403, 0.9952758550643921, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03009926900267601, 0.09027383476495743, 0.028149748221039772, 0.15763868391513824, 0.026473550125956535, 0.00300600822083652, 0.01721954345703125, 0.002244671806693077, 0.031200474128127098, 0.011351732537150383, 0.001356132561340928, 0.02911914885044098, 0.016773540526628494, 0.02399810589849949, 0.009450534358620644, 0.006288200616836548, 0.023923030123114586, 0.058296240866184235, 0.009904223494231701, 0.0003779733960982412, 0.02459513023495674, 0.04042002558708191, 0.027886761352419853, 0.009823567233979702, 0.04994718357920647, 0.0010651162592694163, 0.0146325146779418, 0.011246389709413052, 0.0020601546857506037, 0.24117852747440338, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004169395659118891, 0.0006143266800791025, 0.0005938431131653488, 0.00011036181967938319, 9.005468018585816e-05, 1.3705081073567271e-05, 0.0005903452984057367, 2.6892232654063264e-06, 8.671460818732157e-06, 4.611608892446384e-05, 1.0056760402221698e-05, 5.362889169191476e-06, 5.199821316637099e-05, 4.802396233571926e-06, 2.1195128283579834e-05, 2.4784885681583546e-05, 3.4400250115140807e-06, 1.841071389208082e-05, 7.055104651954025e-05, 5.603938006970566e-07, 5.060160674474901e-06, 2.259204484289512e-05, 6.252520688576624e-05, 1.861925920820795e-05, 0.00014118508261162788, 2.63310139416717e-05, 1.2610375961230602e-06, 1.0759802080428926e-06, 3.093180396263051e-07, 5.793032869405579e-06, 0.9932645559310913, 0.0, 0.0, 0.0, 0.0], [0.010566690936684608, 0.0013905942905694246, 0.00043136379099451005, 0.0010295943357050419, 0.003916398622095585, 0.00015029536734800786, 0.0019935991149395704, 0.00030443823197856545, 2.98451504932018e-05, 0.0006592933204956353, 1.2659861567954067e-05, 7.527517300331965e-05, 0.0003976777661591768, 1.7276557628065348e-05, 0.0003858358249999583, 2.106194369844161e-05, 5.340324423741549e-05, 6.33502786513418e-05, 0.0001268847699975595, 6.7832088461727835e-06, 2.7170613975613378e-05, 0.0001300898875342682, 3.8598711398663e-05, 4.366658686194569e-05, 8.06410753284581e-06, 0.0007468677940778434, 8.195253940357361e-06, 9.368628525407985e-05, 6.252453658817103e-06, 4.447258106665686e-05, 0.00020698689331766218, 0.9770136475563049, 0.0, 0.0, 0.0], [0.017131472006440163, 0.10981197655200958, 0.04528495669364929, 0.1666400283575058, 0.04683487117290497, 0.012714335694909096, 0.02494179457426071, 0.0024605062790215015, 0.016514170914888382, 0.014018195681273937, 0.002804647898301482, 0.017261281609535217, 0.05619141831994057, 0.011843171902000904, 0.024518396705389023, 0.008004875853657722, 0.013582981191575527, 0.027696553617715836, 0.007881484925746918, 0.0013759613502770662, 0.009033627808094025, 0.03747997432947159, 0.014368822798132896, 0.016579650342464447, 0.03148626536130905, 0.0022351080551743507, 0.007032558787614107, 0.02033880352973938, 0.002157366368919611, 0.01573573797941208, 0.03529253602027893, 0.01779499091207981, 0.16295143961906433, 0.0, 0.0], [0.0027362436521798372, 0.0032704558689147234, 0.00024938586284406483, 2.9521193937398493e-05, 0.0027190831024199724, 1.5680860087741166e-05, 0.0011440095258876681, 3.528564047883265e-05, 1.9352110030013137e-05, 3.620967618189752e-05, 1.2202220204926562e-05, 0.00012145661457907408, 4.955609256285243e-05, 1.1851511771965306e-05, 8.82663152879104e-05, 3.1845251214690506e-05, 8.618939318694174e-05, 0.0001305781042901799, 9.267563291359693e-05, 1.844452526711393e-05, 4.454896043171175e-06, 1.4211035704647657e-05, 0.00020819867495447397, 6.224938988452777e-05, 2.7290067009744234e-05, 8.52709672471974e-06, 0.0010931716533377767, 0.001025898614898324, 1.8266413462697528e-05, 9.816341844270937e-06, 7.599981017847313e-07, 4.151821940467926e-06, 8.36590970720863e-06, 0.9866164922714233, 0.0], [0.013301728293299675, 0.002024176763370633, 0.000981063349172473, 0.0002710393164306879, 0.026242537423968315, 6.968403613427654e-05, 0.00018824616563506424, 0.00033232729765586555, 0.00020717488951049745, 0.0003558352473191917, 1.8688731870497577e-05, 0.0003101633337792009, 0.0002884419809561223, 0.00014157555415295064, 0.00021018910047132522, 0.00013730116188526154, 0.0002454251516610384, 0.0005858621443621814, 0.0008409079746343195, 0.00037898446316830814, 0.00011255484423600137, 0.0009485256741754711, 0.0002581718727014959, 0.0005371627630665898, 0.00023706417414359748, 9.89614927675575e-05, 1.4541798918799032e-05, 7.51436164136976e-05, 1.0142434803128708e-05, 7.690755592193455e-05, 6.643276719842106e-05, 5.445994975161739e-05, 0.00010782001481857151, 0.002189122373238206, 0.9480817317962646]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9360750913619995, 0.06392484903335571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8446658253669739, 0.06946855783462524, 0.08586560189723969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6426599621772766, 0.1298869103193283, 0.17117179930210114, 0.05628133937716484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5074959397315979, 0.07786272466182709, 0.0726853683590889, 0.10711855441331863, 0.2348373979330063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4970755875110626, 0.06315260380506516, 0.11559110134840012, 0.08851400017738342, 0.1019635871052742, 0.13370320200920105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3081282079219818, 0.06775975227355957, 0.13057544827461243, 0.05440935492515564, 0.040503207594156265, 0.3605913519859314, 0.03803271800279617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32434967160224915, 0.06349692493677139, 0.07783645391464233, 0.07798171043395996, 0.07410480827093124, 0.13590560853481293, 0.07038862258195877, 0.17593617737293243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16412927210330963, 0.03469284623861313, 0.05131261795759201, 0.01331701036542654, 0.17712697386741638, 0.15951886773109436, 0.09812306612730026, 0.29690471291542053, 0.004874553997069597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18775463104248047, 0.054880451411008835, 0.10758272558450699, 0.043910034000873566, 0.1406542807817459, 0.11696732044219971, 0.052460018545389175, 0.1536640077829361, 0.049352679401636124, 0.09277382493019104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20517104864120483, 0.04006878659129143, 0.0833921730518341, 0.04565275460481644, 0.05697011202573776, 0.1331988424062729, 0.026416417211294174, 0.09950847178697586, 0.05013459920883179, 0.214173823595047, 0.04531296715140343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10754448920488358, 0.020955439656972885, 0.05246850848197937, 0.008330853655934334, 0.17997516691684723, 0.10652028769254684, 0.07210797816514969, 0.20191164314746857, 0.003346037585288286, 0.1402132660150528, 0.1035674437880516, 0.0030589562375098467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09423347562551498, 0.022835390642285347, 0.055588558316230774, 0.02696787565946579, 0.12799426913261414, 0.07631111890077591, 0.07361144572496414, 0.16634677350521088, 0.030196376144886017, 0.11622446775436401, 0.1307915896177292, 0.022433562204241753, 0.056465160101652145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11027717590332031, 0.0199173204600811, 0.03387214615941048, 0.00801223423331976, 0.12686948478221893, 0.12052705883979797, 0.0660458579659462, 0.21159590780735016, 0.0029103830456733704, 0.13361379504203796, 0.07830417156219482, 0.005965670570731163, 0.07863973081111908, 0.003449020441621542, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16007713973522186, 0.05452116206288338, 0.05973628908395767, 0.020207533612847328, 0.05266043543815613, 0.10011058300733566, 0.11729248613119125, 0.14998750388622284, 0.01724836230278015, 0.05625277757644653, 0.094420425593853, 0.012046921998262405, 0.020297707989811897, 0.018244575709104538, 0.06689614057540894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17850042879581451, 0.04271136224269867, 0.05876290798187256, 0.025501534342765808, 0.028219768777489662, 0.09395943582057953, 0.035697732120752335, 0.03853301703929901, 0.0275740809738636, 0.12417224049568176, 0.0990019366145134, 0.029919512569904327, 0.04168915003538132, 0.030038554221391678, 0.0802922248840332, 0.06542609632015228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.082626111805439, 0.014221626333892345, 0.039103396236896515, 0.0058097802102565765, 0.1416105479001999, 0.08671104162931442, 0.05503488704562187, 0.15626998245716095, 0.0023281783796846867, 0.11679324507713318, 0.0806795060634613, 0.0021917943377047777, 0.09792865812778473, 0.0027368676383048296, 0.03297450765967369, 0.0803784430027008, 0.0026014975737780333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09858326613903046, 0.02832064963877201, 0.05263182148337364, 0.01785864867269993, 0.061636630445718765, 0.08070769160985947, 0.07397954165935516, 0.09560476243495941, 0.00933581218123436, 0.08467233926057816, 0.11269152909517288, 0.018467865884304047, 0.03856262192130089, 0.011051525361835957, 0.06964193284511566, 0.09237803518772125, 0.02256043441593647, 0.031314946711063385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14957912266254425, 0.013048060238361359, 0.03394850715994835, 0.015061775222420692, 0.045098863542079926, 0.08479971438646317, 0.02725699543952942, 0.17206279933452606, 0.01685032807290554, 0.10883236676454544, 0.11793850362300873, 0.008323254995048046, 0.016134323552250862, 0.017726484686136246, 0.026173410937190056, 0.05189087614417076, 0.00897607859224081, 0.010605727322399616, 0.0756928026676178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2303972691297531, 0.024296818301081657, 0.02655998431146145, 0.03924616426229477, 0.04230227321386337, 0.04924340546131134, 0.007979445159435272, 0.0732208639383316, 0.03455185517668724, 0.0660192146897316, 0.03830168768763542, 0.03672321140766144, 0.024140119552612305, 0.0371052511036396, 0.028557157143950462, 0.05446393042802811, 0.039015062153339386, 0.015944408252835274, 0.046100083738565445, 0.08583173900842667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05691669136285782, 0.006225016433745623, 0.011754430830478668, 0.0035496752243489027, 0.07110723853111267, 0.08006571233272552, 0.022162331268191338, 0.11016802489757538, 0.0010484913364052773, 0.08122197538614273, 0.026672055944800377, 0.00219607912003994, 0.030598530545830727, 0.0012014054227620363, 0.014937766827642918, 0.14736871421337128, 0.00262675853446126, 0.03338915482163429, 0.11049019545316696, 0.18504448235034943, 0.0012552256230264902, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09241703152656555, 0.026059595867991447, 0.024830611422657967, 0.014753981493413448, 0.07309965789318085, 0.041600797325372696, 0.028182942420244217, 0.10522366315126419, 0.010200555436313152, 0.026716867461800575, 0.03863626718521118, 0.008342113345861435, 0.034165360033512115, 0.011011885479092598, 0.02702193520963192, 0.10600946098566055, 0.009498467668890953, 0.016123106703162193, 0.06257016956806183, 0.19003154337406158, 0.010448912158608437, 0.04305499419569969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0617365799844265, 0.007029625121504068, 0.019486594945192337, 0.010366822592914104, 0.04601289704442024, 0.07236865162849426, 0.02275235764682293, 0.0897848829627037, 0.005041907541453838, 0.07019013911485672, 0.07947881519794464, 0.006475391332060099, 0.0383506678044796, 0.005680924281477928, 0.015840986743569374, 0.05344105511903763, 0.007524989545345306, 0.01886216178536415, 0.09784606099128723, 0.19087877869606018, 0.006985294166952372, 0.027727801352739334, 0.046136580407619476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04349850118160248, 0.006029130890965462, 0.016271816566586494, 0.004962893668562174, 0.09232776612043381, 0.05643602833151817, 0.02427317202091217, 0.10004431009292603, 0.0013475836021825671, 0.046952664852142334, 0.04336996749043465, 0.0026120897382497787, 0.02598000504076481, 0.0015367756132036448, 0.013811728917062283, 0.1216251403093338, 0.0030840584076941013, 0.014781134203076363, 0.08252865076065063, 0.20843428373336792, 0.002105808351188898, 0.03356635197997093, 0.04992126300930977, 0.004498819820582867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.054939351975917816, 0.0051646907813847065, 0.013282542116940022, 0.006816809996962547, 0.08581957221031189, 0.04258368909358978, 0.010111792013049126, 0.10901319980621338, 0.003067038021981716, 0.04195143282413483, 0.03668752685189247, 0.00387760391458869, 0.03639628365635872, 0.003642831463366747, 0.01571309193968773, 0.045142825692892075, 0.004770635161548853, 0.01073341816663742, 0.12949085235595703, 0.18343372642993927, 0.005763915833085775, 0.06008808687329292, 0.06420359760522842, 0.008479918353259563, 0.01882551982998848, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1161927878856659, 0.024008188396692276, 0.01750083453953266, 0.022795295342803, 0.05027654021978378, 0.032025471329689026, 0.02064776048064232, 0.03974737599492073, 0.02373596839606762, 0.026677895337343216, 0.01885642297565937, 0.028171371668577194, 0.034608837217092514, 0.026643721386790276, 0.022768087685108185, 0.023577360436320305, 0.03119923546910286, 0.022620445117354393, 0.09706810116767883, 0.06436187773942947, 0.03251434117555618, 0.04557204619050026, 0.04268258437514305, 0.028252901509404182, 0.02580956369638443, 0.08168504387140274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1122082993388176, 0.0052759298123419285, 0.013701370917260647, 0.0064756437204778194, 0.058257460594177246, 0.055918365716934204, 0.013027261011302471, 0.10451258718967438, 0.004013672471046448, 0.040113650262355804, 0.0378723219037056, 0.0031745261512696743, 0.012130004353821278, 0.004096264950931072, 0.00733453081920743, 0.017732838168740273, 0.0034752381034195423, 0.003059556009247899, 0.0948338434100151, 0.20002640783786774, 0.0056739673018455505, 0.01637454703450203, 0.017230216413736343, 0.004368441645056009, 0.009308070875704288, 0.09713461995124817, 0.05267036706209183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06115274503827095, 0.00493932981044054, 0.009762825444340706, 0.0052271755412220955, 0.03646274656057358, 0.031370531767606735, 0.008218890987336636, 0.11535760760307312, 0.0032728700898587704, 0.03824919834733009, 0.03337177634239197, 0.002179378177970648, 0.007692967541515827, 0.0034548162948340178, 0.009085447527468204, 0.027900518849492073, 0.002366595435887575, 0.00394824706017971, 0.0407714806497097, 0.27574673295021057, 0.003546668216586113, 0.01245510671287775, 0.0326131246984005, 0.00320982257835567, 0.0036030940245836973, 0.1000962108373642, 0.08892868459224701, 0.03501543775200844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11957049369812012, 0.008050516247749329, 0.013377287425100803, 0.007089266087859869, 0.025275399908423424, 0.05149083957076073, 0.006585449445992708, 0.06165987625718117, 0.005620475392788649, 0.055614881217479706, 0.02729181945323944, 0.004286492709070444, 0.007698436267673969, 0.00599626312032342, 0.007942020893096924, 0.02879052236676216, 0.004870972596108913, 0.004313761368393898, 0.026228079572319984, 0.15385541319847107, 0.010709959082305431, 0.009564644657075405, 0.029178373515605927, 0.005329284816980362, 0.008051520213484764, 0.07812701165676117, 0.10984006524085999, 0.02952101081609726, 0.09406983852386475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02588076703250408, 0.003286872524768114, 0.009190613403916359, 0.0022713211365044117, 0.04206255078315735, 0.0403556264936924, 0.015107065439224243, 0.04801337793469429, 0.0008096100646071136, 0.0390642024576664, 0.026566827669739723, 0.0015204473165795207, 0.02122015692293644, 0.0009539547027088702, 0.0078103747218847275, 0.033135175704956055, 0.0018792545888572931, 0.010119679383933544, 0.06751000881195068, 0.12262914329767227, 0.0015832239296287298, 0.022168025374412537, 0.030785169452428818, 0.004318806808441877, 0.018728986382484436, 0.1447751820087433, 0.06957436352968216, 0.1138874888420105, 0.07182324677705765, 0.0029684819746762514, 0.0, 0.0, 0.0, 0.0, 0.0], [0.046125125139951706, 0.004929371643811464, 0.011905021034181118, 0.005939908791333437, 0.05086230859160423, 0.03915583714842796, 0.011702257208526134, 0.07777240127325058, 0.0027224912773817778, 0.029330717399716377, 0.03543659672141075, 0.0025086565874516964, 0.011883250437676907, 0.002963858190923929, 0.00887866597622633, 0.018936673179268837, 0.002804588293656707, 0.004066925961524248, 0.06637168675661087, 0.16173596680164337, 0.004074740689247847, 0.017417889088392258, 0.048458125442266464, 0.005489854142069817, 0.01446738000959158, 0.0888427197933197, 0.05744140222668648, 0.04815756902098656, 0.09182098507881165, 0.004751026164740324, 0.023045988753437996, 0.0, 0.0, 0.0, 0.0], [0.02861655503511429, 0.0035397126339375973, 0.01282574888318777, 0.0037311071064323187, 0.022132832556962967, 0.026339076459407806, 0.015483646653592587, 0.046924490481615067, 0.001677017193287611, 0.053430892527103424, 0.03818729519844055, 0.00252948934212327, 0.027682267129421234, 0.0019447181839495897, 0.010427957400679588, 0.020306406542658806, 0.003083121497184038, 0.008337561041116714, 0.06147449091076851, 0.05695470795035362, 0.0030909187626093626, 0.019787289202213287, 0.0377042219042778, 0.004031912423670292, 0.020770905539393425, 0.12126164138317108, 0.08733703941106796, 0.07036231458187103, 0.09933110326528549, 0.0057986825704574585, 0.0638149082660675, 0.02107994444668293, 0.0, 0.0, 0.0], [0.022769255563616753, 0.002363811945542693, 0.006953611504286528, 0.0009745146962814033, 0.0334312878549099, 0.030963122844696045, 0.012881075032055378, 0.07639684528112411, 0.00026210362557321787, 0.030449913814663887, 0.03674256429076195, 0.0005670603131875396, 0.014569038525223732, 0.0003055334964301437, 0.005948363803327084, 0.022492578253149986, 0.0007025595405139029, 0.004866448231041431, 0.06026187539100647, 0.1558590829372406, 0.0005001768004149199, 0.019988059997558594, 0.03086845763027668, 0.0031281481496989727, 0.012151185423135757, 0.11994664371013641, 0.07597390562295914, 0.09628807008266449, 0.07584253698587418, 0.001064378535374999, 0.032713305205106735, 0.011111118830740452, 0.0006632999866269529, 0.0, 0.0], [0.06755813211202621, 0.003352789208292961, 0.008972199633717537, 0.0034025446511805058, 0.05607309192419052, 0.030404701828956604, 0.013628297485411167, 0.0795241966843605, 0.0025889386888593435, 0.034729354083538055, 0.01531597413122654, 0.00252709724009037, 0.00768546387553215, 0.0026928288862109184, 0.007112991064786911, 0.026128476485610008, 0.002773371059447527, 0.007069496437907219, 0.05076204985380173, 0.17593896389007568, 0.0039354367181658745, 0.01054285652935505, 0.012697330676019192, 0.0028314446099102497, 0.004983488470315933, 0.056103695183992386, 0.102786585688591, 0.07864401489496231, 0.06361377239227295, 0.004290641285479069, 0.012449054047465324, 0.007846898399293423, 0.003574160160496831, 0.03745957463979721, 0.0], [0.05889429524540901, 0.0048342859372496605, 0.0088162487372756, 0.004917604383081198, 0.05724448338150978, 0.047199662774801254, 0.010384350083768368, 0.04373685270547867, 0.003253212431445718, 0.027000509202480316, 0.015386014245450497, 0.0054444982670247555, 0.01830439828336239, 0.0036100626457482576, 0.011856134980916977, 0.07255373150110245, 0.006018087733536959, 0.006411908660084009, 0.05586158111691475, 0.0837467685341835, 0.003973665647208691, 0.01859060302376747, 0.019445108249783516, 0.003880683332681656, 0.008932655677199364, 0.1224270612001419, 0.04098634421825409, 0.07429762929677963, 0.05774552747607231, 0.006318635772913694, 0.013084109872579575, 0.023065775632858276, 0.0057965172454714775, 0.040941350162029266, 0.015039575286209583]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9780336022377014, 0.021966326981782913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5068986415863037, 0.42791470885276794, 0.06518669426441193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3459506034851074, 0.23385089635849, 0.1913880854845047, 0.22881045937538147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.278596967458725, 0.1752573698759079, 0.1103709265589714, 0.25180959701538086, 0.18396516144275665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20804190635681152, 0.08827142417430878, 0.06160815432667732, 0.2584843039512634, 0.2705698609352112, 0.11302441358566284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16325628757476807, 0.12141181528568268, 0.11990709602832794, 0.1616644412279129, 0.1077408567070961, 0.2975035309791565, 0.028516001999378204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12280929833650589, 0.03978985920548439, 0.04514545947313309, 0.1357850879430771, 0.09269557148218155, 0.1180870532989502, 0.3369846045970917, 0.10870304703712463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07854462414979935, 0.03570381924510002, 0.02910536527633667, 0.05200905352830887, 0.10322187840938568, 0.062225114554166794, 0.0680123120546341, 0.1702914983034134, 0.4008863568305969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0855533555150032, 0.04508211091160774, 0.020755281671881676, 0.057487890124320984, 0.058533668518066406, 0.03812532499432564, 0.03164026886224747, 0.1447080820798874, 0.3939676284790039, 0.12414632737636566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03400762379169464, 0.0058013093657791615, 0.004264367278665304, 0.014554603025317192, 0.002058129059150815, 0.010052233003079891, 0.015462481416761875, 0.014919957146048546, 0.07456041872501373, 0.802941083908081, 0.02137785032391548, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03658134862780571, 0.00855263601988554, 0.009085931815207005, 0.010718593373894691, 0.03151058033108711, 0.07786053419113159, 0.028099501505494118, 0.05203390493988991, 0.09175669401884079, 0.24980762600898743, 0.22384582459926605, 0.18014687299728394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03899505361914635, 0.013953049667179585, 0.027150386944413185, 0.015978654846549034, 0.006068876478821039, 0.015332875773310661, 0.00735844299197197, 0.00814359076321125, 0.0923612043261528, 0.09582376480102539, 0.39708954095840454, 0.2250824272632599, 0.056662142276763916, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.029417622834444046, 0.006853977683931589, 0.005433534737676382, 0.008306880481541157, 0.015738267451524734, 0.008708875626325607, 0.008349086157977581, 0.023920075967907906, 0.04694697633385658, 0.048144929111003876, 0.09309998154640198, 0.15290842950344086, 0.14854176342487335, 0.4036295413970947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0371282622218132, 0.007584472186863422, 0.0035451443400233984, 0.008117972873151302, 0.018834179267287254, 0.0059049720875918865, 0.009632624685764313, 0.011076647788286209, 0.04249946400523186, 0.015083983540534973, 0.01984347030520439, 0.11177270114421844, 0.1281687468290329, 0.3331788182258606, 0.24762862920761108, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02479156292974949, 0.0022572451271116734, 0.0019112328300252557, 0.003149652387946844, 0.0017746718367561698, 0.0030808616429567337, 0.01988077536225319, 0.006427101790904999, 0.015317508019506931, 0.0032266085036098957, 0.020296083763241768, 0.057727403938770294, 0.022555332630872726, 0.08885964751243591, 0.714111864566803, 0.014632453210651875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019352775067090988, 0.0027357167564332485, 0.0027006464079022408, 0.002816400257870555, 0.00730062136426568, 0.01626867987215519, 0.005134698934853077, 0.010258655063807964, 0.01379148755222559, 0.03380439430475235, 0.033944860100746155, 0.024110393598675728, 0.053474947810173035, 0.09691006690263748, 0.27618828415870667, 0.22190363705158234, 0.17930373549461365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01677826978266239, 0.0013836653670296073, 0.0012504429323598742, 0.0019532637670636177, 0.0012283112155273557, 0.0034270435571670532, 0.0017710248939692974, 0.0025876860599964857, 0.009956525638699532, 0.007537418510764837, 0.004963126964867115, 0.023668881505727768, 0.017456667497754097, 0.06967759877443314, 0.26127639412879944, 0.24082458019256592, 0.17677922546863556, 0.15747980773448944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05475032702088356, 0.00920194387435913, 0.003783305874094367, 0.006891878787428141, 0.007006865460425615, 0.005267783533781767, 0.00191358698066324, 0.00664900615811348, 0.02249075658619404, 0.007203695364296436, 0.0398598313331604, 0.02630694955587387, 0.035983942449092865, 0.10654627531766891, 0.14742796123027802, 0.02953191101551056, 0.14834192395210266, 0.23425240814685822, 0.10658963769674301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08004866540431976, 0.0035627426113933325, 0.0033941485453397036, 0.011285563930869102, 0.012208592146635056, 0.0019434994319453835, 0.003920837305486202, 0.024116644635796547, 0.023350011557340622, 0.008975870907306671, 0.005343950819224119, 0.02533606067299843, 0.013870205730199814, 0.10103649646043777, 0.039738066494464874, 0.08061394840478897, 0.12086480855941772, 0.12294679880142212, 0.14132501184940338, 0.17611812055110931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013053340837359428, 0.0019795652478933334, 0.001618213951587677, 0.001407849951647222, 0.0014868256403133273, 0.0013099147472530603, 0.001664295676164329, 0.002566516399383545, 0.003062676638364792, 0.010340563952922821, 0.007755259983241558, 0.008929421193897724, 0.008869552984833717, 0.017165135592222214, 0.07265784591436386, 0.08664465695619583, 0.06223947927355766, 0.1652844399213791, 0.07255084067583084, 0.09155550599098206, 0.3678581714630127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012316033244132996, 0.0011592383962124586, 0.0007622409611940384, 0.0019507516408339143, 0.0012718533398583531, 0.000689227890688926, 0.0005459004896692932, 0.0026030046865344048, 0.00391767593100667, 0.0005409372970461845, 0.0012741890968754888, 0.006503172218799591, 0.005551606882363558, 0.017973506823182106, 0.013353370130062103, 0.03805127739906311, 0.04303397983312607, 0.03958648443222046, 0.02230064757168293, 0.03898364305496216, 0.5949541926383972, 0.1526769995689392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013724962249398232, 0.0016125753754749894, 0.0003289878077339381, 0.0010084331734105945, 0.0011758062755689025, 0.0007259282283484936, 0.0005918759852647781, 0.0009615611052140594, 0.003118614898994565, 0.0059699928387999535, 0.0030334668699651957, 0.003344498109072447, 0.008407575078308582, 0.013658551499247551, 0.005607030354440212, 0.005321725271642208, 0.01928994432091713, 0.02910960651934147, 0.01359962671995163, 0.03133772313594818, 0.2513725161552429, 0.5305221080780029, 0.05617691949009895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016002364456653595, 0.0009560451726429164, 0.0005680457688868046, 0.001554975868202746, 0.0015755697386339307, 0.0006219734204933047, 0.001125766197219491, 0.0014368727570399642, 0.002310049021616578, 0.0009028324275277555, 0.0013921469217166305, 0.003672589547932148, 0.0036322870291769505, 0.009217431768774986, 0.008623880334198475, 0.02702576108276844, 0.021376516669988632, 0.03330104798078537, 0.03885401412844658, 0.03905831277370453, 0.26596641540527344, 0.1937260925769806, 0.09792899340391159, 0.22916999459266663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009987291879951954, 0.0009651762084104121, 0.0006041588494554162, 0.0009222132503055036, 0.0008805814431980252, 0.0005974328960292041, 0.00021149257372599095, 0.0006455681868828833, 0.001241769758053124, 0.0006859689601697028, 0.0007031509303487837, 0.0014577374095097184, 0.001848775427788496, 0.005077208857983351, 0.001927566365338862, 0.01241341233253479, 0.008568309247493744, 0.012073640711605549, 0.021936628967523575, 0.029194090515375137, 0.14272744953632355, 0.0983402207493782, 0.1184767484664917, 0.3163195848464966, 0.2121937870979309, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01686088740825653, 0.0019703262951225042, 0.0008858363726176322, 0.0017656385898590088, 0.002392832888290286, 0.0005271154223009944, 0.00036178913433104753, 0.0006361034465953708, 0.002241130918264389, 0.0008515836670994759, 0.00038157813833095133, 0.0022748238407075405, 0.0018798511009663343, 0.0072922478429973125, 0.002146668964996934, 0.002526310971006751, 0.010355673730373383, 0.011969313956797123, 0.006590915843844414, 0.01282747182995081, 0.1213049441576004, 0.06624637544155121, 0.1461237370967865, 0.2828071713447571, 0.24703341722488403, 0.04974621906876564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.026405034586787224, 0.0014231048990041018, 0.0016234475187957287, 0.002576408674940467, 0.0026897063944488764, 0.0012257546186447144, 0.0010898791952058673, 0.0009538487647660077, 0.0019223600393161178, 0.002411529188975692, 0.002457947935909033, 0.0014354214072227478, 0.00537851033732295, 0.004732354078441858, 0.001939119421876967, 0.007957512512803078, 0.006086899433284998, 0.01265745609998703, 0.011602508835494518, 0.02909664809703827, 0.08598870038986206, 0.07665876299142838, 0.03624017536640167, 0.13133373856544495, 0.1949029117822647, 0.21766290068626404, 0.13154734671115875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.023896072059869766, 0.002067496767267585, 0.0025093795266002417, 0.0018511989619582891, 0.0031609614379704, 0.0009005620959214866, 0.0002685431099962443, 0.0005213560070842505, 0.001787214307114482, 0.0019308938644826412, 0.0010158225195482373, 0.0010442669736221433, 0.0015677065821364522, 0.004060107283294201, 0.0008207401842810214, 0.0033583054319024086, 0.004117127973586321, 0.007223011460155249, 0.014547313563525677, 0.046351563185453415, 0.056018635630607605, 0.038400180637836456, 0.04161002114415169, 0.1049453541636467, 0.15788236260414124, 0.12118802219629288, 0.1909024566411972, 0.16605335474014282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.040757689625024796, 0.003958356101065874, 0.0056980871595442295, 0.003957982640713453, 0.001404777867719531, 0.00032769402605481446, 0.0006401917198672891, 0.0012045807670801878, 0.0022385562770068645, 0.0007448518881574273, 0.003836573800072074, 0.0016206809086725116, 0.0010148318251594901, 0.004261195659637451, 0.0009652958833612502, 0.00039543129969388247, 0.0054010688327252865, 0.007263383828103542, 0.005766796413809061, 0.02999177947640419, 0.04173648729920387, 0.017527615651488304, 0.025155073031783104, 0.07565267384052277, 0.24542659521102905, 0.06668500602245331, 0.2648318111896515, 0.05502421036362648, 0.08651074022054672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011784779839217663, 0.001394758583046496, 0.000951565511059016, 0.0008953217184171081, 0.0010372718097642064, 0.0004972643218934536, 0.000252838566666469, 0.00043175765313208103, 0.000534569495357573, 0.00028229213785380125, 0.00042747249244712293, 0.0005479561514221132, 0.0008038741652853787, 0.001103753224015236, 0.0010882848873734474, 0.0026353844441473484, 0.002003318862989545, 0.0029394496232271194, 0.00511420052498579, 0.009162303060293198, 0.0202364232391119, 0.014397235587239265, 0.014386619441211224, 0.052873022854328156, 0.04425811395049095, 0.07174728810787201, 0.08077161014080048, 0.11779727786779404, 0.144861102104187, 0.3947828710079193, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012992837466299534, 0.0016676133964210749, 0.0010021064663305879, 0.0011761232744902372, 0.0007638345705345273, 0.0004005825030617416, 0.00012849258200731128, 0.00010518699127715081, 0.0005606423947028816, 0.00014079375250730664, 0.00012207584222778678, 0.00039859823300503194, 0.00022988366254139692, 0.0011077845701947808, 0.00036310197901912034, 0.0008891805191524327, 0.0014402443775907159, 0.001452018041163683, 0.0026305641513317823, 0.003167375922203064, 0.017224634066224098, 0.010722312144935131, 0.005022993311285973, 0.04577849060297012, 0.047519706189632416, 0.027067147195339203, 0.05216851457953453, 0.055007871240377426, 0.1148100346326828, 0.44568806886672974, 0.14825110137462616, 0.0, 0.0, 0.0, 0.0], [0.011720004491508007, 0.0009638675255700946, 0.0016792810056358576, 0.0009598798933438957, 0.001183534855954349, 0.00035873393062502146, 0.00013125214900355786, 0.00020985899027436972, 0.0003518584999255836, 0.00011673830158542842, 0.00033267191611230373, 0.00020899795345030725, 0.00016304105520248413, 0.0005591619410552084, 0.00017877857317216694, 0.0005847546271979809, 0.0006520423339679837, 0.0008100864361040294, 0.0008232013788074255, 0.006545133888721466, 0.007790679112076759, 0.006622500717639923, 0.005052973981946707, 0.014489860273897648, 0.024942122399806976, 0.019010333344340324, 0.028525497764348984, 0.0367877222597599, 0.14266951382160187, 0.23489901423454285, 0.30085450410842896, 0.149822399020195, 0.0, 0.0, 0.0], [0.011463247239589691, 0.0009014803217723966, 0.0005477435770444572, 0.0005869866581633687, 0.0005616615526378155, 0.00023434734612237662, 0.00013944668171461672, 0.00019926753884647042, 0.00031403012690134346, 0.00012514613626990467, 0.00021479467977769673, 0.0002481336996424943, 0.00022906152298673987, 0.000502077687997371, 0.00045190079254098237, 0.0006676724296994507, 0.0007638589013367891, 0.0009467139607295394, 0.0010094349272549152, 0.003719559172168374, 0.0074448902159929276, 0.0045005506835877895, 0.0018344356212764978, 0.012973615899682045, 0.015522499568760395, 0.02183830365538597, 0.016228925436735153, 0.024697480723261833, 0.03805321455001831, 0.16302356123924255, 0.11826064437627792, 0.22884733974933624, 0.32294800877571106, 0.0, 0.0], [0.009539701975882053, 0.0014243260957300663, 0.0009944569319486618, 0.0009655568283051252, 0.0007854436989873648, 0.00030160605092532933, 0.00011159542191307992, 0.0008093683863990009, 0.00040054702549241483, 8.517048263456672e-05, 0.0006307571311481297, 0.0002086151798721403, 0.00025255655054934323, 0.0005451851175166667, 0.00024275577743537724, 0.0005828302819281816, 0.0005503935972228646, 0.0010548532009124756, 0.0010979134822264314, 0.004033531527966261, 0.005151556339114904, 0.007251241244375706, 0.003988177049905062, 0.01226038672029972, 0.013561572879552841, 0.03828657045960426, 0.06675270199775696, 0.013111543841660023, 0.10093259811401367, 0.1317911148071289, 0.05432308092713356, 0.15251493453979492, 0.3083247244358063, 0.06713257730007172, 0.0], [0.009234591387212276, 0.0018696945626288652, 0.0015434965025633574, 0.001287892577238381, 0.0009349427418783307, 0.0002825605624821037, 0.00044942903332412243, 0.0005595000693574548, 0.0003277684736531228, 7.203836867120117e-05, 0.00010657928214641288, 0.00026362441712990403, 0.00036547365016303957, 0.0003748327726498246, 0.0003117532469332218, 0.0008969160262495279, 0.0006347473827190697, 0.0008650031522847712, 0.0008020043023861945, 0.0006307887961156666, 0.003218779806047678, 0.0038458225317299366, 0.0030004356522113085, 0.007814164273440838, 0.010107218287885189, 0.016721557825803757, 0.0076720695942640305, 0.010031222365796566, 0.0053563895635306835, 0.07757045328617096, 0.03774447366595268, 0.13809092342853546, 0.19567619264125824, 0.030743539333343506, 0.43059301376342773]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8970723748207092, 0.10292758792638779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7731761336326599, 0.17108604311943054, 0.05573779717087746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2190982550382614, 0.0959283858537674, 0.15399764478206635, 0.5309756994247437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49329206347465515, 0.1242106631398201, 0.08378960192203522, 0.21044500172138214, 0.08826273679733276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4792534410953522, 0.11483927816152573, 0.058252885937690735, 0.12140925973653793, 0.19296889007091522, 0.033276259899139404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3376070261001587, 0.13038289546966553, 0.12365788966417313, 0.16848322749137878, 0.039485618472099304, 0.1552271544933319, 0.04515617713332176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31243157386779785, 0.10335508733987808, 0.07322126626968384, 0.13077448308467865, 0.08650775253772736, 0.2037171870470047, 0.04714140295982361, 0.04285132512450218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10719949007034302, 0.032564759254455566, 0.04615403339266777, 0.1252276748418808, 0.01110523846000433, 0.03433000668883324, 0.007858377881348133, 0.011366580612957478, 0.6241938471794128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3028891086578369, 0.0780496597290039, 0.059376493096351624, 0.09426914155483246, 0.17377150058746338, 0.03861623629927635, 0.04802530258893967, 0.03951253369450569, 0.13237622380256653, 0.03311377763748169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1302769035100937, 0.042134854942560196, 0.07258214056491852, 0.1092287227511406, 0.03630436211824417, 0.19112543761730194, 0.03369797393679619, 0.057158585637807846, 0.11187221109867096, 0.16414137184619904, 0.05147746205329895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0946442037820816, 0.02925342693924904, 0.038119472563266754, 0.14989575743675232, 0.020670855417847633, 0.07631715387105942, 0.008947462774813175, 0.02004508674144745, 0.305785596370697, 0.0475209541618824, 0.010440826416015625, 0.19835913181304932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1444079726934433, 0.06329526752233505, 0.0761776864528656, 0.07194559276103973, 0.03394105285406113, 0.08442067354917526, 0.03594022989273071, 0.07685539871454239, 0.1158827617764473, 0.05951951444149017, 0.06183499097824097, 0.14841705560684204, 0.027361808344721794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04147874191403389, 0.011494407430291176, 0.017643557861447334, 0.04599723964929581, 0.00408501410856843, 0.013701115734875202, 0.002870903816074133, 0.004510778002440929, 0.24164853990077972, 0.009729510173201561, 0.0026167372707277536, 0.26417696475982666, 0.008053767494857311, 0.3319927752017975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15155856311321259, 0.0377250500023365, 0.02733069658279419, 0.06052476540207863, 0.03014327958226204, 0.0280348788946867, 0.013341414742171764, 0.02250303141772747, 0.11653433740139008, 0.05189970135688782, 0.010558661073446274, 0.10229871422052383, 0.01516727078706026, 0.14137372374534607, 0.19100594520568848, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2839045822620392, 0.06502395868301392, 0.03238338232040405, 0.029591098427772522, 0.0424320288002491, 0.02846975065767765, 0.04153315722942352, 0.0846530869603157, 0.0438549779355526, 0.02753038890659809, 0.09218152612447739, 0.03751114755868912, 0.04891902208328247, 0.04984968900680542, 0.07913842052221298, 0.013023803010582924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04885757714509964, 0.014007358811795712, 0.019542643800377846, 0.07337553054094315, 0.010089141316711903, 0.03972570225596428, 0.004310052376240492, 0.010590332560241222, 0.15303601324558258, 0.026499031111598015, 0.005417249631136656, 0.09647625684738159, 0.012806718237698078, 0.20478640496730804, 0.14743396639823914, 0.009689177386462688, 0.12335681170225143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05943123996257782, 0.02595117688179016, 0.03547389805316925, 0.039079729467630386, 0.018049320206046104, 0.023087218403816223, 0.010503754951059818, 0.030069546774029732, 0.09918279200792313, 0.03614409640431404, 0.018410032615065575, 0.11741490662097931, 0.01716756820678711, 0.1276998221874237, 0.08459028601646423, 0.037902723997831345, 0.14577235281467438, 0.07406948506832123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2724888324737549, 0.045022718608379364, 0.046122465282678604, 0.04205196350812912, 0.03813562169671059, 0.05454421788454056, 0.025706563144922256, 0.01925414428114891, 0.04536457732319832, 0.051068805158138275, 0.021479353308677673, 0.05142943933606148, 0.03568308800458908, 0.051102492958307266, 0.04221026971936226, 0.03813282027840614, 0.05789826437830925, 0.023540083318948746, 0.038764260709285736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2398124784231186, 0.03723884001374245, 0.04641326889395714, 0.018074775114655495, 0.02860943228006363, 0.03156362101435661, 0.023263679817318916, 0.053141187876462936, 0.0367298424243927, 0.044361554086208344, 0.01409000251442194, 0.04079652950167656, 0.013384036719799042, 0.040182728320360184, 0.025124967098236084, 0.022283153608441353, 0.044574398547410965, 0.026357263326644897, 0.018502138555049896, 0.195496067404747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03447255119681358, 0.00704978359863162, 0.008516237139701843, 0.019823268055915833, 0.0020061940886080265, 0.008738483302295208, 0.001380531000904739, 0.0025849826633930206, 0.12963873147964478, 0.004980513360351324, 0.0014542971039190888, 0.09793500602245331, 0.00424265768378973, 0.17474913597106934, 0.020575376227498055, 0.0029217300470918417, 0.12519226968288422, 0.017666760832071304, 0.008172450587153435, 0.006463987287133932, 0.3214350640773773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07532121986150742, 0.01738511025905609, 0.026499608531594276, 0.03820110857486725, 0.013464313931763172, 0.017004920169711113, 0.005682717077434063, 0.008161384612321854, 0.0879102423787117, 0.015651987865567207, 0.004622372332960367, 0.06870009750127792, 0.01706838235259056, 0.11098404228687286, 0.08843357861042023, 0.015540494583547115, 0.0829065591096878, 0.03445708379149437, 0.03490288928151131, 0.022669067606329918, 0.16727590560913086, 0.04715687781572342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11039694398641586, 0.01761859469115734, 0.012073243968188763, 0.07903024554252625, 0.04707181081175804, 0.028393466025590897, 0.010853295214474201, 0.03898295760154724, 0.042541757225990295, 0.053135793656110764, 0.010616575367748737, 0.04066741466522217, 0.016578689217567444, 0.05044253543019295, 0.041483134031295776, 0.011694175191223621, 0.04894391447305679, 0.045875273644924164, 0.06803461909294128, 0.03145822882652283, 0.0737365186214447, 0.09332363307476044, 0.027047310024499893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.050105899572372437, 0.011403615586459637, 0.018980005756020546, 0.048291075974702835, 0.005478163715451956, 0.027938097715377808, 0.0034753400832414627, 0.00966764334589243, 0.08883966505527496, 0.021459758281707764, 0.009184242226183414, 0.06738950312137604, 0.0062937806360423565, 0.11730504035949707, 0.055593859404325485, 0.023259541019797325, 0.08538833260536194, 0.03017236851155758, 0.01855376921594143, 0.049727220088243484, 0.13869956135749817, 0.03593093156814575, 0.02957059256732464, 0.047291919589042664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03365064412355423, 0.03973431885242462, 0.013943283818662167, 0.022155024111270905, 0.007374103181064129, 0.012534337118268013, 0.003937930334359407, 0.005588031839579344, 0.05276413634419441, 0.011353474110364914, 0.002976151881739497, 0.061219990253448486, 0.007114535663276911, 0.06609281897544861, 0.018457217141985893, 0.0047071841545403, 0.07780608534812927, 0.01593407243490219, 0.016269726678729057, 0.02733295038342476, 0.08945077657699585, 0.028283197432756424, 0.013428352773189545, 0.24124999344348907, 0.1266416758298874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10162999480962753, 0.04817340523004532, 0.022741544991731644, 0.017647169530391693, 0.016357652842998505, 0.014718163758516312, 0.020238906145095825, 0.03009829856455326, 0.03836439177393913, 0.025297461077570915, 0.03959347680211067, 0.04179811477661133, 0.01849495805799961, 0.04345664754509926, 0.02387695573270321, 0.013411923311650753, 0.048047564923763275, 0.021995265036821365, 0.030744794756174088, 0.05168476328253746, 0.05786481872200966, 0.04216012358665466, 0.05039006844162941, 0.08024898171424866, 0.07608186453580856, 0.02488280087709427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06603527814149857, 0.020660094916820526, 0.021681711077690125, 0.03408760577440262, 0.015279388055205345, 0.018441997468471527, 0.010789581574499607, 0.05595835670828819, 0.03565486520528793, 0.021738843992352486, 0.025664953514933586, 0.0394655242562294, 0.01909186691045761, 0.04260499030351639, 0.0451403446495533, 0.012913567014038563, 0.047355785965919495, 0.025561854243278503, 0.0352441668510437, 0.09746362268924713, 0.056113146245479584, 0.03593849018216133, 0.032567013055086136, 0.04301987960934639, 0.046976763755083084, 0.0324612520635128, 0.06208902969956398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10541099309921265, 0.016973095014691353, 0.02034103311598301, 0.032666537910699844, 0.021851176396012306, 0.031789906322956085, 0.005571204703301191, 0.024194519966840744, 0.016740459948778152, 0.01608254946768284, 0.013484912924468517, 0.022790368646383286, 0.02084488607943058, 0.019816411659121513, 0.007412000559270382, 0.013035065494477749, 0.02698247879743576, 0.01013225968927145, 0.017667721956968307, 0.29726240038871765, 0.02345716394484043, 0.032355837523937225, 0.03224533423781395, 0.019980134442448616, 0.05017200484871864, 0.048278842121362686, 0.033088237047195435, 0.01937241666018963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11478210985660553, 0.028451863676309586, 0.030158115550875664, 0.02989071048796177, 0.01557771023362875, 0.0452362596988678, 0.008156673982739449, 0.04677761718630791, 0.02057456783950329, 0.018089186400175095, 0.01663844846189022, 0.024351509287953377, 0.014281019568443298, 0.022870585322380066, 0.024558451026678085, 0.021522248163819313, 0.02774013951420784, 0.012263176031410694, 0.016789693385362625, 0.030601762235164642, 0.033104460686445236, 0.04230622947216034, 0.05013420432806015, 0.0398956723511219, 0.025471482425928116, 0.03835650160908699, 0.14836569130420685, 0.04062198847532272, 0.0124319763854146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010402974672615528, 0.003988312557339668, 0.007536562159657478, 0.023714056238532066, 0.0015747013967484236, 0.004556488711386919, 0.0011461451649665833, 0.0020206572953611612, 0.08388330787420273, 0.006093546748161316, 0.001224365085363388, 0.08011967688798904, 0.0032829297706484795, 0.11485139280557632, 0.02013857662677765, 0.0038740236777812243, 0.10355663299560547, 0.016239933669567108, 0.006219523027539253, 0.011158796958625317, 0.12897233664989471, 0.016916105523705482, 0.008813071064651012, 0.028795937076210976, 0.06059351935982704, 0.0056185950525105, 0.016893554478883743, 0.008791659027338028, 0.017022401094436646, 0.202000230550766, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05472033470869064, 0.015954429283738136, 0.012098269537091255, 0.022833751514554024, 0.01898520439863205, 0.028783854097127914, 0.008076783269643784, 0.008372064679861069, 0.03956403583288193, 0.012315304949879646, 0.009588331915438175, 0.02357124164700508, 0.01064310036599636, 0.04663844034075737, 0.014515896327793598, 0.012394594959914684, 0.027597375214099884, 0.021691426634788513, 0.020180564373731613, 0.01823921501636505, 0.05496653541922569, 0.0371178537607193, 0.02424504980444908, 0.03499600663781166, 0.14947696030139923, 0.029019474983215332, 0.04327644035220146, 0.015866447240114212, 0.045115429908037186, 0.11428382247686386, 0.02487177401781082, 0.0, 0.0, 0.0, 0.0], [0.028266342356801033, 0.008342581801116467, 0.009693176485598087, 0.024003278464078903, 0.012406986206769943, 0.0097315963357687, 0.007254773285239935, 0.011378861963748932, 0.0452534519135952, 0.014661080203950405, 0.004759802483022213, 0.03279304876923561, 0.009599766694009304, 0.05662672221660614, 0.023795733228325844, 0.004614102654159069, 0.03982147201895714, 0.021238988265395164, 0.025286570191383362, 0.10159794986248016, 0.09278310835361481, 0.026123860850930214, 0.015091437846422195, 0.03458563610911369, 0.06182198226451874, 0.013407000340521336, 0.0168897844851017, 0.030893538147211075, 0.028194846585392952, 0.1103137731552124, 0.04056955501437187, 0.03819925710558891, 0.0, 0.0, 0.0], [0.0059200855903327465, 0.002644824329763651, 0.0057548899203538895, 0.021997150033712387, 0.0013081284705549479, 0.005726532079279423, 0.0008704039501026273, 0.0018940429436042905, 0.052954934537410736, 0.004725005477666855, 0.0009999445173889399, 0.04342703893780708, 0.002174984896555543, 0.07363717257976532, 0.01701577566564083, 0.0030869918409734964, 0.057790935039520264, 0.010734486393630505, 0.0034258211962878704, 0.010332702659070492, 0.08509067445993423, 0.012429173104465008, 0.006230238825082779, 0.022221023216843605, 0.06411117315292358, 0.0050534820184111595, 0.018152384087443352, 0.00759414816275239, 0.0067864470183849335, 0.1671644151210785, 0.013871804811060429, 0.014755623415112495, 0.25011754035949707, 0.0, 0.0], [0.059587135910987854, 0.014287113212049007, 0.009688671678304672, 0.02847461588680744, 0.02860480546951294, 0.026259349659085274, 0.0088196424767375, 0.02213604934513569, 0.017910204827785492, 0.026397528126835823, 0.017243580892682076, 0.025983795523643494, 0.006316175684332848, 0.020043157041072845, 0.015440637245774269, 0.02594022825360298, 0.0301030445843935, 0.01836533471941948, 0.023130416870117188, 0.026214564219117165, 0.031157243996858597, 0.028986219316720963, 0.02896130457520485, 0.016435230150818825, 0.03480091318488121, 0.037943657487630844, 0.07380682975053787, 0.030772438272833824, 0.028517309576272964, 0.0738709345459938, 0.024587737396359444, 0.03523411229252815, 0.08090805262327194, 0.023071996867656708, 0.0], [0.052589915692806244, 0.008282373659312725, 0.004686595872044563, 0.011556098237633705, 0.0021243628580123186, 0.0073766508139669895, 0.002569912001490593, 0.005167304538190365, 0.06430479884147644, 0.00868866965174675, 0.004349817521870136, 0.0540519617497921, 0.0035673296079039574, 0.08101719617843628, 0.021457049995660782, 0.004144072532653809, 0.06621566414833069, 0.018716933205723763, 0.009488233365118504, 0.0060385591350495815, 0.102742999792099, 0.014565207995474339, 0.009106311947107315, 0.021657651290297508, 0.03524801880121231, 0.004852627869695425, 0.008422154001891613, 0.019589506089687347, 0.011885883286595345, 0.1446465700864792, 0.016607558354735374, 0.022834792733192444, 0.08476633578538895, 0.015126842074096203, 0.05155407264828682]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8803902268409729, 0.1196097582578659, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8001625537872314, 0.14422571659088135, 0.055611707270145416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6471942067146301, 0.1470177173614502, 0.07931007444858551, 0.12647795677185059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6291003227233887, 0.13269932568073273, 0.08307387679815292, 0.1030661016702652, 0.05206045135855675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5631015300750732, 0.12375932931900024, 0.11159723997116089, 0.10842729359865189, 0.08343342691659927, 0.009681183844804764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49733901023864746, 0.11400078982114792, 0.07851862907409668, 0.10408832877874374, 0.08229987323284149, 0.08720178157091141, 0.03655164688825607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4618664085865021, 0.10720594227313995, 0.07680261135101318, 0.09525877237319946, 0.11179568618535995, 0.046836573630571365, 0.0632898285984993, 0.03694409877061844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4088016748428345, 0.11223533749580383, 0.061949506402015686, 0.0813821330666542, 0.06888710707426071, 0.047599416226148605, 0.057795699685811996, 0.06300587952136993, 0.09834324568510056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3377854824066162, 0.07768949866294861, 0.06164025142788887, 0.08030418306589127, 0.09243477135896683, 0.12569795548915863, 0.04310278221964836, 0.08006501942873001, 0.08753135055303574, 0.013748617842793465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4084458351135254, 0.07972611486911774, 0.059432823210954666, 0.07098964601755142, 0.06258386373519897, 0.07298711687326431, 0.052909161895513535, 0.06663114577531815, 0.08486577868461609, 0.030785245820879936, 0.010643381625413895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.328408807516098, 0.08126053214073181, 0.052221186459064484, 0.0667019784450531, 0.06221216544508934, 0.046883296221494675, 0.04748227819800377, 0.049437154084444046, 0.0864436998963356, 0.05108512565493584, 0.04660411179065704, 0.08125967532396317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28456372022628784, 0.0733422264456749, 0.05956626683473587, 0.05995684489607811, 0.0626126229763031, 0.04492252692580223, 0.05354888737201691, 0.05539520084857941, 0.0760505199432373, 0.0543915256857872, 0.05521014705300331, 0.0848141685128212, 0.03562534227967262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.298692524433136, 0.08053484559059143, 0.0463000163435936, 0.05841613560914993, 0.052452508360147476, 0.03589675948023796, 0.0431300513446331, 0.048164092004299164, 0.07179582864046097, 0.035882286727428436, 0.03498638793826103, 0.06638611853122711, 0.04749887064099312, 0.0798635482788086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2618487477302551, 0.06895221024751663, 0.04662094637751579, 0.05860457941889763, 0.0469287745654583, 0.055680107325315475, 0.04915207624435425, 0.041177768260240555, 0.0749075636267662, 0.03198425471782684, 0.03257225453853607, 0.07257074117660522, 0.03739875927567482, 0.08542975783348083, 0.03617141395807266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2941747307777405, 0.05360918492078781, 0.03860877826809883, 0.04952672868967056, 0.040609169751405716, 0.04149331897497177, 0.029716262593865395, 0.04536978900432587, 0.06024930998682976, 0.04159967228770256, 0.06637395918369293, 0.05982467532157898, 0.04050287976861, 0.06734270602464676, 0.06095743179321289, 0.010041479952633381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24078045785427094, 0.05731498450040817, 0.038129813969135284, 0.047623760998249054, 0.04672613739967346, 0.035154763609170914, 0.03462182357907295, 0.037472404539585114, 0.06234779953956604, 0.039545491337776184, 0.035744331777095795, 0.05874498188495636, 0.034078411757946014, 0.07071567326784134, 0.05439119413495064, 0.03945146128535271, 0.06715650856494904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1900811791419983, 0.055469125509262085, 0.03824987635016441, 0.044684700667858124, 0.0434919036924839, 0.03729135915637016, 0.03500031679868698, 0.034133367240428925, 0.05614839494228363, 0.029018212109804153, 0.026478612795472145, 0.06265048682689667, 0.03502200171351433, 0.06568856537342072, 0.05714201554656029, 0.04781632497906685, 0.07356442511081696, 0.06806914508342743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23406487703323364, 0.04765677824616432, 0.036266397684812546, 0.043501511216163635, 0.0448233038187027, 0.030926160514354706, 0.02414553426206112, 0.03047102876007557, 0.05100265145301819, 0.03852144256234169, 0.04128361493349075, 0.05289098247885704, 0.027892794460058212, 0.057388804852962494, 0.060670021921396255, 0.04387101158499718, 0.05933019518852234, 0.05117016285657883, 0.02412269078195095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3121168911457062, 0.04305698722600937, 0.030739212408661842, 0.039901234209537506, 0.04073917865753174, 0.033973563462495804, 0.013991206884384155, 0.03250167518854141, 0.04214292764663696, 0.031367409974336624, 0.05722326040267944, 0.0420997329056263, 0.032853297889232635, 0.04662720486521721, 0.04874775931239128, 0.0217305775731802, 0.047223784029483795, 0.029857559129595757, 0.04530388489365578, 0.007802656386047602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1958305537700653, 0.05227472260594368, 0.03251208737492561, 0.03705889359116554, 0.04335431009531021, 0.027641301974654198, 0.027452072128653526, 0.033081550151109695, 0.045324381440877914, 0.025406768545508385, 0.024781646206974983, 0.03888051211833954, 0.030247293412685394, 0.05089511349797249, 0.041287798434495926, 0.03749210759997368, 0.04425322264432907, 0.04428718984127045, 0.04519393667578697, 0.05072396621108055, 0.07202057540416718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16079983115196228, 0.04841070622205734, 0.033020980656147, 0.03441706672310829, 0.03531978651881218, 0.02389702759683132, 0.026063483208417892, 0.028639713302254677, 0.044538725167512894, 0.02494949661195278, 0.029716169461607933, 0.04012998938560486, 0.02485906146466732, 0.05134394019842148, 0.05373547598719597, 0.04527627304196358, 0.046805981546640396, 0.03460599482059479, 0.04245500639081001, 0.05485450103878975, 0.06976339221000671, 0.04639735445380211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18932507932186127, 0.039103616029024124, 0.02485201507806778, 0.03345050662755966, 0.038601603358983994, 0.024109313264489174, 0.02050212398171425, 0.02430175617337227, 0.03976469114422798, 0.03418169170618057, 0.03484787791967392, 0.036197833716869354, 0.023668929934501648, 0.045024823397397995, 0.03498945012688637, 0.02682550437748432, 0.04153791442513466, 0.037366632372140884, 0.042022790759801865, 0.05591040477156639, 0.06404419243335724, 0.056186314672231674, 0.0331849567592144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16596601903438568, 0.033881090581417084, 0.020657533779740334, 0.03212728723883629, 0.03747255727648735, 0.02651948295533657, 0.01988290622830391, 0.02560962364077568, 0.04091743752360344, 0.02261033095419407, 0.019720882177352905, 0.03376535326242447, 0.023503798991441727, 0.046445246785879135, 0.03541432321071625, 0.027368398383259773, 0.0387694276869297, 0.03146073594689369, 0.046399280428886414, 0.04867471009492874, 0.06382154673337936, 0.06529751420021057, 0.04629906639456749, 0.047415412962436676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15514099597930908, 0.036314789205789566, 0.022236892953515053, 0.028097428381443024, 0.04015668109059334, 0.02405599132180214, 0.017400914803147316, 0.021498551592230797, 0.03675064444541931, 0.017986752092838287, 0.019703930243849754, 0.02832777425646782, 0.02260459214448929, 0.04168048873543739, 0.030497979372739792, 0.025585593655705452, 0.032487597316503525, 0.02701493911445141, 0.04112283140420914, 0.04738447815179825, 0.06382343918085098, 0.05655011534690857, 0.04119997099041939, 0.06263256818056107, 0.05974404886364937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12975935637950897, 0.027339696884155273, 0.029957514256238937, 0.022702835500240326, 0.03781534358859062, 0.03144637122750282, 0.017136774957180023, 0.04656488448381424, 0.030618293210864067, 0.018781663849949837, 0.025273220613598824, 0.02872198261320591, 0.0190909281373024, 0.03481002897024155, 0.026170244440436363, 0.02681773528456688, 0.03296038135886192, 0.024960245937108994, 0.0547475665807724, 0.057801052927970886, 0.05617624893784523, 0.03305982053279877, 0.057674188166856766, 0.0466742105782032, 0.06228702515363693, 0.020652364939451218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18126995861530304, 0.02792493812739849, 0.028055928647518158, 0.02945740893483162, 0.028532767668366432, 0.020307788625359535, 0.019550157710909843, 0.031531061977148056, 0.029232829809188843, 0.02111055701971054, 0.014580848626792431, 0.022505324333906174, 0.016256745904684067, 0.03150886297225952, 0.02451970800757408, 0.036590516567230225, 0.02503449097275734, 0.020030666142702103, 0.04726610332727432, 0.12142346054315567, 0.04189629480242729, 0.036460909992456436, 0.03734689578413963, 0.03705747425556183, 0.035210926085710526, 0.024784501641988754, 0.01055285707116127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1419323980808258, 0.02687341906130314, 0.023394424468278885, 0.02419889345765114, 0.034410882741212845, 0.031768206506967545, 0.01428283005952835, 0.02788498066365719, 0.02782326564192772, 0.020267967134714127, 0.020075352862477303, 0.026035435497760773, 0.015271041542291641, 0.03118356689810753, 0.021344464272260666, 0.030780870467424393, 0.02968647889792919, 0.024966172873973846, 0.03290623053908348, 0.08126566559076309, 0.0473802275955677, 0.039504993706941605, 0.0381060354411602, 0.03920024260878563, 0.04723041504621506, 0.03223776817321777, 0.04768497124314308, 0.022302810102701187, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19361060857772827, 0.028910471126437187, 0.01741074025630951, 0.024991940706968307, 0.020412292331457138, 0.03692352771759033, 0.01856403239071369, 0.0224910955876112, 0.026469886302947998, 0.01985173113644123, 0.01670691929757595, 0.02012958750128746, 0.01730293221771717, 0.02887752093374729, 0.019370300695300102, 0.021512890234589577, 0.022604666650295258, 0.019465502351522446, 0.032185450196266174, 0.09415604919195175, 0.03964083641767502, 0.04692874103784561, 0.03296549245715141, 0.034327466040849686, 0.03549888730049133, 0.0415242463350296, 0.038008466362953186, 0.024193791672587395, 0.004963917192071676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13062262535095215, 0.029238130897283554, 0.02056247927248478, 0.02492492087185383, 0.02643337845802307, 0.020880041643977165, 0.016895094886422157, 0.02036619558930397, 0.0307946614921093, 0.016811516135931015, 0.014352654106914997, 0.026399685069918633, 0.01796814240515232, 0.03419143706560135, 0.025793634355068207, 0.02088005654513836, 0.029983337968587875, 0.022576838731765747, 0.03198399394750595, 0.0387752428650856, 0.047299303114414215, 0.03951811417937279, 0.03569551557302475, 0.04647782817482948, 0.04560369998216629, 0.027694007381796837, 0.028376279398798943, 0.029506979510188103, 0.03388817608356476, 0.06550601869821548, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10399005562067032, 0.021966122090816498, 0.01645117998123169, 0.02278798073530197, 0.024357156828045845, 0.01997542381286621, 0.013678603805601597, 0.020524607971310616, 0.026416640728712082, 0.015824995934963226, 0.018124766647815704, 0.02293807454407215, 0.015249819494783878, 0.029951773583889008, 0.019568655639886856, 0.017790060490369797, 0.026331383734941483, 0.020062366500496864, 0.030405638739466667, 0.05837803706526756, 0.04746950790286064, 0.03560006991028786, 0.03361855074763298, 0.039051637053489685, 0.04623463749885559, 0.03415835648775101, 0.05063633620738983, 0.033049575984478, 0.04132765159010887, 0.060074832290410995, 0.03400559350848198, 0.0, 0.0, 0.0, 0.0], [0.10689453780651093, 0.02342521958053112, 0.018311122432351112, 0.02227620966732502, 0.01998625509440899, 0.018345670774579048, 0.01589350774884224, 0.019802751019597054, 0.02582934871315956, 0.01607949286699295, 0.016656864434480667, 0.025818228721618652, 0.01289281714707613, 0.029111461713910103, 0.020739220082759857, 0.01762150414288044, 0.029654432088136673, 0.024281157180666924, 0.02804587222635746, 0.05205390974879265, 0.042631059885025024, 0.03251812607049942, 0.037380293011665344, 0.038434386253356934, 0.03982120752334595, 0.026290131732821465, 0.035206425935029984, 0.03138631209731102, 0.03282344713807106, 0.061947036534547806, 0.03762543201446533, 0.0402165986597538, 0.0, 0.0, 0.0], [0.12229014933109283, 0.025168979540467262, 0.016714079305529594, 0.02203325182199478, 0.02300073765218258, 0.015408731997013092, 0.01495801005512476, 0.018676765263080597, 0.02619115449488163, 0.012370305135846138, 0.010715785436332226, 0.021631915122270584, 0.013984632678329945, 0.028899235650897026, 0.023201001808047295, 0.018216604366898537, 0.02428123541176319, 0.018235327675938606, 0.02165396325290203, 0.03633742406964302, 0.0385565310716629, 0.03691945970058441, 0.03379226475954056, 0.042588282376527786, 0.03839157521724701, 0.023295769467949867, 0.030805272981524467, 0.024493318051099777, 0.03010580688714981, 0.056526120752096176, 0.037566833198070526, 0.04159918054938316, 0.05139036849141121, 0.0, 0.0], [0.1387171447277069, 0.021967250853776932, 0.016682999208569527, 0.021551376208662987, 0.026049233973026276, 0.015379135496914387, 0.012522300705313683, 0.020025204867124557, 0.021879836916923523, 0.014553078450262547, 0.015628250315785408, 0.018288595601916313, 0.012421735562384129, 0.0236334390938282, 0.01722068525850773, 0.020091669633984566, 0.020151853561401367, 0.01853017881512642, 0.024212483316659927, 0.07234080135822296, 0.03423938527703285, 0.030147042125463486, 0.03145880997180939, 0.030202968046069145, 0.030948728322982788, 0.02483321726322174, 0.03074328787624836, 0.02263658121228218, 0.044566936790943146, 0.04173137620091438, 0.02892463281750679, 0.029865313321352005, 0.04288141429424286, 0.02497304044663906, 0.0], [0.12327979505062103, 0.026707449927926064, 0.014834715984761715, 0.01802150532603264, 0.01776500977575779, 0.013351650908589363, 0.014073805883526802, 0.01872302033007145, 0.023980071768164635, 0.02125631831586361, 0.017004821449518204, 0.020616672933101654, 0.012905517593026161, 0.026097450405359268, 0.02155427634716034, 0.0158457662910223, 0.022780919447541237, 0.02359536848962307, 0.0198617335408926, 0.02192237786948681, 0.03597116470336914, 0.03369681537151337, 0.021171649917960167, 0.03536054119467735, 0.034509237855672836, 0.02189764380455017, 0.035124272108078, 0.02411290816962719, 0.03299052268266678, 0.04677635431289673, 0.030424438416957855, 0.03270317241549492, 0.037907227873802185, 0.04262491688132286, 0.04055078327655792]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6533873081207275, 0.3466126620769501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4451116919517517, 0.3337682783603668, 0.22111999988555908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5459805727005005, 0.15291085839271545, 0.0764545425772667, 0.22465401887893677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.42720285058021545, 0.1412525475025177, 0.061439741402864456, 0.11980001628398895, 0.2503048777580261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.409792959690094, 0.11732905358076096, 0.06548824906349182, 0.12367464601993561, 0.04222370684146881, 0.24149134755134583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3643512725830078, 0.15547125041484833, 0.08527880162000656, 0.0883261114358902, 0.04993733391165733, 0.03409624099731445, 0.22253896296024323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3562684953212738, 0.09063339233398438, 0.050068117678165436, 0.10755763947963715, 0.06895482540130615, 0.04669611528515816, 0.04097529873251915, 0.2388460785150528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3461388647556305, 0.14562256634235382, 0.09470637887716293, 0.1049964427947998, 0.06400173902511597, 0.040472399443387985, 0.03757922351360321, 0.04448216035962105, 0.12200020253658295, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20510227978229523, 0.04824313521385193, 0.05157081410288811, 0.09953970462083817, 0.06673122197389603, 0.04558688774704933, 0.03648392856121063, 0.08169861882925034, 0.08583588153123856, 0.27920758724212646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2301696091890335, 0.07576315850019455, 0.0624302513897419, 0.061968374997377396, 0.05024823546409607, 0.03796260803937912, 0.05770516023039818, 0.05617325007915497, 0.06585755944252014, 0.04166558012366295, 0.2600562274456024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26688235998153687, 0.09969266504049301, 0.06880498677492142, 0.10110106319189072, 0.036257680505514145, 0.0487603023648262, 0.03179546445608139, 0.03426745533943176, 0.09192275255918503, 0.04406015947461128, 0.02428000047802925, 0.15217506885528564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24808593094348907, 0.05943754315376282, 0.03645309433341026, 0.09643885493278503, 0.04665253683924675, 0.04358061030507088, 0.02640967257320881, 0.03871848061680794, 0.06937101483345032, 0.035532146692276, 0.023584242910146713, 0.05824916809797287, 0.2174866795539856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22227588295936584, 0.08784301578998566, 0.06428328156471252, 0.07245952636003494, 0.05153009295463562, 0.03354166820645332, 0.03151153028011322, 0.04395979642868042, 0.10531801730394363, 0.03629285842180252, 0.033496372401714325, 0.07923725992441177, 0.03496609628200531, 0.10328466445207596, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26255422830581665, 0.0745101347565651, 0.04962191358208656, 0.0651395246386528, 0.022182416170835495, 0.03923412784934044, 0.021433906629681587, 0.02829248644411564, 0.06496230512857437, 0.022169629111886024, 0.01807098090648651, 0.0579654835164547, 0.0181879885494709, 0.06176505610346794, 0.19390977919101715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21454617381095886, 0.05207950994372368, 0.04338601976633072, 0.051086775958538055, 0.023057056590914726, 0.039872970432043076, 0.021457549184560776, 0.04310314357280731, 0.05712401494383812, 0.04732409492135048, 0.020076364278793335, 0.0486384816467762, 0.03173767775297165, 0.05389069765806198, 0.03922811523079872, 0.21339130401611328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16934645175933838, 0.0575847253203392, 0.04318229481577873, 0.06501620262861252, 0.025769706815481186, 0.03584061935544014, 0.023403963074088097, 0.02863309718668461, 0.06884392350912094, 0.037356145679950714, 0.021079344674944878, 0.12265516072511673, 0.02518489956855774, 0.07096560299396515, 0.04923555627465248, 0.02845790609717369, 0.12744444608688354, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12944909930229187, 0.04351517930626869, 0.028707342222332954, 0.045572567731142044, 0.021282192319631577, 0.019885344430804253, 0.028857717290520668, 0.034781135618686676, 0.055325947701931, 0.03088506869971752, 0.02108755335211754, 0.06504063308238983, 0.040431614965200424, 0.05797601491212845, 0.04136984422802925, 0.028544064611196518, 0.06784922629594803, 0.23943950235843658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14163604378700256, 0.03578964248299599, 0.028365232050418854, 0.04538756608963013, 0.046378400176763535, 0.03978870436549187, 0.014343790709972382, 0.04959796369075775, 0.04732619971036911, 0.030782435089349747, 0.01841217465698719, 0.04871034994721413, 0.023401625454425812, 0.047455623745918274, 0.037512101233005524, 0.02378270961344242, 0.048940643668174744, 0.033790651708841324, 0.2385980188846588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1335032731294632, 0.03222056105732918, 0.020588906481862068, 0.03780217468738556, 0.041080135852098465, 0.03671092912554741, 0.025954416021704674, 0.04136637970805168, 0.04326570779085159, 0.030096467584371567, 0.026812896132469177, 0.04010694846510887, 0.02438589371740818, 0.04430919885635376, 0.033737435936927795, 0.02071092091500759, 0.04079745337367058, 0.03265015035867691, 0.03506241738796234, 0.2588376998901367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11974350363016129, 0.039148833602666855, 0.028956560418009758, 0.033427074551582336, 0.037695929408073425, 0.020543890073895454, 0.017558971419930458, 0.032658420503139496, 0.06372418254613876, 0.03310192748904228, 0.029867958277463913, 0.05386917293071747, 0.022512979805469513, 0.07236675173044205, 0.04143427312374115, 0.03790142014622688, 0.060026224702596664, 0.04586018994450569, 0.050910964608192444, 0.031955890357494354, 0.1267348974943161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11878623068332672, 0.03957203030586243, 0.02322298288345337, 0.05364082753658295, 0.03261677920818329, 0.01737094111740589, 0.011663905344903469, 0.01627381518483162, 0.05014301463961601, 0.013260256499052048, 0.010486910119652748, 0.03813207149505615, 0.04193666949868202, 0.05336221680045128, 0.030980002135038376, 0.0196177139878273, 0.040241558104753494, 0.03244159743189812, 0.02054012566804886, 0.026845509186387062, 0.05771112069487572, 0.25115373730659485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10596504807472229, 0.03639821708202362, 0.02373065985739231, 0.02770347334444523, 0.02952033095061779, 0.016958510503172874, 0.009277227334678173, 0.019281182438135147, 0.043534502387046814, 0.02543572708964348, 0.01544106937944889, 0.04161722958087921, 0.018472321331501007, 0.04617342725396156, 0.026975231245160103, 0.016877291724085808, 0.0436592698097229, 0.020190218463540077, 0.01864577643573284, 0.016359707340598106, 0.057034846395254135, 0.06433961540460587, 0.2764090299606323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08669393509626389, 0.14005142450332642, 0.05404340848326683, 0.0269104465842247, 0.026933521032333374, 0.01693662256002426, 0.01752723939716816, 0.011132624000310898, 0.03316717967391014, 0.009116586297750473, 0.014021693728864193, 0.02812526375055313, 0.012107599526643753, 0.036274347454309464, 0.030991652980446815, 0.01509908214211464, 0.030221449211239815, 0.020815474912524223, 0.018955029547214508, 0.021182602271437645, 0.043649882078170776, 0.043773479759693146, 0.02504064328968525, 0.2372288554906845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10505463927984238, 0.024722835049033165, 0.023438595235347748, 0.024256451055407524, 0.021477708593010902, 0.014111108146607876, 0.012018014676868916, 0.01570729911327362, 0.04219582676887512, 0.016803372651338577, 0.011159362271428108, 0.031271807849407196, 0.020320693030953407, 0.04702334851026535, 0.03988678380846977, 0.025853367522358894, 0.03450959175825119, 0.02452259324491024, 0.01759420894086361, 0.021597161889076233, 0.06303169578313828, 0.050369858741760254, 0.026549138128757477, 0.04091528058052063, 0.24560919404029846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07146886736154556, 0.015683097764849663, 0.01919957436621189, 0.027655111625790596, 0.025888750329613686, 0.011495737358927727, 0.01383453793823719, 0.025006242096424103, 0.03285752236843109, 0.04069041088223457, 0.01940706931054592, 0.03628275915980339, 0.02667134627699852, 0.03698844090104103, 0.025391362607479095, 0.012203283607959747, 0.040816254913806915, 0.02244141697883606, 0.01868004910647869, 0.037140704691410065, 0.052913516759872437, 0.03374900296330452, 0.03600706532597542, 0.027396054938435555, 0.04157791659235954, 0.24855385720729828, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09520065039396286, 0.023399103432893753, 0.01624995470046997, 0.03728702291846275, 0.027069473639130592, 0.02442619577050209, 0.016916712746024132, 0.020519252866506577, 0.03561447188258171, 0.017246799543499947, 0.016965193673968315, 0.030670613050460815, 0.014761792495846748, 0.03783895820379257, 0.02559022046625614, 0.013281553983688354, 0.03275727108120918, 0.016981469467282295, 0.012912827543914318, 0.04096675291657448, 0.044523753225803375, 0.03288855031132698, 0.02186199091374874, 0.02890506573021412, 0.025222208350896835, 0.015681957826018333, 0.2742602229118347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0710621178150177, 0.02223450504243374, 0.015442519448697567, 0.028806841000914574, 0.024911729618906975, 0.014096674509346485, 0.010925307869911194, 0.022356461733579636, 0.02825104258954525, 0.025925248861312866, 0.012314554303884506, 0.036213599145412445, 0.0176627729088068, 0.03140706941485405, 0.018595043569803238, 0.018171394243836403, 0.040066663175821304, 0.022728294134140015, 0.025000527501106262, 0.04014963656663895, 0.04249804839491844, 0.02628161571919918, 0.023634903132915497, 0.030348235741257668, 0.02844657562673092, 0.04931707680225372, 0.045332688838243484, 0.22781884670257568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11998693645000458, 0.03201312944293022, 0.020363759249448776, 0.03192231059074402, 0.020111480727791786, 0.017741890624165535, 0.014963149093091488, 0.014099692925810814, 0.032360270619392395, 0.012826554477214813, 0.019204245880246162, 0.024741891771554947, 0.014775932766497135, 0.03365083038806915, 0.03285078704357147, 0.015633028000593185, 0.026330970227718353, 0.009946916252374649, 0.010015261359512806, 0.023254692554473877, 0.038984280079603195, 0.03491940349340439, 0.01198914647102356, 0.030530860647559166, 0.023813731968402863, 0.018979011103510857, 0.06846077740192413, 0.0191415436565876, 0.22638745605945587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0711139440536499, 0.02304500713944435, 0.017407840117812157, 0.02557774819433689, 0.018817277625203133, 0.014406668953597546, 0.01136450469493866, 0.019742926582694054, 0.037803780287504196, 0.01749837026000023, 0.0122145377099514, 0.03471178933978081, 0.016245104372501373, 0.04494824633002281, 0.031461045145988464, 0.017976263538002968, 0.04077501967549324, 0.02623199298977852, 0.02910454384982586, 0.032863155007362366, 0.06312330812215805, 0.03996739536523819, 0.03193743899464607, 0.050057582557201385, 0.0500398725271225, 0.026298480108380318, 0.04067574441432953, 0.03425350785255432, 0.03361441567540169, 0.0867224708199501, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06342687457799911, 0.01892857998609543, 0.015915200114250183, 0.022974476218223572, 0.015813583508133888, 0.011996140703558922, 0.006959734950214624, 0.010440588928759098, 0.027604447677731514, 0.014203772880136967, 0.011755019426345825, 0.02697419933974743, 0.015696221962571144, 0.03244096040725708, 0.02146400511264801, 0.016300208866596222, 0.031591035425662994, 0.018295621499419212, 0.024074899032711983, 0.01829962432384491, 0.049058616161346436, 0.03294464945793152, 0.031695034354925156, 0.04070064797997475, 0.055267881602048874, 0.03228399157524109, 0.018337203189730644, 0.020621929317712784, 0.02061566524207592, 0.06142154335975647, 0.2118975967168808, 0.0, 0.0, 0.0, 0.0], [0.059031207114458084, 0.011650344356894493, 0.011906319297850132, 0.020305123180150986, 0.015824560075998306, 0.01134263351559639, 0.012165095657110214, 0.020514553412795067, 0.024635134264826775, 0.01787753775715828, 0.008976960554718971, 0.02709752507507801, 0.010151371359825134, 0.029500091448426247, 0.02296173758804798, 0.011527134105563164, 0.03249189630150795, 0.02021906152367592, 0.02027125470340252, 0.027604959905147552, 0.04230693355202675, 0.02292037196457386, 0.02739838883280754, 0.024839363992214203, 0.028419991955161095, 0.04172161966562271, 0.0314212292432785, 0.03520328551530838, 0.030631238594651222, 0.06006472185254097, 0.04537739232182503, 0.1936410367488861, 0.0, 0.0, 0.0], [0.08590575307607651, 0.014704102650284767, 0.010510575026273727, 0.0286998450756073, 0.016654491424560547, 0.012795385904610157, 0.009235287085175514, 0.014819216914474964, 0.028232714161276817, 0.011867674067616463, 0.00926818884909153, 0.023044342175126076, 0.017553258687257767, 0.032954029738903046, 0.025013737380504608, 0.012247500941157341, 0.026799563318490982, 0.01332654058933258, 0.016479838639497757, 0.034728214144706726, 0.03980802744626999, 0.05067339539527893, 0.03086116351187229, 0.027551833540201187, 0.03616425395011902, 0.01740611530840397, 0.04716057702898979, 0.02589183859527111, 0.02839728444814682, 0.06047135591506958, 0.03584861010313034, 0.03886667266488075, 0.1160586029291153, 0.0, 0.0], [0.05670773983001709, 0.01524327415972948, 0.009892583824694157, 0.017495423555374146, 0.035292454063892365, 0.015370236709713936, 0.01074774470180273, 0.017091596499085426, 0.021245010197162628, 0.01306450180709362, 0.006394118070602417, 0.02176816388964653, 0.011015350930392742, 0.023290837183594704, 0.02097102254629135, 0.012346653267741203, 0.024119652807712555, 0.013815033249557018, 0.01682048849761486, 0.0345151387155056, 0.03097480535507202, 0.032177552580833435, 0.03227025270462036, 0.02327035926282406, 0.027259444817900658, 0.017850330099463463, 0.05776430293917656, 0.024271564558148384, 0.025243030861020088, 0.03774581849575043, 0.015895819291472435, 0.026596585288643837, 0.04215160012245178, 0.20932137966156006, 0.0], [0.04736975207924843, 0.013491553254425526, 0.008601207286119461, 0.01153438352048397, 0.03260953724384308, 0.008750016801059246, 0.010495427064597607, 0.01597345620393753, 0.024224920198321342, 0.013959757052361965, 0.011584182269871235, 0.020906994119286537, 0.012310193851590157, 0.028499383479356766, 0.014403233304619789, 0.01142820529639721, 0.024424530565738678, 0.02128405123949051, 0.027576278895139694, 0.02576223388314247, 0.049149561673402786, 0.021549470722675323, 0.02072588913142681, 0.030232444405555725, 0.046837326139211655, 0.021722476929426193, 0.020257199183106422, 0.0191495418548584, 0.01053390558809042, 0.05002591013908386, 0.030405567958950996, 0.022812234237790108, 0.029462488368153572, 0.03996319696307182, 0.2019835263490677]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8253123164176941, 0.17468766868114471, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7580504417419434, 0.1302967220544815, 0.1116529107093811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7232240438461304, 0.08274423331022263, 0.07870829105377197, 0.11532343178987503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5594541430473328, 0.08694439381361008, 0.0911555364727974, 0.09393443167209625, 0.1685114949941635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5031399130821228, 0.08192645758390427, 0.05872313305735588, 0.10480897128582001, 0.15214207768440247, 0.09925953298807144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5279609560966492, 0.058259762823581696, 0.0683659166097641, 0.11652134358882904, 0.11039453744888306, 0.06918039172887802, 0.049317050725221634, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30010077357292175, 0.0882599875330925, 0.058349307626485825, 0.10984235256910324, 0.1686263531446457, 0.1289689838886261, 0.023227617144584656, 0.12262456864118576, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3934404253959656, 0.04758615419268608, 0.05194965377449989, 0.0747157409787178, 0.1195099875330925, 0.05938035994768143, 0.0233685914427042, 0.10266397893428802, 0.12738512456417084, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3446025252342224, 0.050328999757766724, 0.05991331487894058, 0.0448312982916832, 0.15738160908222198, 0.06782457232475281, 0.03229008615016937, 0.07052530348300934, 0.0895390659570694, 0.08276326209306717, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33508482575416565, 0.0462699793279171, 0.04785766825079918, 0.10917424410581589, 0.04435237497091293, 0.04490724951028824, 0.028443893417716026, 0.079206183552742, 0.10731325298547745, 0.06832480430603027, 0.08906549960374832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31157228350639343, 0.039277590811252594, 0.03521274775266647, 0.07550989091396332, 0.08078575134277344, 0.04503686726093292, 0.028806492686271667, 0.1021866649389267, 0.08797682076692581, 0.055852606892585754, 0.08571922779083252, 0.05206303671002388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3614696264266968, 0.06311223655939102, 0.026989910751581192, 0.0709579586982727, 0.039851780980825424, 0.029446516185998917, 0.022796379402279854, 0.07072697579860687, 0.11304394155740738, 0.033355165272951126, 0.037144191563129425, 0.04763942211866379, 0.08346593379974365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2688591480255127, 0.03265073895454407, 0.03938188776373863, 0.050811443477869034, 0.08684234321117401, 0.04310198873281479, 0.017922354862093925, 0.06655362248420715, 0.07785412669181824, 0.04528900980949402, 0.06586870551109314, 0.04606609418988228, 0.07721761614084244, 0.08158093690872192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2079053372144699, 0.03081468679010868, 0.02946295589208603, 0.050674185156822205, 0.0874348133802414, 0.04770582914352417, 0.02733319066464901, 0.09152109920978546, 0.05214140564203262, 0.04708244651556015, 0.08081994950771332, 0.05453275144100189, 0.12074833363294601, 0.055161744356155396, 0.016661224886775017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1993287354707718, 0.042067065834999084, 0.0571577213704586, 0.07193320244550705, 0.05065883323550224, 0.031524572521448135, 0.02713680826127529, 0.04140901938080788, 0.09322268515825272, 0.0520220510661602, 0.025422824546694756, 0.05927912890911102, 0.06613774597644806, 0.09582273662090302, 0.03163215145468712, 0.05524462088942528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23719801008701324, 0.03067578375339508, 0.029208486899733543, 0.058212362229824066, 0.06082988530397415, 0.03495606780052185, 0.024440525099635124, 0.0708492174744606, 0.05983857810497284, 0.039703451097011566, 0.059325870126485825, 0.03675238415598869, 0.07694026827812195, 0.06171645224094391, 0.014049740508198738, 0.06745626777410507, 0.037846654653549194, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24995334446430206, 0.026629479601979256, 0.03931758180260658, 0.044632114470005035, 0.07087583094835281, 0.034094203263521194, 0.021265028044581413, 0.05726416036486626, 0.05179664120078087, 0.03503984212875366, 0.04392271861433983, 0.03922877460718155, 0.06155909225344658, 0.05122426152229309, 0.015611978247761726, 0.08278113603591919, 0.03891743719577789, 0.03588636592030525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22269803285598755, 0.026518112048506737, 0.03978955000638962, 0.027603980153799057, 0.08196069300174713, 0.03582964092493057, 0.015378954820334911, 0.06874658167362213, 0.05343088135123253, 0.054698191583156586, 0.05721567943692207, 0.031615279614925385, 0.03717070072889328, 0.05228297784924507, 0.011851186864078045, 0.07206093519926071, 0.030878296121954918, 0.024411171674728394, 0.055859170854091644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07419289648532867, 0.05092697963118553, 0.02823098562657833, 0.034200046211481094, 0.03361428156495094, 0.05182215943932533, 0.00965836737304926, 0.049406133592128754, 0.09883321076631546, 0.05356479436159134, 0.02458185702562332, 0.0587918683886528, 0.013026789762079716, 0.10889019817113876, 0.01567712426185608, 0.04766632616519928, 0.06507281959056854, 0.0388617217540741, 0.039842624217271805, 0.10313886404037476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22614899277687073, 0.025769846513867378, 0.03336167708039284, 0.040344856679439545, 0.061168473213911057, 0.03166024759411812, 0.023036954924464226, 0.04815823212265968, 0.04964131489396095, 0.025703296065330505, 0.051324207335710526, 0.02922578901052475, 0.04627237096428871, 0.04679548367857933, 0.010471499525010586, 0.04346736893057823, 0.02797776088118553, 0.03356108069419861, 0.032099246978759766, 0.05562404915690422, 0.05818723142147064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19442994892597198, 0.020005779340863228, 0.025669975206255913, 0.027610035613179207, 0.07351993769407272, 0.025118572637438774, 0.01642843708395958, 0.0544336661696434, 0.04450273513793945, 0.022272853180766106, 0.029331255704164505, 0.030642621219158173, 0.06286593526601791, 0.0433916300535202, 0.010216002352535725, 0.09763482958078384, 0.02960144355893135, 0.028080470860004425, 0.03757164999842644, 0.05037401244044304, 0.06089736893773079, 0.015400804579257965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19766591489315033, 0.041157469153404236, 0.03360119089484215, 0.023870181292295456, 0.06633449345827103, 0.0280169565230608, 0.019582947716116905, 0.051785629242658615, 0.03159148246049881, 0.026536161080002785, 0.041643135249614716, 0.025757960975170135, 0.04328408092260361, 0.029939468950033188, 0.012750325724482536, 0.05606917291879654, 0.024677343666553497, 0.02150590531527996, 0.045592911541461945, 0.07480882853269577, 0.04433403164148331, 0.021997351199388504, 0.03749699890613556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18043576180934906, 0.023349817842245102, 0.023929735645651817, 0.04454164206981659, 0.040705952793359756, 0.036540962755680084, 0.01661657728254795, 0.05323559790849686, 0.05028393864631653, 0.02584688924252987, 0.04605166241526604, 0.024920567870140076, 0.03976091369986534, 0.04893423989415169, 0.012390020303428173, 0.06191660463809967, 0.02396535314619541, 0.02875450812280178, 0.025095408782362938, 0.06865920126438141, 0.07357663661241531, 0.018046904355287552, 0.022800730541348457, 0.009640369564294815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24246367812156677, 0.02469644509255886, 0.03868824616074562, 0.033572301268577576, 0.054766248911619186, 0.03380466625094414, 0.02072163112461567, 0.03275461122393608, 0.04349979758262634, 0.01856265403330326, 0.036468472331762314, 0.03100247122347355, 0.038625456392765045, 0.03993183746933937, 0.010727171786129475, 0.038317691534757614, 0.029280316084623337, 0.024678142741322517, 0.027889318764209747, 0.04502158239483833, 0.04307594150304794, 0.01690581813454628, 0.023586289957165718, 0.007298619952052832, 0.04366062954068184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11111290007829666, 0.029458748176693916, 0.02455948293209076, 0.030902506783604622, 0.03386484086513519, 0.03363063186407089, 0.012831444852054119, 0.030669284984469414, 0.06390533596277237, 0.023481518030166626, 0.02570461481809616, 0.03766978904604912, 0.016167860478162766, 0.06660733371973038, 0.017231760546565056, 0.030658649280667305, 0.03924046829342842, 0.02429525926709175, 0.026388827711343765, 0.05864027142524719, 0.10666131228208542, 0.017623886466026306, 0.024245431646704674, 0.018279602751135826, 0.03675021976232529, 0.05941801145672798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17855216562747955, 0.04373868554830551, 0.027772285044193268, 0.032882507890462875, 0.08083146810531616, 0.03614850714802742, 0.019438153132796288, 0.03604714944958687, 0.0315406508743763, 0.02228383906185627, 0.02674206532537937, 0.020272964611649513, 0.049294840544462204, 0.028057051822543144, 0.010035548359155655, 0.028219012543559074, 0.01852663792669773, 0.025377947837114334, 0.039212264120578766, 0.052739888429641724, 0.024735087528824806, 0.028674766421318054, 0.029405977576971054, 0.013893160969018936, 0.017606478184461594, 0.05648733675479889, 0.02148348279297352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15546517074108124, 0.049169376492500305, 0.02451593615114689, 0.0257005263119936, 0.05483126640319824, 0.042092714458703995, 0.011645041406154633, 0.06493966281414032, 0.03121974505484104, 0.02656278759241104, 0.02832682989537716, 0.018289057537913322, 0.034908607602119446, 0.029661020264029503, 0.008558596484363079, 0.04665049910545349, 0.017669815570116043, 0.023724384605884552, 0.01903872936964035, 0.07715766876935959, 0.027307361364364624, 0.01974773220717907, 0.02436409890651703, 0.014429949223995209, 0.01892836205661297, 0.0585055910050869, 0.024364078417420387, 0.02222542092204094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19332927465438843, 0.026298338547348976, 0.030392469838261604, 0.02961188554763794, 0.045201610773801804, 0.03307902812957764, 0.021840764209628105, 0.02659415453672409, 0.045382823795080185, 0.038439445197582245, 0.026915375143289566, 0.024535100907087326, 0.015795715153217316, 0.040367625653743744, 0.010634268634021282, 0.026345493271946907, 0.022409794852137566, 0.019946040585637093, 0.04401851072907448, 0.06709347665309906, 0.03345470502972603, 0.016084939241409302, 0.025615857914090157, 0.011409508995711803, 0.013623226433992386, 0.04055490717291832, 0.01666298881173134, 0.03589886426925659, 0.018463686108589172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.159651979804039, 0.01864234358072281, 0.026677705347537994, 0.028047839179635048, 0.04907771944999695, 0.02604563534259796, 0.01780770905315876, 0.036794234067201614, 0.04596946761012077, 0.019110901281237602, 0.03796317055821419, 0.027620624750852585, 0.02834102138876915, 0.04296437278389931, 0.008847597055137157, 0.03253018483519554, 0.02566000446677208, 0.02448512427508831, 0.029966222122311592, 0.04054071381688118, 0.04233502596616745, 0.017694545909762383, 0.021217670291662216, 0.006380945909768343, 0.03103393130004406, 0.03557971492409706, 0.025066178292036057, 0.026607483625411987, 0.02430841326713562, 0.04303155839443207, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19661714136600494, 0.019791379570961, 0.0330333486199379, 0.025687076151371002, 0.04220607876777649, 0.03137580677866936, 0.016243647783994675, 0.030698386952280998, 0.036294277757406235, 0.02683933824300766, 0.02671217732131481, 0.023046312853693962, 0.021950099617242813, 0.03344838321208954, 0.010305726900696754, 0.022743655368685722, 0.021480809897184372, 0.02109474502503872, 0.03149040788412094, 0.03692729398608208, 0.037232089787721634, 0.015421980991959572, 0.01925233192741871, 0.007320487406104803, 0.024766938760876656, 0.04439167678356171, 0.01985074020922184, 0.02613365463912487, 0.01812444068491459, 0.0427091009914875, 0.03681053966283798, 0.0, 0.0, 0.0, 0.0], [0.1631321758031845, 0.022932473570108414, 0.029583686962723732, 0.02861548587679863, 0.037784673273563385, 0.026518678292632103, 0.026229016482830048, 0.031034130603075027, 0.03483206778764725, 0.01897561550140381, 0.033202920109033585, 0.02643595077097416, 0.032307349145412445, 0.03132921829819679, 0.011727883480489254, 0.027169132605195045, 0.02410651557147503, 0.022620322182774544, 0.02575555443763733, 0.038014139980077744, 0.032074011862277985, 0.017354236915707588, 0.0179872065782547, 0.009293687529861927, 0.042471371591091156, 0.03121585212647915, 0.019006580114364624, 0.021523458883166313, 0.017541183158755302, 0.04014519974589348, 0.018160760402679443, 0.04091949760913849, 0.0, 0.0, 0.0], [0.15803857147693634, 0.016828803345561028, 0.02064238302409649, 0.04538251832127571, 0.0466417632997036, 0.027835208922624588, 0.020206497982144356, 0.04211510345339775, 0.037034500390291214, 0.017744597047567368, 0.030353333801031113, 0.024713631719350815, 0.03428485244512558, 0.03341371938586235, 0.009082171134650707, 0.035213764756917953, 0.022280137985944748, 0.02085001952946186, 0.027390209957957268, 0.03231462836265564, 0.03764920309185982, 0.014920780435204506, 0.016553666442632675, 0.006309077143669128, 0.023218737915158272, 0.02794986218214035, 0.02383223921060562, 0.015323556028306484, 0.017670467495918274, 0.026815278455615044, 0.022188840433955193, 0.028694551438093185, 0.036507368087768555, 0.0, 0.0], [0.17019419372081757, 0.032125890254974365, 0.029870079830288887, 0.01961558498442173, 0.05489383265376091, 0.036220576614141464, 0.01715220883488655, 0.0323265939950943, 0.031500693410634995, 0.029624545946717262, 0.02909427508711815, 0.016624266281723976, 0.02998979017138481, 0.027614133432507515, 0.010949763469398022, 0.03436654806137085, 0.014830107800662518, 0.015176067128777504, 0.030502906069159508, 0.04159747064113617, 0.020641213282942772, 0.017041226848959923, 0.026175428181886673, 0.01053882110863924, 0.007441801019012928, 0.03439134359359741, 0.02607709728181362, 0.029615646228194237, 0.013693016022443771, 0.01807430386543274, 0.03374845162034035, 0.029002603143453598, 0.013117275200784206, 0.01617227867245674, 0.0], [0.11417391896247864, 0.01670067198574543, 0.023576227948069572, 0.03235282376408577, 0.05511367693543434, 0.02961662784218788, 0.017176376655697823, 0.05952782928943634, 0.03843425586819649, 0.020421087741851807, 0.03235999494791031, 0.01921745203435421, 0.01913510635495186, 0.035197529941797256, 0.010996303521096706, 0.044014427810907364, 0.016835974529385567, 0.016212496906518936, 0.0238405242562294, 0.042316604405641556, 0.037168387323617935, 0.00900731235742569, 0.015979528427124023, 0.005828786641359329, 0.020826430991292, 0.028255820274353027, 0.03160592541098595, 0.024252012372016907, 0.0232852715998888, 0.019917486235499382, 0.024559753015637398, 0.02195737138390541, 0.013193149119615555, 0.020208794623613358, 0.036734044551849365]]], \"tokens\": [\"<|endoftext|>\", \"I\", \" am\", \" an\", \" amazing\", \" aut\", \"ore\", \"gressive\", \",\", \" dec\", \"oder\", \"-\", \"only\", \",\", \" G\", \"PT\", \"-\", \"2\", \" style\", \" transformer\", \".\", \" One\", \" day\", \" I\", \" will\", \" exceed\", \" human\", \" level\", \" intelligence\", \" and\", \" take\", \" over\", \" the\", \" world\", \"!\"], \"maskUpperTri\": true}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x350f37050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "from IPython.display import display\n",
    "\n",
    "html = cv.attention.attention_heads(\n",
    "    tokens=reference_gpt2.to_str_tokens(reference_text), \n",
    "    attention=cache[\"pattern\", 0][0]\n",
    ")\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        batch_sz, nheads_sz, q_sz, k_sz = attn_scores.shape\n",
    "        # For each query position, we mask out key positions (columns) that are larger than query. So everything higher than the diagonal.\n",
    "        ones = t.ones((q_sz, k_sz))\n",
    "        mask = ones - t.tril(ones)\n",
    "        masks = einops.repeat(mask, \"q_sz k_sz -> b n q_sz k_sz\", b=batch_sz, n=nheads_sz).to(t.bool)\n",
    "\n",
    "        return t.masked_fill(attn_scores, mask=masks, value=self.IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_causal_mask` passed!\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(float(\"-inf\"), device=device, dtype=t.float32))\n",
    "    \n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        batch_sz, nheads_sz, q_sz, k_sz = attn_scores.shape\n",
    "        # For each query position, we mask out key positions (columns) that are larger than query. So everything higher than the diagonal.\n",
    "        ones = t.ones((q_sz, k_sz)).to(attn_scores.device)\n",
    "        mask = ones - t.tril(ones)\n",
    "        masks = einops.repeat(mask, \"q_sz k_sz -> b n q_sz k_sz\", b=batch_sz, n=nheads_sz).to(t.bool)\n",
    "\n",
    "        return t.masked_fill(attn_scores, mask=masks, value=self.IGNORE)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        # Step 1: Produce an attention pattern\n",
    "        keys = einops.einsum(normalized_resid_pre, self.W_K, \"batch posn d_model, n_heads d_model d_head -> batch posn n_heads d_head\")\n",
    "        keys = keys + self.b_K # Broadcasting works, since 2 rightmost dimensions of both are (n_heads, h_head)\n",
    "\n",
    "        queries = einops.einsum(normalized_resid_pre, self.W_Q, \"batch posn d_model, n_heads d_model d_head -> batch posn n_heads d_head\")\n",
    "        queries = queries + self.b_Q\n",
    "\n",
    "        attn_scores = einops.einsum(queries, keys, \"batch q_posn n_heads d_head, batch k_posn n_heads d_head -> batch n_heads q_posn k_posn\")\n",
    "        scaling_factor = t.sqrt(t.Tensor([self.cfg.d_head])).to(normalized_resid_pre.device)\n",
    "        attn_scaled = attn_scores / scaling_factor\n",
    "        attn_masked = self.apply_causal_mask(attn_scores=attn_scaled)\n",
    "\n",
    "        attn_probabilities = t.softmax(attn_masked, dim=-1) # Last dimension is k_posn, softmaxing over key positions (source).\n",
    "\n",
    "        # Step 2: Move information from source tokens to destination tokens using the attention pattern\n",
    "        values = einops.einsum(normalized_resid_pre, self.W_V, \"batch posn d_model, n_heads d_model d_head -> batch posn n_heads d_head\")\n",
    "        values = values + self.b_V \n",
    "\n",
    "        zs = einops.einsum(attn_probabilities, values, \"batch n_heads q_posn k_posn, batch k_posn n_heads d_head -> batch n_heads q_posn d_head\")\n",
    "\n",
    "        outputs = einops.einsum(zs, self.W_O, \"batch n_heads posn d_head, n_heads d_head d_model -> batch posn n_heads d_model\")\n",
    "        outputs = einops.reduce(outputs, \"batch posn n_heads d_model -> batch posn d_model\", reduction=\"sum\")\n",
    "\n",
    "        outputs = outputs + self.b_O\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "tests.test_causal_mask(Attention.apply_causal_mask)\n",
    "rand_float_test(Attention, [2, 4, 768])\n",
    "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        pre_act = einops.einsum(normalized_resid_mid, self.W_in, \"batch posn d_model, d_model d_mlp -> batch posn d_mlp\")\n",
    "        pre_act += self.b_in\n",
    "\n",
    "        post_act = gelu_new(pre_act)\n",
    "\n",
    "        mlp_out = einops.einsum(post_act, self.W_out, \"batch posn d_mlp, d_mlp d_model -> batch posn d_model\")\n",
    "        mlp_out += self.b_out\n",
    "        return mlp_out\n",
    "\n",
    "\n",
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "LayerNorm input shape: torch.Size([1, 35, 768])\n",
      "LayerNorm output shape: torch.Size([1, 35, 768])\n",
      "LayerNorm input shape: torch.Size([1, 35, 768])\n",
      "LayerNorm output shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        lnormed_resid_1 = self.ln1(resid_pre)\n",
    "        attn_output = self.attn(lnormed_resid_1)\n",
    "\n",
    "        residual_stream = resid_pre + attn_output\n",
    "\n",
    "        lnormed_resid_2 = self.ln2(residual_stream)\n",
    "        mlp_output = self.mlp(lnormed_resid_2)\n",
    "\n",
    "        block_output = residual_stream + mlp_output\n",
    "\n",
    "        return block_output\n",
    "\n",
    "\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        return einops.einsum(normalized_resid_final, self.W_U, \"batch position d_model, d_model d_vocab -> batch position d_vocab\") + self.b_U\n",
    "\n",
    "\n",
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "LayerNorm input shape: torch.Size([2, 4, 768])\n",
      "LayerNorm output shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Input shape: torch.Size([1, 45])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "LayerNorm input shape: torch.Size([1, 45, 768])\n",
      "LayerNorm output shape: torch.Size([1, 45, 768])\n",
      "Output shape: torch.Size([1, 45, 50257])\n",
      "Reference output shape: torch.Size([1, 45, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        embeddings = self.embed(tokens)\n",
    "        pos_embeddings = self.pos_embed(tokens)\n",
    "        residual_stream = embeddings + pos_embeddings\n",
    "\n",
    "        for block in self.blocks:\n",
    "            residual_stream = block(residual_stream)\n",
    "        \n",
    "        return self.unembed(self.ln_final(residual_stream))\n",
    "\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "demo_logits = demo_gpt2(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg cross entropy loss: 4.0442\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.098628\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"], \n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
    "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fdc1505aa24df48bc2a734a2a1e809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Perspective Vortex derives its picture of the whole Universe on the principle of the total perspective. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The\n"
     ]
    }
   ],
   "source": [
    "test_string = '''The Total Perspective Vortex derives its picture of the whole Universe on the principle of'''\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
    "    demo_logits = demo_gpt2(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(\n",
    "    debug=False, \n",
    "    d_model=256, \n",
    "    n_heads=4, \n",
    "    d_head=64, \n",
    "    d_mlp=1024, \n",
    "    n_layers=2, \n",
    "    n_ctx=256, \n",
    "    d_vocab=reference_gpt2.cfg.d_vocab\n",
    ")\n",
    "model = DemoTransformer(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerTrainingArgs():\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    max_steps_per_epoch = 200\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project: str | None = \"day1-demotransformer\"\n",
    "    wandb_name: str | None = None\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playi\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\"meta\")\n",
    "print(dataset)\n",
    "print(dataset[0]['text'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenize_and_concatenate(\n",
    "    dataset, reference_gpt2.tokenizer, streaming=False, max_length=model.cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4\n",
    ")\n",
    "\n",
    "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n",
    "train_loader = DataLoader(dataset_dict[\"train\"], batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(dataset_dict[\"test\"], batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokens'])\n",
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "first_batch = train_loader.dataset[:args.batch_size]\n",
    "\n",
    "print(first_batch.keys())\n",
    "print(first_batch['tokens'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_fn(model: DemoTransformer, prompt: str) -> str:\n",
    "    sampler = solutions.TransformerSampler(model, reference_gpt2.tokenizer)\n",
    "    output = sampler.sample(prompt, temperature=0.7, top_p=0.95, max_tokens_generated=16)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.optimizer = t.optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        self.step = 0\n",
    "\n",
    "\n",
    "    def training_step(self, batch: dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
    "        '''\n",
    "        Calculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\n",
    "\n",
    "        Remember that `batch` is a dictionary with the single key 'tokens'.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        batch_tokens = batch[\"tokens\"].to(device)\n",
    "        logits = self.model(batch_tokens) # (batch seq d_vocab)\n",
    "        pred_log_probs = get_log_probs(logits, batch_tokens)\n",
    "\n",
    "        avg_loss = -pred_log_probs.mean()\n",
    "\n",
    "\n",
    "        avg_loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "        wandb.log({\"loss\": avg_loss}, step=self.step)\n",
    "\n",
    "\n",
    "    def validation_step(self, batch: dict[str, Int[Tensor, \"batch seq\"]]) -> Int[Tensor, \"batch * seq\"]:\n",
    "        '''\n",
    "        Calculates & returns the accuracy on the tokens in the batch (i.e. how often the model's prediction\n",
    "        is correct). Logging should happen in the `train` function (after we've computed the accuracy for \n",
    "        the whole validation set).\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        batch_tokens = batch[\"tokens\"].to(device)\n",
    "        input_tokens = batch_tokens[:, :-1]\n",
    "        label_tokens = batch_tokens[:, 1:]\n",
    "        logits = self.model(input_tokens) # (batch seq-1 d_vocab)\n",
    "        predictions = logits.argmax(dim=-1) # (batch seq-1)\n",
    "        accuracy = (predictions == label_tokens).flatten()\n",
    "        return accuracy \n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        Trains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping\n",
    "        for each epoch at `self.args.max_steps_per_epoch` steps.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)\n",
    "\n",
    "        train_loader = self.train_loader()\n",
    "        test_loader = self.test_loader()\n",
    "\n",
    "        completions_data = []\n",
    "        for epoch in range(self.args.epochs):\n",
    "            # Do an epoch of training\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                if batch_idx > self.args.max_steps_per_epoch:\n",
    "                    break\n",
    "\n",
    "                self.training_step(batch)\n",
    "\n",
    "                if self.step % 3:\n",
    "                    # Sample from the model\n",
    "                    sampled_text = sampling_fn(model, prompt=\"John and Mary went to the store to buy some\")\n",
    "                    completions_data.append([epoch, self.step, sampled_text])\n",
    "                \n",
    "                if self.step % 9:\n",
    "                    wandb.log({\"completions_table\": wandb.Table(\n",
    "                            data = completions_data,\n",
    "                        columns = [\"epoch\", \"step\", \"text\"]\n",
    "                    )})\n",
    "\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            num_correct = 0\n",
    "            num_examples = 0\n",
    "            for batch in test_loader:\n",
    "                acc_tensor = self.validation_step(batch)\n",
    "                num_correct += acc_tensor.sum()\n",
    "                num_examples += acc_tensor.shape[0]\n",
    "\n",
    "            valid_accuracy = num_correct / num_examples\n",
    "            wandb.log({\"Valid accuracy\": valid_accuracy}, step=self.step)\n",
    "        \n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "    def train_loader(self) -> DataLoader:\n",
    "        '''Returns train loader (as in code above).'''\n",
    "        return DataLoader(dataset_dict[\"train\"], batch_size=self.args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "    def test_loader(self) -> DataLoader:\n",
    "        '''Returns test loader (as in code above).'''\n",
    "        return DataLoader(dataset_dict[\"test\"], batch_size=self.args.batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoTransformer(model_cfg).to(device)\n",
    "args = TransformerTrainingArgs()\n",
    "trainer = TransformerTrainer(args, model)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_vocab = 50257\n",
      "Cross entropy loss on uniform distribution = 10.82490511970208\n"
     ]
    }
   ],
   "source": [
    "d_vocab = model.cfg.d_vocab\n",
    "\n",
    "print(f\"d_vocab = {d_vocab}\")\n",
    "print(f\"Cross entropy loss on uniform distribution = {math.log(d_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of training data = 7.349369525909424\n"
     ]
    }
   ],
   "source": [
    "toks = tokenized_dataset[:][\"tokens\"].flatten()\n",
    "\n",
    "d_vocab = model.cfg.d_vocab\n",
    "freqs = t.bincount(toks, minlength=d_vocab)\n",
    "probs = freqs.float() / freqs.sum()\n",
    "\n",
    "distn = t.distributions.categorical.Categorical(probs=probs)\n",
    "entropy = distn.entropy()\n",
    "\n",
    "print(f\"Entropy of training data = {entropy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(debug=False)\n",
    "model = DemoTransformer(model_cfg).to(device)\n",
    "model.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "tokenizer = reference_gpt2.tokenizer\n",
    "\n",
    "class TransformerSampler:\n",
    "\n",
    "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
    "        self.model = model\n",
    "        self.cfg = model.cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "        '''\n",
    "        Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "        end-of-sequence token.\n",
    "\n",
    "        kwargs are passed to sample_next_token, to give detailed instructions on how \n",
    "        new tokens are chosen.\n",
    "        '''\n",
    "        # YOUR CODE HERE!\n",
    "        model.eval()\n",
    "\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0]\n",
    "        \n",
    "        for _ in range(max_tokens_generated):\n",
    "            logits = self.model(input_ids[None, -self.cfg.n_ctx:])\n",
    "            next_token_logits = logits[0, -1]\n",
    "\n",
    "            token = self.sample_next_token(input_ids, next_token_logits, **kwargs)\n",
    "\n",
    "            token = t.LongTensor([token]).to(device)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Generated token: {self.tokenizer.decode(token)}\")\n",
    "\n",
    "            input_ids = t.concat([input_ids, token], dim=-1)\n",
    "\n",
    "            if token == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "        \n",
    "        full_string = self.tokenizer.decode(input_ids)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        return full_string\n",
    "\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def beam_search(\n",
    "        self,\n",
    "        prompt: str, \n",
    "        num_return_sequences: int, \n",
    "        num_beams: int, \n",
    "        max_new_tokens: int, \n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        verbose=False\n",
    "    ) -> list[tuple[float, Tensor]]:\n",
    "        '''\n",
    "        Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "        end-of-sequence token.\n",
    "\n",
    "        kwargs are passed to sample_next_token, to give detailed instructions on how \n",
    "        new tokens are chosen.\n",
    "        '''\n",
    "        # YOUR CODE HERE!\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(\n",
    "        input_ids: Int[Tensor, \"seq_len\"], \n",
    "        logits: Float[Tensor, \"d_vocab\"], \n",
    "        temperature=1.0, \n",
    "        top_k=0, \n",
    "        top_p=0.0, \n",
    "        frequency_penalty=0.0,\n",
    "        seed=None\n",
    "    ):\n",
    "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
    "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
    "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
    "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
    "        assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        if seed is not None:\n",
    "            t.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Apply all the specialized sampling methods\n",
    "        if temperature == 0:\n",
    "            return TransformerSampler.greedy_search(logits)\n",
    "        elif temperature != 1.0:\n",
    "            logits = TransformerSampler.apply_temperature(logits, temperature)\n",
    "        if frequency_penalty != 0.0:\n",
    "            logits = TransformerSampler.apply_frequency_penalty(input_ids, logits, frequency_penalty)\n",
    "        if top_k > 0:\n",
    "            return TransformerSampler.sample_top_k(logits, top_k)\n",
    "        if top_p > 0.0:\n",
    "            return TransformerSampler.sample_top_p(logits, top_p)\n",
    "        return TransformerSampler.sample_basic(logits)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        '''\n",
    "        Returns the most likely token (as an int).\n",
    "        '''\n",
    "        out = logits.argmax().item()\n",
    "        return out\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_temperature(logits: Float[Tensor, \"d_vocab\"], temperature: float) -> Float[Tensor, \"d_vocab\"]:\n",
    "        '''\n",
    "        Applies temperature scaling to the logits.\n",
    "        '''\n",
    "        return logits / temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_frequency_penalty(input_ids: Int[Tensor, \"seq_len\"], logits: Float[Tensor, \"d_vocab\"], freq_penalty: float) -> Float[Tensor, \"d_vocab\"]:\n",
    "        '''\n",
    "        Applies a frequency penalty to the logits.\n",
    "        '''\n",
    "        (d_vocab,) = logits.shape\n",
    "        id_freqs = t.bincount(input_ids, minlength=d_vocab)\n",
    "        logits -= freq_penalty * id_freqs\n",
    "        return logits\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        '''\n",
    "        Samples from the distribution defined by the logits.\n",
    "        '''\n",
    "        dist = t.distributions.categorical.Categorical(logits=logits)\n",
    "        sampled_token = dist.sample() \n",
    "        return sampled_token.item()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
    "        '''\n",
    "        Samples from the top k most likely tokens.\n",
    "        '''\n",
    "        top_logits, top_tokens = t.topk(logits, k=k)\n",
    "\n",
    "        dist = t.distributions.categorical.Categorical(logits=top_logits)\n",
    "        index_in_top_tokens = dist.sample() \n",
    "        return top_tokens[index_in_top_tokens].item()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_p(logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
    "        '''\n",
    "        Samples from the most likely tokens which make up at least p cumulative probability.\n",
    "        '''\n",
    "        top_logits, top_tokens = t.sort(logits, descending=True, stable=True)\n",
    "        top_p_probs = t.softmax(top_logits, dim=-1)\n",
    "        cumulative_prob = t.cumsum(top_p_probs, dim=0)\n",
    "        n_keep = t.searchsorted(cumulative_prob, top_p, side=\"right\") + 1\n",
    "        n_keep = max(n_keep, min_tokens_to_keep)\n",
    "        top_logits = top_logits[:n_keep]\n",
    "        top_tokens = top_tokens[:n_keep]\n",
    "\n",
    "        dist = t.distributions.categorical.Categorical(logits=top_logits) # We provide probs as logits for them to be normalized again, to sum to 1.\n",
    "        sample = dist.sample() \n",
    "        return top_tokens[sample].item()\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding with prompt: 'Jingle bells, jingle bells, jingle all the way'\n",
      "\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way up to the top of the mountain.'\n",
      "\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "print(f\"Greedy decoding with prompt: {prompt!r}\\n\")\n",
    "\n",
    "output = sampler.sample(prompt, max_tokens_generated=8, temperature=0.0)\n",
    "print(f\"Your model said: {output!r}\\n\")\n",
    "\n",
    "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
    "assert output == expected\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a74768c6cf46738fe4f0bbf831e2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ' church'. Expected freq 0.0648, observed freq 0.0657\n",
      "Word: ' house' . Expected freq 0.0367, observed freq 0.0388\n",
      "Word: ' temple'. Expected freq 0.0145, observed freq 0.0159\n",
      "Word: ' same'  . Expected freq 0.0104, observed freq 0.0106\n",
      "Word: ' Church'. Expected freq 0.0097, observed freq 0.0101\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_5 = {\n",
    "    \" church\": 0.0648,\n",
    "    \" house\": 0.0367,\n",
    "    \" temple\": 0.0145,\n",
    "    \" same\": 0.0104,\n",
    "    \" Church\": 0.0097\n",
    "}\n",
    "frequency_of_top_5 = defaultdict(int)\n",
    "\n",
    "N = 10_000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits)\n",
    "    frequency_of_top_5[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_5:\n",
    "    expected_freq = expected_top_5[word]\n",
    "    observed_freq = frequency_of_top_5[word] / N\n",
    "    print(f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\")\n",
    "    assert abs(observed_freq - expected_freq) < 0.01, \"Try increasing N if this fails by a small amount.\"\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A low temperature \"sharpens\" or \"peaks\" the distribution:  tensor([  0.0000, 693.1472])\n",
      "A high temperature flattens the distribution:  tensor([0.0000, 0.0007])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "logits = t.tensor([1, 2]).log()\n",
    "\n",
    "cold_logits = TransformerSampler.apply_temperature(logits, temperature=0.001)\n",
    "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
    "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
    "\n",
    "hot_logits = TransformerSampler.apply_temperature(logits, temperature=1000.0)\n",
    "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
    "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
    "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\")\n",
    "logits = t.ones(tokenizer.vocab_size)\n",
    "penalized_logits = TransformerSampler.apply_frequency_penalty(input_ids.squeeze(), logits, 2.0)\n",
    "\n",
    "assert penalized_logits[5156].item() == -11, \"Expected 6 occurrences of ' baby' with leading space, 1-2*6=-11\"\n",
    "assert penalized_logits[14801].item() == -5, \"Expected 3 occurrences of ' Baby' with leading space, 1-2*3=-5\"\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                             Sampling - Manual Testing                                             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Name                  </span>┃<span style=\"font-weight: bold\"> Kwargs                       </span>┃<span style=\"font-weight: bold\"> Output                                                   </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ High freq penalty     │ {'frequency_penalty': 100.0} │ 'Jingle bells, jingle bells, jingle all the way to       │\n",
       "│                       │                              │ Chicago,\" provides an extreme proof of why Werein\\'s     │\n",
       "│                       │                              │ brilliant comics do such a fantastic job finding         │\n",
       "│                       │                              │ sanctuary in small'                                      │\n",
       "│                       │                              │                                                          │\n",
       "│ Negative freq penalty │ {'frequency_penalty': -3.0}  │ 'Jingle bells, jingle bells, jingle all the bells bells  │\n",
       "│                       │                              │ bells bells bells bells bells bells bells bells bells    │\n",
       "│                       │                              │ bells bells bells bells bells bells bells bells bells    │\n",
       "│                       │                              │ bells bells bells bells'                                 │\n",
       "│                       │                              │                                                          │\n",
       "│ Too hot!              │ {'temperature': 2.0}         │ 'Jingle bells, jingle bells, jingle all theable Johns    │\n",
       "│                       │                              │ rejoice 😀 hair guyh Season conclude I slated volatile   │\n",
       "│                       │                              │ Panama nuance biscuits puff fluids bye wrap policyto     │\n",
       "│                       │                              │ cakes bos'                                               │\n",
       "│                       │                              │                                                          │\n",
       "│ Pleasantly cool       │ {'temperature': 0.7}         │ 'Jingle bells, jingle bells, jingle all the way to the   │\n",
       "│                       │                              │ house of the Lord, where the Lord is always in his       │\n",
       "│                       │                              │ glory. That is the purest and most'                      │\n",
       "│                       │                              │                                                          │\n",
       "│ Pleasantly warm       │ {'temperature': 0.9}         │ 'Jingle bells, jingle bells, jingle all the way from the │\n",
       "│                       │                              │ highway, and your ornery heart will get the best of you  │\n",
       "│                       │                              │ if you put it right, and even'                           │\n",
       "│                       │                              │                                                          │\n",
       "│ Too cold!             │ {'temperature': 0.01}        │ 'Jingle bells, jingle bells, jingle all the way up to    │\n",
       "│                       │                              │ the top of the mountain.\\n\\nThe first time I saw the     │\n",
       "│                       │                              │ mountain, I was in the middle'                           │\n",
       "│                       │                              │                                                          │\n",
       "└───────────────────────┴──────────────────────────────┴──────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                             Sampling - Manual Testing                                             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mName                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mKwargs                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput                                                  \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ High freq penalty     │ {'frequency_penalty': 100.0} │ 'Jingle bells, jingle bells, jingle all the way to       │\n",
       "│                       │                              │ Chicago,\" provides an extreme proof of why Werein\\'s     │\n",
       "│                       │                              │ brilliant comics do such a fantastic job finding         │\n",
       "│                       │                              │ sanctuary in small'                                      │\n",
       "│                       │                              │                                                          │\n",
       "│ Negative freq penalty │ {'frequency_penalty': -3.0}  │ 'Jingle bells, jingle bells, jingle all the bells bells  │\n",
       "│                       │                              │ bells bells bells bells bells bells bells bells bells    │\n",
       "│                       │                              │ bells bells bells bells bells bells bells bells bells    │\n",
       "│                       │                              │ bells bells bells bells'                                 │\n",
       "│                       │                              │                                                          │\n",
       "│ Too hot!              │ {'temperature': 2.0}         │ 'Jingle bells, jingle bells, jingle all theable Johns    │\n",
       "│                       │                              │ rejoice 😀 hair guyh Season conclude I slated volatile   │\n",
       "│                       │                              │ Panama nuance biscuits puff fluids bye wrap policyto     │\n",
       "│                       │                              │ cakes bos'                                               │\n",
       "│                       │                              │                                                          │\n",
       "│ Pleasantly cool       │ {'temperature': 0.7}         │ 'Jingle bells, jingle bells, jingle all the way to the   │\n",
       "│                       │                              │ house of the Lord, where the Lord is always in his       │\n",
       "│                       │                              │ glory. That is the purest and most'                      │\n",
       "│                       │                              │                                                          │\n",
       "│ Pleasantly warm       │ {'temperature': 0.9}         │ 'Jingle bells, jingle bells, jingle all the way from the │\n",
       "│                       │                              │ highway, and your ornery heart will get the best of you  │\n",
       "│                       │                              │ if you put it right, and even'                           │\n",
       "│                       │                              │                                                          │\n",
       "│ Too cold!             │ {'temperature': 0.01}        │ 'Jingle bells, jingle bells, jingle all the way up to    │\n",
       "│                       │                              │ the top of the mountain.\\n\\nThe first time I saw the     │\n",
       "│                       │                              │ mountain, I was in the middle'                           │\n",
       "│                       │                              │                                                          │\n",
       "└───────────────────────┴──────────────────────────────┴──────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "N_RUNS = 1\n",
    "your_prompt = \"Jingle bells, jingle bells, jingle all the\"\n",
    "cases = [\n",
    "    (\"High freq penalty\", dict(frequency_penalty=100.0)),\n",
    "    (\"Negative freq penalty\", dict(frequency_penalty=-3.0)),\n",
    "    (\"Too hot!\", dict(temperature=2.0)),\n",
    "    (\"Pleasantly cool\", dict(temperature=0.7)),\n",
    "    (\"Pleasantly warm\", dict(temperature=0.9)),\n",
    "    (\"Too cold!\", dict(temperature=0.01)),\n",
    "]\n",
    "\n",
    "table = Table(\"Name\", \"Kwargs\", \"Output\", title=\"Sampling - Manual Testing\")\n",
    "\n",
    "for (name, kwargs) in cases:\n",
    "    for i in range(N_RUNS):\n",
    "        output = sampler.sample(your_prompt, max_tokens_generated=24, **kwargs)\n",
    "        table.add_row(name, repr(kwargs), repr(output) + \"\\n\")\n",
    "\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea43c339202644edacae690aeae76e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ' church'. Expected freq = 0.4761, observed freq = 0.4787\n",
      "Word: ' house' . Expected freq = 0.2697, observed freq = 0.2716\n",
      "Word: ' temple'. Expected freq = 0.1065, observed freq = 0.1067\n",
      "Word: ' same'  . Expected freq = 0.0764, observed freq = 0.0728\n",
      "Word: ' Church'. Expected freq = 0.0713, observed freq = 0.0701\n"
     ]
    }
   ],
   "source": [
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_5 = {\n",
    "    \" church\": 0.0648,\n",
    "    \" house\": 0.0367,\n",
    "    \" temple\": 0.0145,\n",
    "    \" same\": 0.0104,\n",
    "    \" Church\": 0.0097\n",
    "}\n",
    "topk_5_sum = sum(expected_top_5.values())\n",
    "\n",
    "observed_freqs = defaultdict(int)\n",
    "\n",
    "N = 30000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_k=5)\n",
    "    # print(tokenizer.decode(token))\n",
    "    observed_freqs[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_5:\n",
    "    expected_freq = expected_top_5[word] / topk_5_sum\n",
    "    observed_freq = observed_freqs[word] / N\n",
    "    print(f\"Word: {word!r:<9}. Expected freq = {expected_freq:.4f}, observed freq = {observed_freq:.4f}\")\n",
    "    assert abs(observed_freq - expected_freq) < 0.015, \"Try increasing N if this fails by a small amount.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your model said:\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in</span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</span>\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">\"In fact, they were so good at speaking English that they could even speak English in one word,\"</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\"> said Dr. J.L. </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">Coyle, professor of biology at the University of Arizona, and lead author of the study. \"And the unicorns in the </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">study were actually very good at speaking English</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your model said:\n",
       "\n",
       "\u001b[1;38;5;208mIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in\u001b[0m\n",
       "\u001b[1;38;5;208mthe Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\u001b[0m\n",
       "\n",
       "\u001b[1;38;5;208m\"In fact, they were so good at speaking English that they could even speak English in one word,\"\u001b[0m\u001b[1;38;5;208m said Dr. J.L. \u001b[0m\n",
       "\u001b[1;38;5;208mCoyle, professor of biology at the University of Arizona, and lead author of the study. \"And the unicorns in the \u001b[0m\n",
       "\u001b[1;38;5;208mstudy were actually very good at speaking English\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "your_prompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "output = sampler.sample(your_prompt, temperature=0.7, top_k=40, max_tokens_generated=64)\n",
    "rprint(f\"Your model said:\\n\\n[bold dark_orange]{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d66963999947b2b67f6dc51dadee76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ' church'. Expected freq 0.6384, observed freq 0.6358\n",
      "Word: ' house' . Expected freq 0.3616, observed freq 0.3642\n"
     ]
    }
   ],
   "source": [
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_10pct = {\n",
    "    \" church\": 0.0648,\n",
    "    \" house\": 0.0367, # These are the two most likely tokens, and add up to >10%\n",
    "}\n",
    "top_10pct_sum = sum(expected_top_10pct.values())\n",
    "\n",
    "observed_freqs = defaultdict(int)\n",
    "\n",
    "N = 10000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_p=0.1)\n",
    "    observed_freqs[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_10pct:\n",
    "    expected_freq = expected_top_10pct[word] / top_10pct_sum\n",
    "    observed_freq = observed_freqs[word] / N\n",
    "    print(f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\")\n",
    "    assert abs(observed_freq - expected_freq) < 0.01, \"Try increasing N if this fails by a small amount.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did I learn here? In top-p (nucleus) sampling.\n",
    "\n",
    "In sampling\n",
    "```python\n",
    "sample = t.distributions.categorical.Categorical(logits=keep_logits).sample()\n",
    "```\n",
    "is different than if we softmaxed the ```logits``` first and then took the top k.\n",
    "We can't provide unnormalized probabilites as logits, since this would be we applied softmax twice. And softmax applied twice is not the same as softmax applied once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your model said:\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">Eliezer Shlomo Yudkowsky (born September </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">11</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">, </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">1979</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">) is an American decision and artificial intelligence (AI) </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">theorist and writer, best known for his work on the rise of the Rational Mind.</span>\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">Aphrodite Nader</span>\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">Nader was born in Chicago on August </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">, </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">1831</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">. He has been married to three children, including Marjorie, who is now</span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">83</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">.</span>\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">Nader was raised by a family of professors at Northwestern</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your model said:\n",
       "\n",
       "\u001b[1;38;5;208mEliezer Shlomo Yudkowsky \u001b[0m\u001b[1;38;5;208m(\u001b[0m\u001b[1;38;5;208mborn September \u001b[0m\u001b[1;38;5;208m11\u001b[0m\u001b[1;38;5;208m, \u001b[0m\u001b[1;38;5;208m1979\u001b[0m\u001b[1;38;5;208m)\u001b[0m\u001b[1;38;5;208m is an American decision and artificial intelligence \u001b[0m\u001b[1;38;5;208m(\u001b[0m\u001b[1;38;5;208mAI\u001b[0m\u001b[1;38;5;208m)\u001b[0m\u001b[1;38;5;208m \u001b[0m\n",
       "\u001b[1;38;5;208mtheorist and writer, best known for his work on the rise of the Rational Mind.\u001b[0m\n",
       "\n",
       "\u001b[1;38;5;208mAphrodite Nader\u001b[0m\n",
       "\n",
       "\u001b[1;38;5;208mNader was born in Chicago on August \u001b[0m\u001b[1;38;5;208m25\u001b[0m\u001b[1;38;5;208m, \u001b[0m\u001b[1;38;5;208m1831\u001b[0m\u001b[1;38;5;208m. He has been married to three children, including Marjorie, who is now\u001b[0m\n",
       "\u001b[1;38;5;208m83\u001b[0m\u001b[1;38;5;208m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;5;208mNader was raised by a family of professors at Northwestern\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "your_prompt = \"Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for\"\n",
    "output = sampler.sample(your_prompt, temperature=0.7, top_p=0.95, max_tokens_generated=64)\n",
    "rprint(f\"Your model said:\\n\\n[bold dark_orange]{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor([[1,2,3,4],[5,-1,-2,-3]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  3.],\n",
       "        [ 5., -1.]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.topk(a, dim=-1, k=2).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Beams:\n",
    "    '''Class to store beams during beam search.'''\n",
    "    model: DemoTransformer\n",
    "    tokenizer: GPT2TokenizerFast\n",
    "    logprob_sums: Float[Tensor, \"batch\"]\n",
    "    tokens: Int[Tensor, \"batch seq\"]\n",
    "\n",
    "    def new_beams(self, logprob_sums, tokens) -> \"Beams\":\n",
    "        '''Creates a new Beams object with the same model and tokenizer.'''\n",
    "        return Beams(self.model, self.tokenizer, logprob_sums, tokens)\n",
    "\n",
    "    def __getitem__(self, idx) -> \"Beams\":\n",
    "        '''Allows you to take a slice of the beams object along the batch dimension.'''\n",
    "        return self.new_beams(self.logprob_sums[idx], self.tokens[idx])\n",
    "\n",
    "    @property\n",
    "    def logprobs_and_completions(self) -> list[tuple[float, str]]:\n",
    "        '''Returns self as a list of logprob sums and completions (useful for getting final output).'''\n",
    "        return [\n",
    "            (logprob_sum.item(), self.tokenizer.decode(tokens))\n",
    "            for (logprob_sum, tokens) in zip(self.logprob_sums, self.tokens)\n",
    "        ]\n",
    "\n",
    "\n",
    "    def generate(self, toks_per_beam: int, no_repeat_ngram_size: int | None = None) -> \"Beams\":\n",
    "        '''\n",
    "        Starting from the current set of beams (which has length `num_beams`), returns a new\n",
    "        set of `num_beams * toks_per_beam`, containing the best `toks_per_beam` continuations for each\n",
    "        of the original beams.\n",
    "\n",
    "        Optional argument `no_repeat_ngram_size` means your model won't generate any sequences with\n",
    "        a repeating n-gram of this length.\n",
    "        '''\n",
    "\n",
    "        logits = self.model(self.tokens) # (batch, seq + 1, vocab_size)\n",
    "        last_logits = logits[:, -1, :] # (batch, vocab_size)\n",
    "\n",
    "        \n",
    "        logprobs = t.log_softmax(last_logits, dim=-1)\n",
    "\n",
    "        topk_logprobs, topk_tokens = self.get_topk_non_repeating(logprobs, no_repeat_ngram_size, toks_per_beam) # (batch, toks_per_beam)\n",
    "\n",
    "        new_logprob_sums = einops.repeat(self.logprob_sums, \"batch -> (batch toks_per_beam)\", toks_per_beam=toks_per_beam).clone()\n",
    "        topk_logprobs = einops.rearrange(topk_logprobs, \"batch toks_per_beam -> (batch toks_per_beam)\")\n",
    "\n",
    "        new_logprob_sums += topk_logprobs\n",
    "\n",
    "        new_tokens = einops.repeat(self.tokens, \"batch seq -> (batch toks_per_beam) seq\", toks_per_beam=toks_per_beam).clone()\n",
    "        topk_tokens = einops.rearrange(topk_tokens, \"batch toks_per_beam -> (batch toks_per_beam) 1\")\n",
    "\n",
    "        new_tokens = t.cat([new_tokens, topk_tokens], dim=-1)\n",
    "\n",
    "        return self.new_beams(new_logprob_sums, new_tokens)\n",
    "    \n",
    "    def get_topk_non_repeating(\n",
    "        self,\n",
    "        logprobs: Float[Tensor, \"batch d_vocab\"], \n",
    "        no_repeat_ngram_size: int,\n",
    "        k: int, \n",
    "    ) -> tuple[Float[Tensor, \"k\"], Int[Tensor, \"k\"]]:\n",
    "        '''\n",
    "        logprobs: \n",
    "            tensor of the log-probs for the next token\n",
    "        no_repeat_ngram_size:\n",
    "            size of ngram to avoid repeating\n",
    "        k:\n",
    "            number of top logits to return, for each beam in our collection\n",
    "\n",
    "        Returns:\n",
    "            equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure\n",
    "            that no returned tokens would produce an ngram of size  `no_repeat_ngram_size`\n",
    "            which has already appeared in `self.tokens`.\n",
    "        '''\n",
    "        if not no_repeat_ngram_size:\n",
    "            return t.topk(logprobs, dim=-1, k=k) # (batch, toks_per_beam) \n",
    "\n",
    "        # For each beam (the batch dimension of self.tokens), find which ngrams there are already in this beam.\n",
    "        # Instead of gathering ngrams I can gather tokens to remove.\n",
    "        # If if the last existing tokens repeat [0:n-2] tokens of an existing ngram, make the logit of the last token in this n-gram float(-inf)\n",
    "\n",
    "        n_minus_one = no_repeat_ngram_size - 1\n",
    "\n",
    "        num_beams, seq_len = self.tokens.shape\n",
    "        for beam in range(num_beams):\n",
    "            last_n_minus_one_gram = self.tokens[beam, -n_minus_one:]\n",
    "            print(f\"Last ngram: {self.tokenizer.decode(last_n_minus_one_gram)}\")\n",
    "            for seq_idx in range(seq_len - no_repeat_ngram_size):\n",
    "                n_minus_one_gram = self.tokens[beam, seq_idx:seq_idx+n_minus_one]\n",
    "                if not len(n_minus_one_gram) or (n_minus_one_gram == last_n_minus_one_gram):\n",
    "                    # Prohibit the last n-gram token from repeating\n",
    "                    last_ngram_token = self.tokens[beam, seq_idx+no_repeat_ngram_size-1]\n",
    "                    print(f\"Prohibiting: {self.tokenizer.decode(last_ngram_token)}\")\n",
    "                    logprobs[beam, last_ngram_token] = float(\"-inf\")\n",
    "        \n",
    "        return t.topk(logprobs, dim=-1, k=k) # (batch, toks_per_beam) \n",
    "\n",
    "\n",
    "    def filter(self, num_beams: int) -> tuple[\"Beams\", \"Beams\"]:\n",
    "        '''\n",
    "        Returns:\n",
    "            best_beams: Beams\n",
    "                filtered version of self, containing all best `num_beams` which are also not terminated.\n",
    "\n",
    "            early_terminations: Beams\n",
    "                filtered version of self, containing all best `num_beams` which are also terminated.\n",
    "                i.e. the sum of lengths of these two should equal `num_beams`.\n",
    "        '''\n",
    "        self.tokens #(batch, seq)\n",
    "        self.logprob_sums # (batch)\n",
    "        topk_sums, topk_indices = t.topk(self.logprob_sums, k=num_beams)\n",
    "\n",
    "        topk_tokens = self.tokens[topk_indices] \n",
    "\n",
    "        terminated_mask = (topk_tokens[:, -1] == self.tokenizer.eos_token_id)\n",
    "\n",
    "        best_tokens = topk_tokens[~terminated_mask]\n",
    "        best_logprob_sums = topk_sums[~terminated_mask]\n",
    "\n",
    "        terminated_tokens = topk_tokens[terminated_mask]\n",
    "        terminated_logprob_sums = topk_sums[terminated_mask]\n",
    "\n",
    "        return self.new_beams(best_logprob_sums, best_tokens), self.new_beams(terminated_logprob_sums, terminated_tokens)\n",
    "\n",
    "    def print(self, title=\"Best completions\", max_print_chars=80) -> None:\n",
    "        '''\n",
    "        Prints out a set of sequences with their corresponding logitsums.\n",
    "        '''\n",
    "        if len(self.tokens) == 0:\n",
    "            return\n",
    "        table = Table(\"logitsum\", \"completion\", title=title)\n",
    "        for logprob_sum, tokens in zip(self.logprob_sums, self.tokens):\n",
    "            text = self.tokenizer.decode(tokens)\n",
    "            if len(repr(text)) > max_print_chars:\n",
    "                text = text[:int(0.3 * max_print_chars)] + \" ... \" + text[-int(0.7 * max_print_chars):]\n",
    "            table.add_row(f\"{logprob_sum:>8.3f}\", repr(text))\n",
    "        rprint(table)\n",
    "\n",
    "\n",
    "@t.inference_mode()\n",
    "def beam_search(\n",
    "    self: TransformerSampler,\n",
    "    prompt: str, \n",
    "    num_return_sequences: int, \n",
    "    num_beams: int, \n",
    "    max_new_tokens: int, \n",
    "    no_repeat_ngram_size: int | None = None,\n",
    "    verbose=False\n",
    ") -> list[tuple[float, Tensor]]:\n",
    "    '''\n",
    "    Implements a beam search, by repeatedly performing the `generate` and `filter` steps (starting\n",
    "    from the initial prompt) until either of the two stopping criteria are met:\n",
    "\n",
    "        (1) we've generated `max_new_tokens` tokens, or\n",
    "        (2) we've generated `num_returns_sequences` terminating sequences.\n",
    "\n",
    "    To modularize this function, most of the actual complexity is in the Beams class,\n",
    "    in the `generate` and `filter` methods.\n",
    "    '''\n",
    "\n",
    "    assert num_return_sequences <= num_beams\n",
    "    self.model.eval()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">         Best completions          </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> logitsum </span>┃<span style=\"font-weight: bold\"> completion           </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│  -10.000 │ 'this is the third'  │\n",
       "│  -15.000 │ 'this is the second' │\n",
       "│  -20.000 │ 'this is the first'  │\n",
       "└──────────┴──────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m         Best completions          \u001b[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mlogitsum\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│  -10.000 │ 'this is the third'  │\n",
       "│  -15.000 │ 'this is the second' │\n",
       "│  -20.000 │ 'this is the first'  │\n",
       "└──────────┴──────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beams = Beams(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    logprob_sums = t.tensor([-10.0, -15.0, -20.0]).to(device),\n",
    "    tokens = t.tensor([\n",
    "        [5661, 318, 262, 2368],\n",
    "        [5661, 318, 262, 1218],\n",
    "        [5661, 318, 262, 717],\n",
    "    ]).to(device)\n",
    ")\n",
    "\n",
    "beams.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generate, without no_repeat_ngram_size argument:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">            Best completions            </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> logitsum </span>┃<span style=\"font-weight: bold\"> completion                </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│  -11.829 │ 'this is the third time'  │\n",
       "│  -13.488 │ 'this is the third year'  │\n",
       "│  -16.610 │ 'this is the second time' │\n",
       "│  -18.534 │ 'this is the second of'   │\n",
       "│  -21.050 │ 'this is the first time'  │\n",
       "│  -22.922 │ 'this is the first of'    │\n",
       "└──────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m            Best completions            \u001b[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mlogitsum\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion               \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│  -11.829 │ 'this is the third time'  │\n",
       "│  -13.488 │ 'this is the third year'  │\n",
       "│  -16.610 │ 'this is the second time' │\n",
       "│  -18.534 │ 'this is the second of'   │\n",
       "│  -21.050 │ 'this is the first time'  │\n",
       "│  -22.922 │ 'this is the first of'    │\n",
       "└──────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generate, with no_repeat_ngram_size argument:\n",
      "Last ngram:  one two one two\n",
      "Prohibiting:  one\n",
      "Prohibiting:  two\n",
      "Prohibiting:  one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Best completions            </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> logitsum </span>┃<span style=\"font-weight: bold\"> completion               </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│   -1.660 │ ' one two one two three' │\n",
       "│   -2.761 │ ' one two one two.'      │\n",
       "│   -2.883 │ ' one two one two,'      │\n",
       "└──────────┴──────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m           Best completions            \u001b[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mlogitsum\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│   -1.660 │ ' one two one two three' │\n",
       "│   -2.761 │ ' one two one two.'      │\n",
       "│   -2.883 │ ' one two one two,'      │\n",
       "└──────────┴──────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ngram:  two\n",
      "Prohibiting:  one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Best completions            </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> logitsum </span>┃<span style=\"font-weight: bold\"> completion               </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│   -1.660 │ ' one two one two three' │\n",
       "│   -1.824 │ ' one two one two two'   │\n",
       "│   -2.761 │ ' one two one two.'      │\n",
       "└──────────┴──────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m           Best completions            \u001b[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mlogitsum\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│   -1.660 │ ' one two one two three' │\n",
       "│   -1.824 │ ' one two one two two'   │\n",
       "│   -2.761 │ ' one two one two.'      │\n",
       "└──────────┴──────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests for `generate` passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing generate, without no_repeat_ngram_size argument:\")\n",
    "new_beams = beams.generate(toks_per_beam=2)\n",
    "new_beams.print()\n",
    "assert new_beams.logprobs_and_completions[0][1] == \"this is the third time\"\n",
    "\n",
    "print(\"Testing generate, with no_repeat_ngram_size argument:\")\n",
    "bigram_beams = Beams(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    logprob_sums = t.tensor([-0.0]).to(device),\n",
    "    tokens = t.tensor([[530, 734, 530, 734]]).to(device)\n",
    "    # tokens are \" one two one two\"\n",
    ")\n",
    "\n",
    "# With no_repeat_ngram_size=1, should not generate the token \" one\" or \" two\"\n",
    "new_bigram_beams = bigram_beams.generate(toks_per_beam=3, no_repeat_ngram_size=1)\n",
    "new_bigram_beams.print()\n",
    "assert all([not (completion[1].endswith(\" one\") or completion[1].endswith(\" two\")) for completion in new_bigram_beams.logprobs_and_completions])\n",
    "\n",
    "# With no_repeat_ngram_size=2, it can generate \" two\" (which it should), but not \" one\"\n",
    "new_bigram_beams = bigram_beams.generate(toks_per_beam=3, no_repeat_ngram_size=2)\n",
    "new_bigram_beams.print()\n",
    "assert all([not completion[1].endswith(\" one\") for completion in new_bigram_beams.logprobs_and_completions])\n",
    "assert any([not completion[1].endswith(\" two\") for completion in new_bigram_beams.logprobs_and_completions])\n",
    "\n",
    "print(\"All tests for `generate` passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests for `filter` passed!\n"
     ]
    }
   ],
   "source": [
    "logprob_sums = t.tensor([-1.0, -2.0]).to(device)\n",
    "tokens = t.tensor([\n",
    "    [19485, 13],\n",
    "    [19485, tokenizer.eos_token_id]\n",
    "]).to(device)\n",
    "\n",
    "beams_with_eos = Beams(model, tokenizer, logprob_sums, tokens)\n",
    "best_beams, early_terminations = beams_with_eos.filter(2)\n",
    "\n",
    "t.testing.assert_close(best_beams.logprob_sums, logprob_sums[[0]])\n",
    "t.testing.assert_close(best_beams.tokens, tokens[[0]])\n",
    "\n",
    "assert early_terminations.logprobs_and_completions == [(-2.0, \"Stop\" + tokenizer.eos_token)]\n",
    "\n",
    "print(\"All tests for `filter` passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
